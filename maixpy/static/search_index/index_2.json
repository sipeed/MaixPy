{"/maixpy/doc/en/README_no_screen.html":{"title":"MaixCAM MaixPy Screenless Edition Quick Start","content":" title: MaixCAM MaixPy Screenless Edition Quick Start ## About This Document As mentioned in the [Quick Start Guide](./index.html), it is **strongly recommended** to purchase the version with a screen for development, as it provides a better development experience, including using the built in APP, accessing apps from the MaixHub App Store, and easier debugging (e.g., common settings can be completed directly by touching the screen interface, and images can be viewed in real time). However, if you are unable to purchase the version with a screen or require a screenless version for mass production, please refer to this document. ## Getting a MaixCAM Device * **MaixCAM**: Purchase it from the [Sipeed Taobao Store](https://item.taobao.com/item.htm?id 784724795837) or the [Sipeed Aliexpress Store](https://www.aliexpress.com/store/911876460). You can find more information about MaixCAM [here](https://wiki.sipeed.com/maixcam). ## Initial Setup ### Preparing the TF Image Card and Inserting it into the Device If your package includes a TF card, it already contains the factory image. If the TF card was not installed in the device during manufacturing, carefully open the case (be careful not to disconnect any cables inside) and insert the TF card. Additionally, since the factory firmware may be outdated, it is **essential** to update the system to the latest version by following the <a href \"./basic/os\" target \"_blank\">updating to the latest system</a> instructions; otherwise, some applications and APIs may not function properly. If you did not purchase a TF card, you will need to flash the system onto your own TF card. Follow the <a href \"./basic/os\" target \"_blank\">updating to the latest system</a> guide, then install the card into the board. ### Powering On Use a `Type C` data cable to connect the `MaixCAM` device to provide power and wait for the device to boot. **Firstly**: Ensure that the USB cable is of good quality and that the USB port on your computer is reliable (power supply > 5V 500mA, normal interference resistance). The first boot may take about 20 seconds, after which your computer will detect one or two virtual network adapters (visible in your computer's network manager). If the virtual network adapter is not detected: * Ensure that you purchased the TF card package. If you have confirmed that the TF card is inserted into the device, try [updating to the latest system](./basic/os.html). * If you did not purchase the TF card package, you need to flash the latest system onto the TF card following the [Upgrade and Flash System](./basic/os.html) guide. * Check if the USB connection is loose and whether the USB cable is of good quality; you can try using a better quality cable. * Ensure that the USB port provides sufficient power. You can try another USB port or even another computer if possible. ### Logging into the Terminal If you need to log into the terminal, the default username for `MaixCAM` is `root`, and the password is `root`. ## Preparing to Connect the Computer and Device To enable communication between your computer (PC) and the device (MaixCAM), they need to be on the same local area network. Two methods are provided; we will first use Method 1: * **Method 1**: Wired connection. The device connects to the computer via a USB cable, and it will be recognized as a virtual USB network adapter, placing it on the same local area network as the computer. If you encounter issues, refer to the [FAQ](./faq.html) for common problems. .. details::Method 2 involves driver installation on different computer systems: :open: true There are two default USB virtual network adapter drivers (NCM and RNDIS) to meet the needs of different systems: * **Windows**: All Windows systems will automatically install the RNDIS driver. Only Win11 will automatically install the NCM driver. **Either one that works is fine** (NCM is faster than RNDIS). * Open Task Manager > Performance, and you will see a virtual Ethernet connection with an IP, for example, `10.131.167.100` is the computer's IP, and the device's IP is the same except the last digit changed to `1`, i.e., `10.131.167.1`. If it's Win11, you will see two virtual network adapters; you can use any one of the IPs. * Additionally, you can open the `Device Manager` on your computer (search `Device Manager` in the search bar). If the RNDIS and NCM drivers are correctly installed, **either one that works is fine**: ![RNDIS ok](../../static/image/rndis_windows.jpg) ![NCM ok](../../static/image/windows_ncm_ok.png) * **Linux**: No extra setup is required. Just plug in the USB cable. Use `ifconfig` or `ip addr` to see `usb0` and `usb1` network adapters, and you can use either IP. **Note** that the IP, for example, `10.131.167.100`, is the computer's IP, and the device's IP is the same except the last digit changed to `1`, i.e., `10.131.167.1`. * **MacOS**: Check the `usb` network adapter in `System Settings` > `Network`. **Note** that the IP, for example, `10.131.167.100`, is the computer's IP, and the device's IP is the same except the last digit changed to `1`, i.e., `10.131.167.1`. * **Method 2**: Wireless connection. The device connects to the same router or WiFi hotspot that the computer is connected to (if you experience screen lag or high latency with WiFi, use a wired connection). There are two methods for connecting to a wireless hotspot: * Modify the `wifi.ssid` and `wifi.pass` files in the TF card's boot partition and reboot to connect. Modification methods: * If you are familiar with SSH, you can connect to the device via SSH (if wired connection is available) and modify the files in the `/boot` directory. * You can also enter upgrade mode as described in the previous section, after which a USB drive will appear on the computer. Modify the files in it, ensuring to safely eject the drive before rebooting. * You can also use a card reader, and a USB drive will appear on the computer. Modify the `wifi.ssid` and `wifi.pass` files in it, ensuring to safely eject the drive before rebooting. * If the wired connection is already available, you can follow the next step and use MaixVision to run code. Modify the `tools/wifi_connect.py` script with your SSID and PASSWORD, then run it. ## Preparing the Development Environment * First, ensure that the computer and device are on the same local area network. * Download and install [MaixVision](https://wiki.sipeed.com/maixvision). * Use a Type C cable to connect the device and computer, open MaixVision, and click the `Connect` button at the bottom left. The software will automatically search for the device. Wait a moment until the device appears, then click the device to connect. If **the device is not detected**, you can find solutions in the [FAQ](./faq.html). Here is a video tutorial on using MaixVision: <video style \"width:100%\" controls muted preload src \"/static/video/maixvision.mp4\"></video> ### Connecting to the Internet The first run requires a network connection to activate the device and install the runtime library. If you do not have a router, you can use your phone to create a hotspot. In MaixVision, modify the `tools/wifi_connect.py` script with your SSID and PASSWORD, then run it. For other WiFi connection methods, see the previous section. ### Upgrading the Runtime Library **This step is very important!!!** If this step is not completed, other applications and features may not function properly (e.g., crashing). * First, ensure that the WiFi connection from the previous step is completed and that you have an IP address with internet access. * Run the `tools/install_runtime.py` script from the MaixVision examples to install the latest runtime library. If `Request failed` or a similar error appears, please check if the network is connected and able to access the internet. If the problem persists, take a photo and contact customer service for assistance. ## Running Examples Click on the `Example Code` on the left side of MaixVision, select an example, and click the `Run` button at the bottom left to send the code to the device for execution. For example: * `hello_maix.py`, click the `Run` button, and you will see messages printed by the device in the MaixVision terminal, and an image will appear in the top right corner. * `camera_display.py`, this example opens the camera and displays the camera feed on the screen. ```python from maix import camera, display, app disp display.Display() # Create a display object and initialize the screen cam camera.Camera(640, 480) # Create a camera object, manually setting the resolution to 640x480, and initialize the camera while not app.need_exit(): # Keep looping until the program exits (can exit by pressing the device's function button or clicking the stop button in MaixVision) img cam.read() # Read the camera feed into the img variable, print(img) can be used to print img details disp.show (img) # Display img on the screen ``` * `yolov5.py` detects objects in the camera feed, draws bounding boxes around them, and displays them on the screen. It supports detecting 80 different objects. For more details, see [YOLOv5 Object Detection](./vision/yolov5.html). You can try other examples on your own. > If you experience image lag when using the camera examples, it may be due to poor network connection, low quality USB cable, or poor USB port quality on the host. Try changing the connection method or using a different cable, USB port, or computer. ## Installing Applications on the Device The above steps allow you to run code on the device. Once `MaixVision` is disconnected, the code will stop running. If you want the code to appear in the boot menu, you can package it as an application and install it on the device. Click the install application button at the bottom left of `MaixVision`, fill in the application information, and it will be installed on the device. You will then see the application on the device. You can also choose to package the application and share it on the [MaixHub App Store](https://maixhub.com/app). > The default examples do not include an explicit exit function. Press the device's function button to exit the application (for MaixCAM, it is the user button). If you want the program to start automatically at boot, you can modify and run the `tools/set_autostart.py` script."},"/maixpy/doc/en/projects/line_tracking_robot.html":{"title":"MaixCAM MaixPy Line Tracking Robot (/Car)","content":" title: MaixCAM MaixPy Line Tracking Robot (/Car) update: date: 2024 05 09 author: lxowalle version: 1.0.0 content: Initial documentation Before reading this article, make sure you know how to develop with MaixCAM. For details, please read [Quick Start](../index.html). ## Introduction This article describes how to implement a line tracking robot using MaixPy. ## How to implement line tracking robot using MaixPy 1. Preparation of MaixCAM and trolley 2. Implementing the line tracking function 3. Implement the trolley control function ### Preparation of MaixCAM and trolley TODO ### Implementing the line tracking function You can quickly find straight lines using the `get_regression` of the `image` module, see [Line tracking](. /line_tracking.html). Code： ```python from maix import camera, display, image cam camera.Camera(320, 240) disp display.Display() # thresholds [[0, 80, 40, 80, 10, 80]] # red thresholds [[0, 80, 120, 10, 0, 30]] # green # thresholds [[0, 80, 30, 100, 120, 60]] # blue while 1: img cam.read() lines img.get_regression(thresholds, area_threshold 100) for a in lines: img.draw_line(a.x1(), a.y1(), a.x2(), a.y2(), image.COLOR_GREEN, 2) theta a.theta() rho a.rho() if theta > 90: theta 270 theta else: theta 90 theta img.draw_string(0, 0, \"theta: \" + str(theta) + \", rho: \" + str(rho), image.COLOR_BLUE) disp.show(img) ``` The above code implements the function of finding a straight line, note: Use `a.theta()` to get the angle of the line. Use `a.rho()` to get the distance between the line and the origin (the origin is in the upper left corner). After find the straight line with reference to the above code, you can use `a.theta()` and `a.rho()` to control the direction of the cart. ### Implement the trolley control function TODO"},"/maixpy/doc/en/projects/index.html":{"title":"Introduction and Summary of MaixCAM MaixPy Project Practices","content":" title: Introduction and Summary of MaixCAM MaixPy Project Practices ## Introduction This section provides: * Several common project practice examples for community members to reference and replicate, as well as to inspire more and better applications and projects. * Some open source projects from community members for learning and reference. Besides this document, there are several other ways to find MaixPy based projects: ### MaixPy Official Documentation You can find practical projects in the menu on the left side of this documentation, such as `Line Following Car`. If you have a good project or a recommended one, you're very welcome to click the \"Edit this page\" button in the top right corner and submit a PR (Pull Request) to add it to the documentation. ### MaixHub Project Sharing Plaza You can find shared projects in the [MaixHub Project Sharing](https://maixhub.com/share?type project) section. High quality projects may also be linked in the MaixPy official documentation. You’re encouraged to share your project tutorials there—this earns you official (guaranteed) and community member cash rewards (especially for high quality, urgently needed solutions). ### MaixHub App Sharing Besides project sharing, you can also find runnable applications in the [MaixHub App Store](https://maixhub.com/app). Some of them may be written with MaixPy, and if the author provides source code or detailed tutorials, they’re worth learning from. ### GitHub Search Searching for `MaixPy` or `MaixCAM` on [GitHub](https://github.com) can also lead you to many high quality open source projects. ## Open Source Project Summary These are usually complete projects that include source code, documentation, demo videos, and more. ### Built in Applications Applications pre installed on platforms such as `MaixCAM`, `MaixCAM Pro`, and `MaixCAM2` Built in Application Supported Platforms Description Documentation : : : : **App Store** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Install other applications [Documentation](https://maixhub.com/app/225) **Settings** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Modify system settings [Documentation](https://maixhub.com/app/224) **Benchmark** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Tests comprehensive performance of CPU/NPU and other hardware [Documentation](https://maixhub.com/app/188) **Local Chat** `MaixCAM2` Offline voice chat [Documentation](https://maixhub.com/app/187) **Desktop Monitor** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Host performance monitoring gadget [Documentation](https://maixhub.com/app/13) **Face Emotion** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Emotion recognition [Documentation](https://maixhub.com/app/189) **Face Landmarks** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Key point recognition [Documentation](https://maixhub.com/app/186) **Face Recognizer** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Face recognition [Documentation](https://maixhub.com/app/190) **Face Tracking** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Face tracking [Documentation](https://maixhub.com/app/31) **Gesture Classifier** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Recognize different gestures [Documentation](https://maixhub.com/app/192) **Hand Landmarks** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Hand key point detection [Documentation](https://maixhub.com/app/227) **HTTP File Browser** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` View and download files via browser [Documentation](https://maixhub.com/app/59) **Human Pose** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Human pose recognition [Documentation](https://maixhub.com/app/191) **Pose Classifier** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Recognize different human poses [Documentation](https://maixhub.com/app/193) **Image Generation** `MaixCAM2` Text to Image, Image to Image [Documentation](https://maixhub.com/app/198) **IMU AHRS** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` IMU data calculation [Documentation](https://maixhub.com/app/128) **MaixHub Client** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Interact with MaixHub [Documentation](https://maixhub.com/app/48) **Depth Estimation** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Monocular depth estimation [Documentation](https://maixhub.com/app/195) **Mouse Simulator** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Use device as a mouse [Documentation](https://maixhub.com/app/196) **OCR** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Text recognition (OCR) [Documentation](https://maixhub.com/app/70) **RTMP Live** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` RTMP streaming [Documentation](https://maixhub.com/app/35) **RTSP Stream** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` RTSP streaming [Documentation](https://maixhub.com/app/197) **Scan QR Code** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Scan and recognize barcodes, QR codes, Apriltag [Documentation](https://maixhub.com/app/199) **Self Learn Classifier** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Learn targets and classify [Documentation](https://maixhub.com/app/200) **Self Learn Tracker** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Learn targets and detect [Documentation](https://maixhub.com/app/62) **Thermal256 Camera** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` External thermal imaging module [Documentation](https://maixhub.com/app/208) **Thermal Night Vision** `MaixCAM2` Fusion of thermal camera and AI night vision [Documentation](https://maixhub.com/app/228) **Tracker Counter** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` People flow counting [Documentation](https://maixhub.com/app/61) **USB Hand Contrl** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Control mouse via gestures [Documentation](https://maixhub.com/app/223) **USB Pose Mario** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Control keyboard via human pose [Documentation](https://maixhub.com/app/178) **Local LLM** `MaixCAM2` Image to text [Documentation](https://maixhub.com/app/194) **WebRTC Stream** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` WebRTC streaming [Documentation](https://maixhub.com/app/202) **YOLO11 OBB** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Image detection with rotation angle [Documentation](https://maixhub.com/app/203) **YOLO11 Seg** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Image segmentation [Documentation](https://maixhub.com/app/204) **YOLO World** `MaixCAM2` YOLO World detection [Documentation](https://maixhub.com/app/229) **Camera** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Take photos, record videos [Documentation](https://maixhub.com/app/221) **Photos** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Display photos [Documentation](https://maixhub.com/app/222) **AI Classifier** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` AI classification [Documentation](https://maixhub.com/app/211) **AI Detector** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` AI detection [Documentation](https://maixhub.com/app/213) **Find blobs** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Find color blocks [Documentation](https://maixhub.com/app/33) **Line tracking** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Find lines [Documentation](https://maixhub.com/app/215) **Speech Recognition (Maix Speech)** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` Speech to text [Documentation](https://maixhub.com/app/216) **Speech Recognition (AI LLM)** `MaixCAM2` Speech to text [Documentation](https://maixhub.com/app/217) **Thermal Camera** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` External thermal camera module (PMOD_Thermal32) [Documentation](https://maixhub.com/app/218) **ToF Camera** `MaixCAM`, `MaixCAM Pro`, `MaixCAM2` External ToF module (ToF100) [Documentation](https://maixhub.com/app/219) **UVC Camera** `MaixCAM`, `MaixCAM Pro` Act as USB camera [Documentation](https://maixhub.com/app/220) ### Tools * [MaixPy UI Lib](https://github.com/aristorechina/MaixPy UI Lib): A lightweight UI library developed with MaixPy, written in pure Python. It supports a variety of widgets, is easy to use and extend, and includes many example codes such as LAB/HSV offline threshold tuning tools. * [Offline Threshold Tuning Tool Based on MaixPy](https://maixhub.com/share/103): A class based offline threshold tuning tool with no third party dependencies—ready to use directly. * [Using Serial Screen for Offline Threshold Adjustment on Maixcam](https://maixhub.com/share/104): Connects to an external serial screen to enable offline threshold adjustment, suitable for applications requiring an external display. * [CAM Offline Manual Threshold Editor](https://maixhub.com/share/102): Another manual offline threshold editor with simple and easy to understand code, great for learning and reference. ### Competition * [MaixCam Tic Tac Toe – 2024 National College Student Electronic Design Competition (Problem E Tic Tac Toe Game Device)](https://github.com/HYK X/MaixCam_Tic_Tac_Toe_2024): A tic tac toe robot project based on the Sipeed Maix series development board, using computer vision techniques. This is a complete solution for the 2024 National College Student Electronic Design Competition (Problem E Tic Tac Toe Game Device). ### Photography Waiting for your contribution. ### Surveillance and Smart Home Waiting for your contribution. ### Automation & Efficiency Tools Waiting for your contribution. ### Robotics Waiting for your contribution. ### More For more categories, feel free to submit an issue to discuss and suggest additions. ## Experience Sharing Summary These are usually simpler tips, experiences, and code snippets shared by the community, useful for learning and reference. ### UI Related * [MaixPy UI Lib](https://github.com/aristorechina/MaixPy UI Lib): A lightweight UI library written in pure Python for MaixPy, supporting multiple widgets, simple to use, easy to extend, and includes many example codes like LAB/HSV offline threshold tuning tools. * [Offline Threshold Tuning Tool based on MaixPy](https://maixhub.com/share/103): A class wrapped threshold tuning tool with no third party dependencies—ready to use. * [Using Serial Screen for Offline Threshold Adjustment on maixcam](https://maixhub.com/share/104): Connects to a serial screen to enable offline threshold adjustment, suitable for external screen applications. * [CAM Offline Manual Threshold Editor](https://maixhub.com/share/102): Another simple and easy to understand offline threshold editor, ideal for learning and reference. ### Peripheral Related * [Driving WS2812 for Lighting and Color Compensation with MaixCam](https://maixhub.com/share/90): Uses MaixCAM to drive WS2812 LED lights via hardware SPI. * [Capturing PWM Frequency and Duty Cycle with MaixCAM Pro](https://maixhub.com/share/98): Captures PWM frequency and duty cycle using SPI, suitable for measuring PWM signals. * [Using MaixCAM Bluetooth Function · Hardware Part](https://maixhub.com/share/58) ### Image Algorithm Related * [Open source: Simple Pose Estimation under Perspective Projection using cv2.solvePnP](https://maixhub.com/share/93): Detects the 3D pose of an object. * [Generating QR Codes with MaixPy](https://maixhub.com/share/79): Generates QR codes using the `qrcode` library. ## More This summary is not updated in real time. For more resources, please refer to the methods mentioned at the beginning of this article."},"/maixpy/doc/en/projects/face_tracking.html":{"title":"MaixCAM MaixPy Face Tracking 2 axis servo gimbal","content":" title: MaixCAM MaixPy Face Tracking 2 axis servo gimbal update: date: 2024 06 11 author: iawak9lkm version: 1.0.0 content: Initial documentation Before reading this article, make sure you know how to develop with MaixCAM. For details, please read [Quick Start](../index.html). [Source Code](https://github.com/sipeed/MaixPy/blob/main/projects/app_face_tracking) [Download APP](https://maixhub.com/app/31) ## Description Face recognition and tracking is accomplished using a gimbal consisting of two servos and MaixCAM. ![](../../assets/face_tracking1.jpg) ![](../../assets/face_tracking2.jpg) ## Usage of this example program * Assemble your Gimbal and MaixCAM. * Modify the parameters in `main.py`. Modify the MaixCAM pins used for each servo. The specified pins must have PWM capability.The `servos.Servos` constructor then configures the pin for PWM functionality. ```python ROLL_PWM_PIN_NAME \"A17\" PITCH_PWM_PIN_NAME \"A16\" ``` Modify the initial positions of the two servos. ```python init_pitch 80 # init position, value: [0, 100], means minimum angle to maxmum angle of servo init_roll 50 # 50 means middle ``` You need to modify the min max PWM duty cycle for the active range of each of the two servos. NOTE: Certain Gimbal configurations may have unintended consequences when servos exceed their physically limited maximum range of motion. Ensure that there is no obstruction within the range of motion of the servos corresponding to the following setpoints. ```python PITCH_DUTY_MIN 3.5 # The minimum duty cycle corresponding to the range of motion of the y axis servo. PITCH_DUTY_MAX 9.5 # Maximum duty cycle corresponding to the y axis servo motion range. ROLL_DUTY_MIN 2.5 # Minimum duty cycle for x axis servos. ROLL_DUTY_MAX 12.5 # Maxmum duty cycle for x axis servos. ``` You need to select the direction of motion of the servos. ```python pitch_reverse False # reverse out value direction roll_reverse True # reverse out value direction ``` * Just execute the code at the end. If you installed the application from MaixHub, click face_tracking in the launcher to execute the program. If you got the source code from Github, you can import the project folder in [MaixVision](https://wiki.sipeed.com/maixvision) and execute the whole project. Please refer to [MaixVision Description](https://wiki.sipeed.com/maixpy/doc/zh/basic/maixvision.html) for more information about MaixVision. Of course, you can also copy the whole project folder to our MaixCAM in your favorite way and execute it with python. * If you want to exit the program, just press the button in the upper left corner. ![](../../../../projects/app_face_tracking/assets/exit.jpg) ### FAQs * The face tracking is not ideal. Different Gimbal use different PID parameters, you can adjust the PID value to make the effect better. ```python pitch_pid [0.3, 0.0001, 0.0018, 0] # [P I D I_max] roll_pid [0.3, 0.0001, 0.0018, 0] # [P I D I_max] ``` * After completing the tracking, the gimbal jerks small left and right for a period of time against a motionless face. You can usually make this effect as small as possible by adjusting the PID; however, there is no way to avoid the jitter caused by the physical structure of the gimbal. You can try to adjust the deadband to minimize the jitter. ```python target_ignore_limit 0.08 # when target error < target_err_range*target_ignore_limit , set target error to 0 ``` * The display shows or the terminal prints `PIN: XXX does not exist`. This is because the pin does not exist in the pinout of the MaixCAM board. Please select a pin with PWM function on MaixCAM. * The display shows or the terminal prints `Pin XXX doesn't have PWM function`. This is because the pin does not have a PWM function, you need to select a pin with a PWM function. ## How to track other objects * In `main.py` there exists a class `Target` which is used to customize the target to be tracked. * In `__init__`, initialize the objects you need to use, such as the camera. * In `__get_target()`, you need to calculate the center point of the tracked object, and if the tracked object does not exist in the frame, return 1, 1 to make sure that the program does not do anything for a while if the target is not found. You also need to call `self.__exit_listener(img)` and `self.disp.show(img)` before returning to the point to make sure that the program can interact with you properly."},"/maixpy/doc/en/network/socket.html":{"title":"Using Socket for TCP/UDP Communication with MaixPy MaixCAM","content":" title: Using Socket for TCP/UDP Communication with MaixPy MaixCAM ## Introduction to Sockets Sockets are software abstractions for TCP/UDP communication. Through socket interfaces, we can perform TCP/UDP communication. Since MaixPy is based on Python, we can directly use the built in `socket` library for communication. For more documentation and tutorials, please search online. Here, we introduce simple usage methods. With these example codes, you can perform basic TCP and UDP communication on MaixPy MaixCAM. Remember to modify the IP address and port number according to your actual situation. ## Socket TCP Client This example requests a TCP server, sends a message, waits for a response, and then closes the connection. ```python import socket def tcp_client(ip, port): client_socket socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_address (ip, port) client_socket.connect(server_address) try: # Send data to the server message 'Hello, Server!' print(\"Send:\", message) client_socket.sendall(message.encode('utf 8')) # Receive the server's response data client_socket.recv(1024) print('Received:', data.decode('utf 8')) finally: # Close the connection client_socket.close() if __name__ \"__main__\": tcp_client(\"10.228.104.1\", 8080) ``` ## Socket TCP Server This example creates a socket server that continuously waits for client connections. Once a client connects, a thread is created to communicate with the client, reading the client's message and echoing it back. ```python import socket import threading local_ip \"0.0.0.0\" local_port 8080 def receiveThread(conn, addr): while True: print('Reading...') client_data conn.recv(1024) if not client_data: break print(client_data) conn.sendall(client_data) print(f\"Client {addr} disconnected\") ip_port (local_ip, local_port) sk socket.socket(socket.AF_INET, socket.SOCK_STREAM) sk.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sk.bind(ip_port) sk.listen(50) print(\"Waiting for clients...\") while True: conn, addr sk.accept() print(f\"Client {addr} connected\") # Create a new thread to communicate with this client t threading.Thread(target receiveThread, args (conn, addr)) t.daemon True t.start() ``` ## Socket UDP Client ```python import socket def udp_send(ip, port): # Create a socket object udp_socket socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # Define the server's IP address and port number server_address (ip, port) try: # Send data to the server message 'Hello, Server!' udp_socket.sendto(message.encode('utf 8'), server_address) finally: # Close the connection udp_socket.close() # Call the function udp_send(\"10.228.104.1\", 8080) ``` ## Socket UDP Server ```python import socket def udp_receive(ip, port): # Create a socket object udp_socket socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # Define the server's IP address and port number server_address (ip, port) # Bind the port udp_socket.bind(server_address) print('Waiting for a message...') while True: data, address udp_socket.recvfrom(1024) print('Received:', data.decode('utf 8')) print('From:', address) # Close the connection udp_socket.close() # Call the function udp_receive('0.0.0.0', 8080) ```"},"/maixpy/doc/en/network/http.html":{"title":"Using HTTP Network Communication with MaixPy MaixCAM","content":" title: Using HTTP Network Communication with MaixPy MaixCAM ## Introduction HTTP is an application layer network protocol based on TCP. Through it, we can send and receive information to and from network servers, such as retrieving webpage content from a web server. For more information, you can search for HTTP. ## Using HTTP Requests in MaixPy Since MaixPy is based on Python, you can directly use the built in `requests` library. The `requests` library is a very robust and user friendly library, so it won't be elaborated on here. Please search for related documentation and tutorials for more information. Here is an example of fetching the homepage content of `https://example.com`. ```python import requests url 'https://example.com' response requests.get(url) print(\"Response:\") print(\" status code:\", response.status_code) print(\"\") print(\" headers:\", response.headers) print(\"\") print(\" content:\", response.content) print(\"\") print(\" text:\", response.text) print(\"\") ```"},"/maixpy/doc/en/network/websocket.html":{"title":"Using WebSocket with MaixPy MaixCAM","content":" title: Using WebSocket with MaixPy MaixCAM ## Introduction Similar to sockets, WebSocket enables long lived communication connections and supports communication with web pages. Since MaixPy is based on Python, you can use the commonly available Python `websockets` and `asyncio` modules. For more detailed information, please refer to the documentation and tutorials available online. ## WebSocket Client The following example connects to a server, sends a message 10 times, and then ends the connection: ```python import asyncio import websockets import time async def send_msg(websocket): count 1 while count < 10: msg f\"hello {count}\" await websocket.send(msg) recv_text await websocket.recv() print(f\"Received: {recv_text}\", end \"\\n\") count + 1 time.sleep(1) await websocket.close(reason \"client exit\") async def main_logic(ip, port): async with websockets.connect(f'ws://{ip}:{port}') as websocket: await send_msg(websocket) ip \"10.228.104.100\" port 5678 asyncio.get_event_loop().run_until_complete(main_logic(ip, port)) ``` ## WebSocket Server The following example accepts client connections and responds with `ack for msg:` followed by the received message. ```python import asyncio import websockets import functools async def recv_msg(websocket): print(\"New client connected, recv_msg start\") while True: try: recv_text await websocket.recv() except Exception as e: print(\"Receive failed\") break print(\"Received:\", recv_text) response_text f\"ack for msg: {recv_text}\" await websocket.send(response_text) print(\"recv_msg end\") async def main_logic(websocket, path, other_param): await recv_msg(websocket) ip \"0.0.0.0\" port 5678 start_server websockets.serve(functools.partial(main_logic, other_param \"test_value\"), ip, port) print(\"Start server\") asyncio.get_event_loop().run_until_complete(start_server) print(\"Start server loop\") asyncio.get_event_loop().run_forever() ```"},"/maixpy/doc/en/network/network_settings.html":{"title":"Network Settings for MaixPy MaixCAM WiFi Configuration","content":" title: Network Settings for MaixPy MaixCAM WiFi Configuration ## Introduction To enable MaixCAM to use the network, it first needs to connect to the network via WiFi. MaixCAM provides several methods to connect to a WiFi hotspot. ## Using the Built in Settings Application After powering on, enter the `Settings` application and select the `WiFi` function. You can connect by sharing a `WiFi QR code` from your phone or by generating a QR code at [maixhub.com/wifi](https://maixhub.com/wifi) and scanning it. Alternatively, you can manually scan for `WiFi` hotspots and enter the password to connect. Once connected successfully and the DHCP assigns an IP address, the IP will be displayed on the screen. ## Connecting via MaixPy ```python from maix import network, err w network.wifi.Wifi() print(\"IP:\", w.get_ip()) SSID \"Sipeed_Guest\" PASSWORD \"qwert123\" print(\"Connecting to\", SSID) e w.connect(SSID, PASSWORD, wait True, timeout 60) err.check_raise(e, \"Failed to connect to WiFi\") print(\"IP:\", w.get_ip()) ``` ## DNS Server Configuration In practice, some users may find that their router's DNS resolution cannot resolve certain domain names. Therefore, the default system sets the DNS servers in the `/boot/resolv.conf` file: ```shell nameserver 114.114.114.114 # China nameserver 223.5.5.5 # Aliyun China nameserver 8.8.4.4 # Google nameserver 8.8.8.8 # Google nameserver 223.6.6.6 # Aliyun China ``` Generally, there is no need to modify this file. If you encounter DNS resolution issues, you can modify this file. The actual configuration file used by the system is located at `/etc/resolv.conf`. This file is automatically copied from `/boot/resolv.conf` at startup. Therefore, the simplest solution after modification is to reboot. If you prefer not to reboot, you need to modify both files simultaneously."},"/maixpy/doc/en/network/mqtt.html":{"title":"Using MQTT with MaixPy MaixCAM for Message Subscription and Publishing","content":" title: Using MQTT with MaixPy MaixCAM for Message Subscription and Publishing ## MQTT Introduction MQTT allows for quick and easy real time communication using a publish subscribe model. System components: * **MQTT Server (broker):** Responsible for forwarding messages. * **MQTT Clients:** Subscribe to topics from the server, receive messages, and publish messages to specific topics on the server. Communication process: * Clients connect to the MQTT server. * Clients subscribe to topics they are interested in, such as `topic1`. * When other clients or the server publish information on the `topic1` topic, it is pushed to the subscribing clients in real time. * Clients can also actively publish messages to specific topics. All clients subscribed to that topic will receive the messages. For example, if a client publishes a message to `topic1`, all clients subscribed to `topic1` will receive it, including the publishing client itself. ## Using MQTT in MaixPy MaixCAM The `paho mqtt` module can be used for this purpose. You can look up the usage of `paho mqtt` online or refer to the examples in the [MaixPy/examples](https://github.com/sipeed/MaixPy/tree/main/examples/network) repository. If you are using an older system, you might need to manually install the `paho mqtt` package. Installation instructions can be found in the [Adding Extra Python Packages](../basic/python_pkgs.html) guide."},"/maixpy/doc/en/network/flask.html":{"title":"Using Flask to Build an HTTP Web Server with MaixPy MaixCAM","content":" title: Using Flask to Build an HTTP Web Server with MaixPy MaixCAM ## Introduction MaixPy is based on Python, so you can use the Python library Flask to quickly set up a web server. As it is a common Python library, you can find specific uses and methods online, so they won't be elaborated on here. If you only want to create a page that displays camera images, you can also refer to the HTTP image server method in [JPEG Streaming](../video/jpeg_streaming.html). ## Simple HTTP Service Example After running the following program, accessing `http://device_ip:8000` in a computer browser will display the \"hello world\" text and an image. ```python from flask import Flask, request, send_file import maix # we not use it but we import it to listen for key events to exit this program app Flask(__name__) @app.route(\"/\", methods [\"GET\", \"POST\"]) def root(): print(\" \") print(request.remote_addr) print(f'headers:\\n{request.headers}') print(f'data: {request.data}') print(\" \") return 'hello world<br><img src \"/img\" style \"background color: black\">' @app.route(\"/<path:path>\") def hello(path): print(path) print(f'headers:\\n{request.headers}') print(f'data: {request.data}') print(\" \\n\\n\") return f\"hello from {path}\" @app.route(\"/img\") def img(): return send_file(\"/maixapp/share/icon/detector.png\") if __name__ \"__main__\": app.run(host \"0.0.0.0\", port 8000) ```"},"/maixpy/doc/en/source_code/add_c_module.html":{"title":"Adding a C/C++ Module to MaixCAM MaixPy","content":" title: Adding a C/C++ Module to MaixCAM MaixPy ## Introduction Sometimes you need to execute a function efficiently, and Python's speed is insufficient. In such cases, you can implement the function using C/C++ or other compiled languages. ## General Function Wrapping If the function you want to wrap does not depend on other features of MaixPy, you can directly use the general methods for adding modules to Python using C/C++. You can search for methods like `ffi` or `ctype` on the internet. > PRs are welcome to add more methods. ## If Your Module Needs to Depend on Other MaixPy Basic APIs ### Method 1 Directly modify the MaixPy firmware and then compile it. Refer to [View MaixPy API Source Code](../basic/view_src_code.html). This method is the simplest and fastest. If the code is well packaged, it can be merged into the official repository (by submitting a PR). * Follow [Compiling MaixPy Source Code](./build.html) to get the `dist/***.whl` installation package. * Send the `.whl` package from the `dist` directory to the device, then run the code `import os; os.system(\"pip install /root/xxxxx.whl\")` (replace the path accordingly). * If installing the `.whl` package is too slow during debugging, you can use `maixcdk build` to compile and then use `scp r maix_xxx root@10.228.104.1:/usr/lib/python3.11/site packages` to directly copy it to the device system to overwrite the package. Adjust the package name and device IP as needed. * Once you have finished debugging and feel that the features you added are valuable, consider merging them into the official repository. You can learn how to do this by searching for keywords like \"github submit PR\" on search engines. Modifying the code: As described in [View MaixPy API Source Code](../basic/view_src_code.html), you can view and modify the source code, add C++ functions, and include comments. After compiling, you can call them in MaixPy. It's very simple. For example: ```cpp namespace maix::test { /** * My function, add two integers. * @param a arg a, int type * @param b arg b, int type * @return int type, a + b * @maixpy maix.test.add */ int add(int a, int b); } ``` Yes, simply write a C++ function. Note the `@maixpy` comment. During compilation, a Python function will be automatically generated. It's that simple! Then you can call the function with `maix.test.add(1, 2)`. ### Method 2 Create a MaixPy module project based on an engineering template. This method is suitable for adding a package without modifying the MaixPy source code and still using MaixPy (MaixCDK) APIs. The method is as follows: * First, [compile MaixPy source code](./build.html) to ensure the compilation environment is set up correctly. * Copy the [MaixPy/tools/maix_module](https://github.com/sipeed/MaixPy/tree/main/tools/maix_module) project template to a new directory. It can be in the same directory as `MaixPy`. For example, copy all files and directories to the `maix_xxx` directory. * In the `maix_xxx` directory, run `python init_files.py` in the terminal to initialize the project files. * Change the project name: Modify the `module_name.txt` file to the desired module name, starting with `maix_`. This makes it easier for others to find your project on [pypi.org](https://pypi.org) or [github.com](https://github.com). * Run `python setup.py bdist_wheel linux` in the project root directory to build for the computer. * After building, you can directly run `python c \"import maix_xxx; maix_xxx.basic.print('Li Hua')\"` in the project root directory to test your module functions. * Run `python setup.py bdist_wheel maixcam` to build the package for `MaixCAM`. Note that the code prompt file (pyi file) can only be generated when building for the `linux` platform. Therefore, before releasing, first build for the `linux` platform to generate the code prompt file, then execute this command to generate the package for the `MaixCAM` platform. * Send the `.whl` package from the `dist` directory to the device, then run `import os; os.system(\"pip install /root/xxxxx.whl\")` (replace the path accordingly). * If installing the `.whl` package is too slow during debugging, you can use `maixcdk build` to compile and then use `scp r maix_xxx root@10.228.104.1:/usr/lib/python3.11/site packages` to directly copy it to the device system to overwrite the package. Adjust the package name and device IP as needed. * Once you have debugged your code, consider open sourcing it on [github.com](https://github.com) and uploading it to [pypi.org](https://pypi.org). You can refer to the official documentation or search for tutorials on how to upload. Generally, you need to run `pip install twine` and then `twine upload dist/maix_xxx***.whl`. After completing this, feel free to share your achievements on [maixhub.com/share](https://maixhub.com/share)! Modifying the code: As described in [View MaixPy API Source Code](../basic/view_src_code.html), add source files in the `components/maix/include` and `components/maix/src` directories, add C++ functions, and include comments. After compiling, you can call them directly. It's very simple. For example: ```cpp namespace maix_xxx::test { /** * My function, add two integers. * @param a arg a, int type * @param b arg b, int type * @return int type, a + b * @maix_xxx maix_xxx.test.add */ int add(int a, int b); } ``` Yes, simply write a C++ function. Note the `@maix_xxx` comment. During compilation, a Python function will be automatically generated. It's that simple! Then you can call the function with `maix_xxx.test.add(1, 2)`."},"/maixpy/doc/en/source_code/contribute.html":{"title":"Contributing to MaixCAM MaixPy Documentation Modification and Code Contribution","content":" title: Contributing to MaixCAM MaixPy Documentation Modification and Code Contribution ## Contributing to MaixPy Documentation Modification * Click the \"Edit this page\" button in the top right corner of the documentation you want to modify to enter the GitHub source documentation page. * Make sure you are logged in to your GitHub account. * Click the pencil icon in the top right corner of the GitHub preview documentation page to modify the content. * GitHub will prompt you to fork a copy to your own repository. Click the \"Fork\" button. > This step forks the MaixPy source code repository to your own account, allowing you to freely modify it. * Modify the documentation content, then fill in the modification description at the bottom of the page, and click \"Commit changes\". * Then find the \"Pull requests\" button in your repository and click to create a new Pull request. * In the pop up page, fill in the modification description and click \"Submit Pull request\". Others and administrators can then see your modifications on the [Pull requests page](https://github.com/sipeed/MaixPy/pulls). * Wait for the administrator to review and approve, and your modifications will be merged into the MaixPy source code repository. * After the merge is successful, the documentation will be automatically updated to the [MaixPy official documentation](https://wiki.sipeed.com/maixpy). > Due to CDN caching, it may take some time to see the update. For urgent updates, you can contact the administrator for manual refreshing. > You can also visit [en.wiki.sipeed.com/maixpy](https://en.wiki.sipeed.com/maixpy) to view the GitHub Pages service version, which is updated in real time without caching. ## Contributing to MaixPy Code Contribution * Visit the MaixPy code repository address: [github.com/sipeed/MaixPy](https://github.com/sipeed/MaixPy) * Before modifying the code, it is best to create an [issue](https://github.com/sipeed/MaixPy/issues) first, describing the content you want to modify to let others know your ideas and plans, so that everyone can participate in the modification discussion and avoid duplication of effort. * Click the \"Fork\" button in the top right corner to fork a copy of the MaixPy code repository to your own account. * Then clone a copy of the code from your account to your local machine. * After modifying the code, commit it to your repository. * Then find the \"Pull requests\" button in your repository and click to create a new Pull request. * In the pop up page, fill in the modification description and click \"Submit Pull request\". Others and administrators can then see your modifications on the [Pull requests page](https://github.com/sipeed/MaixPy/pulls). * Wait for the administrator to review and approve, and your modifications will be merged into the MaixPy source code repository. > Note that most of the MaixPy code is automatically generated from [MaixCDK](https://github.com/sipeed/MaixCDK), so if you modify the C/C++ source code, you may need to modify this repository first."},"/maixpy/doc/en/source_code/faq.html":{"title":"MaixCAM MaixPy Source Code FAQ","content":"MaixCAM MaixPy Source Code FAQ ## subprocess.CalledProcessError: Command '('lsb_release', ' a')' returned non zero exit status 1. Edit `/usr/bin/lsb_release` as root, change the first line from `#!/usr/bin/python3` to `python3`. Then compile again and it should work. ## ImportError: arg(): could not convert default argument 'format: maix::image::Format' in method '<class 'maix._maix.camera.Camera'>.__init__' into a Python object (type not registered yet?) Pybind11 need you to register `image::Format` first, then you can use it in `camera::Camera`, to we must fist define `image::Format` in generated `build/maixpy_wrapper.cpp` source file. To achieve this, edit `components/maix/headers_priority.txt`, the depended on should be placed before the one use it. e.g. ``` maix_image.hpp maix_camera.hpp ``` ## /usr/bin/ld: /lib/libgdal.so.30: undefined reference to `std::condition_variable::wait(std::unique_lock<std::mutex>&)@GLIBCXX_3.4.30' collect2: error: ld returned 1 exit status This issue commonly arises when building for Linux and using a conda environment, due to some libraries in the conda environment having compilation parameter problems. The solution is to not use conda, or to individually locate the problematic library within conda and replace it with the system's version or simply delete it (the system will then locate the necessary library)."},"/maixpy/doc/en/source_code/maixcdk.html":{"title":"MaixCAM Switching to MaixCDK for C/C++ Application Development","content":" title: MaixCAM Switching to MaixCDK for C/C++ Application Development In addition to developing with MaixPy, there is also a corresponding C/C++ SDK available, called [MaixCDK](https://github.com/sipeed/MaixCDK). ## Introduction to MaixCDK MaixPy is built on top of MaixCDK, and most of MaixPy's APIs are automatically generated based on MaixCDK's APIs. Therefore, any functionality available in MaixPy is also included in MaixCDK. If you are more familiar with C/C++ programming or require higher performance, you can use MaixCDK for development. ## Using MaixCDK The MaixCDK code repository is located at [github.com/sipeed/MaixCDK](https://github.com/sipeed/MaixCDK), where you can find the MaixCDK code and documentation."},"/maixpy/doc/en/source_code/build.html":{"title":"MaixCAM MaixPy develop source code guide","content":" title: MaixCAM MaixPy develop source code guide ## Get source code ```shell mkdir p ~/maix cd ~/maix git clone https://github.com/sipeed/MaixPy ``` ## Getting MaixCDK Source Code The MaixPy project depends on MaixCDK. You need to clone it first and place it in a directory on your computer (do not place it under the MaixPy directory). ```shell cd ~/maix git clone https://github.com/sipeed/MaixCDK ``` Then, you need to set the environment variable MAIXCDK_PATH to specify the path to MaixCDK, which can be added in ~/.bashrc or ~/.zshrc (depending on your shell): ```shell export MAIXCDK_PATH ~/maix/MaixCDK ``` Only after successfully setting the environment variable can MaixPy locate the MaixCDK source code. ## Build and pack to wheel ```shell cd ~/maix/MaixPy python setup.py bdist_wheel maixcam ``` `maixcam` Can be replaced with other board config, see [setup.py]([./configs](https://github.com/sipeed/MaixPy/blob/main/setup.py)) 's `platform_names` variable. After build success, you will find wheel file in `dist` directory, use `pip install U MaixPy****.whl` on your device to install or upgrade. > `python setup.py bdist_wheel maixcam skip build` will not execute build command and only pack wheel, so you can use `maixcdk menuconfig` and `maixcdk build` first to customize building. > Additionally, if you are debugging APIs and need to install frequently, using pip can be slow. You can compile and then copy the maix directory directly to the /usr/lib/python3.11/site packages directory on your device to overwrite the old files. ## Build manually ```shell maixcdk build ``` ## Run test after modify source code * First, build source code by ```shell maixcdk build ``` * If build for PC self(platform `linux`): Then execute `./run.sh your_test_file_name.py` to run python script. ```shell cd test ./run.sh examples/hello_maix.py ``` * If cross compile for board: * The fastest way is copy `maix` dir to device's `/usr/lib/python3.11/site packages/` directory, then run script on device. * Or pack wheel and install on device by `pip install U MaixPy****.whl`, then run script on device. ## Preview documentation locally Documentation in [docs](https://github.com/sipeed/MaixPy/tree/main/docs) directory, use `Markdown` format, you can use [teedoc](https://github.com/teedoc/teedoc) to generate web version documentation. And the API doc is generated when build MaixPy firmware, **if you don't build MaixPy, the API doc will be empty**. ```shell pip install teedoc U cd docs teedoc install i https://pypi.tuna.tsinghua.edu.cn/simple teedoc serve ``` Then visit `http://127.0.0.1:2333` to preview documentation on web browser. ## For developers who want to contribute See [MaixPy develop source code guide](./contribute.html) If you encounter any problems when use source code, please refer to [FAQ](./faq.html) first."},"/maixpy/doc/en/kit/microscope.html":{"title":"MaixCAM MaixPy Microscope Kit","content":" title: MaixCAM MaixPy Microscope Kit ![maixcam microscope](../../assets/maixcam_microscope.png) ## Introduction The MaixCAM Microscope Kit transforms your MaixCAM / MaixCAM Pro into a portable digital microscope, surpassing other digital microscopes in the same price range in terms of performance. With its open source nature, you can customize and create unique features tailored to your needs! The MaixCAM Microscope Kit is ideal for applications such as magnified soldering of small components, biological specimen observation, and microscopic focus stacking photography. Explore even more possibilities with this versatile tool! For more details, check out the **[documentation](https://wiki.sipeed.com/microscope)** and purchase page: <div style \"padding: 0 0 0 0; display: flex; justify content: left\"> <a target \"_blank\" style \"margin: 0.1em;color: white; font size: 0.9em; border radius: 0.3em; padding: 0.5em 2em; background color: #a80202\" href \"https://item.taobao.com/item.htm?id 878126152834\">Taobao</a> <a target \"_blank\" style \"margin: 0.1em;color: white; font size: 0.9em; border radius: 0.3em; padding: 0.5em 2em; background color: #a80202\" href \"https://wiki.sipeed.com/store\">AliExpress</a> </div> ## Demonstration ![](../../assets/maixcam_microscope_demo.png) ![](../../assets/maixcam_microscope_demo2.png)"},"/maixpy/doc/en/mllm/asr_sensevoice.html":{"title":"Running the SenseVoice Model on MaixPy MaixCAM","content":" title: Running the SenseVoice Model on MaixPy MaixCAM update: date: 2026 01 05 author: lxowalle version: 1.0.0 content: Added SenseVoice documentation ## SenseVoice Model Overview SenseVoice is a multilingual audio recognition model that supports Chinese, English, Cantonese, Japanese, and Korean. It provides features including speech recognition, automatic language detection, emotion recognition, automatic punctuation, and streaming recognition. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [sensevoice maixcam2](https://huggingface.co/sipeed/sensevoice maixcam2) MaixCAM2 1G Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy > Note: MaixPy version `4.12.3` or later is required ### Non Streaming Recognition ```python from maix import sensevoice model_path \"/root/models/sensevoice maixcam2\" client sensevoice.Sensevoice(model model_path+\"/model.mud\", stream False) client.start() if client.is_ready(block True) is False: print(\"Failed to start service or model.\") exit() audio_file \"/maixapp/share/audio/demo.wav\" text client.refer(path audio_file) print(text) # You can comment out this line of code, which will save time on the next startup. # But it will cause the background service to continuously occupy CMM memory. client.stop() ``` Output: ```shell 开始愉快的探索吧。 ``` Explanation: When creating the `sensevoice.Sensevoice` object, setting `stream False` enables non streaming recognition. The interface will wait until recognition is complete and then return the result at once. When the `refer` function is called with the `path` parameter, it recognizes an audio file. Currently, only the `wav` format is supported. Audio format requirements: `16,000` Hz sample rate, mono channel, 16 bit width. When the `refer` function is called with the `audio_data` parameter, it recognizes `bytes type PCM` data. Audio format requirements are the same: `16,000` Hz sample rate, mono channel, 16 bit width. The start function starts the `SenseVoice` background service, and the `stop` function stops it. Running `SenseVoice` as a background service allows multi process operation and prevents the foreground application from being blocked during model execution. ### Streaming Recognition ```python from maix import sensevoice model_path \"/root/models/sensevoice maixcam2\" client sensevoice.Sensevoice(model model_path+\"/model.mud\", stream True) client.start() if client.is_ready(block True) is False: print(\"Failed to start service or model.\") exit() audio_file \"/maixapp/share/audio/demo.wav\" print('start refer stream') for text in client.refer_stream(path audio_file): print(text) # You can comment out this line of code, which will save time on the next startup. # But it will cause the background service to continuously occupy CMM memory. client.stop() ``` Output: ```shell 开始愉快 开始愉快的探索 开始愉快的探索吧 ``` Explanation: When creating the `sensevoice.Sensevoice` object, setting `stream True` enables streaming recognition. Partial recognition results are returned immediately as they become available, until the entire audio is processed. Other behaviors are the same as described above. ### Real Time Speech Recognition via Microphone In practical development, you may need to capture audio data from a microphone and pass it to the model for speech to text processing. Please refer to the example:[asr_sensevoice.py](https://github.com/sipeed/MaixPy/tree/main/examples/audio/asr/sensevoice/asr_sensevoice.py)"},"/maixpy/doc/en/mllm/llm_deepseek.html":{"title":"Running the DeepSeek R1 Large Language Model on MaixPy MaixCAM","content":" title: Running the DeepSeek R1 Large Language Model on MaixPy MaixCAM update: date: 2025 05 28 author: neucrack version: 1.0.0 content: Added DeepSeek code and documentation ## Introduction to the DeepSeek Large Language Model In recent years, large language models (LLMs) have become extremely popular, bringing great convenience to both work and daily life. With LLMs, we can interact with them in natural conversation, ranging from casual chat to professional guidance. DeepSeek R1 is a large language model (LLM) developed by DeepSeek AI. It features reasoning capabilities and functions similarly to Qwen. Like other models, it comes in various parameter sizes, such as 72B, 32B, 7B, and 1.5B. Due to memory and computational limitations, MaixCAM2 can only run the 1.5B version. The 1.5B version is essentially a distilled version based on Qwen2.5. That means it is fundamentally a Qwen2.5 model, with differences in datasets and training methods. Therefore, the usage is basically the same as Qwen, and we won’t repeat the instructions here. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [deepseek r1 distill qwen 1.5B maixcam2](https://huggingface.co/sipeed/deepseek r1 distill qwen 1.5B maixcam2) MaixCAM2 4G Refer to the [Large Model User Guide](./basic.html) to download the model. ### Running the Model with MaixPy ```python from maix import nn, err, log, sys model \"/root/models/deepseek r1 distill qwen 1.5B/model.mud\" log.set_log_level(log.LogLevel.LEVEL_ERROR, color False) def show_mem_info(): print(\"memory info:\") for k, v in sys.memory_info().items(): print(f\"\\t{k:12}: {sys.bytes_to_human(v)}\") print(\"\") show_mem_info() qwen nn.Qwen(model) show_mem_info() def on_reply(obj, resp): print(resp.msg_new, end \"\") qwen.set_system_prompt(\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\") qwen.set_reply_callback(on_reply) msg \"Hello, please introduce yourself.\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) msg \"Please calculate 1990 + 35 and provide the calculation steps.\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) qwen.clear_context() msg \"please calculate 1990 + 35\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) # print(resp.msg) ``` Result: ``` >> Hello, please introduce yourself. <think> Alright, the user sent \"Hello, please introduce yourself.\" and then added a bot message saying, \"You are Qwen, created by Alibaba Cloud.\" I should respond warmly in Chinese. I need let them know I'm here to help with any questions they have and also mention how I can assist them further. I should phrase it politely, making sure it's clear and friendly. Also, it's good to offer to help them with whatever they're curious about. I think that covers it. I'll keep it concise and positive. </think> 你好！我是AI助手，由阿里巴巴AI研究有限公司开发，很高兴能为您提供帮助。有什么我可以帮助你的吗？ >> Please calculate 1990 + 35 and provide the calculation steps. <think> 好，让我看看用户的查询。用户给了一个计算题：“请计算1990加35，并提供计算步骤。”看起来用户是一位学生，可能刚刚学过数学，需要练习加法。用户希望得到详细的计算步骤。 首先，我需要确认用户的具体需求。用户明确要求计算步骤，所以我要仔细检查计算是否正确，以及步骤是否清晰。计算1990加35，首先可以从个位开始相加，然后是十位，再是百位和千位，如果有进位的话。 我得确保每个步骤都准确无误，确保用户理解每一步是怎么进行的。另外，我要注意数字的排列是否正确，避免笔误。如果用户需要，我可以补充其他数学问题，帮助他们进一步学习。 总之，我的回答应该包括计算过程的详细说明，确保用户能够一步步理解如何进行加法操作，同时也提供一些额外的信息和可能的补充帮助。 </think> 好的，让我来计算一下1990加35的步骤吧! 首先，写下这两个数字： ``` 1990 + 35 ``` **步骤1：对齐数字** 将两个数字对齐，确保它们的位数对齐。也就是说，个位对齐，十位对齐，百位对齐，千位对齐。 ``` 1990 + 35 ``` **步骤2：从个位开始相加** 个位的数字是0和5，相加的结果是5。写下这个结果，个位是5。 ``` 1990 + 35 5 ``` **步骤3：处理十位** 十位的数字是9和3相加，再加上前面个位的进位（但这里没有进位）。9加3等于12。写下2，并将1作为进位。 ``` 1990 + 35 25 ``` **步骤4：处理百位** 百位的数字是9和0相加，再加上前面的进位（1）。9加0等于9，加上进位的1，等于10。写下0，并将1作为进位到千位。 ``` 1990 + 35 125 ``` **步骤5：处理千位** 千位的数字是1和0，再加上前面的进位（1）。1加0等于1，加上进位的1，等于2。写下2。 ``` 1990 + 35 2025 ``` 所以，最终的计算结果是： ``` 1990 + 35 2025 ``` >> please calculate 1990 + 35 <think> Okay, so I need to calculate 1,9990 plus 35. Wait, that doesn't seem right. 1,990 sounds like a number with a comma in it, but I'm not sure. Maybe the comma is a thousands separator? So, 1,990 would be 1,990, right? Hmm, that makes more sense. So, I'm supposed to add 1,990 and 35. Let me try that again. 1,900 plus 90 is 1,990. Yeah, okay, that's correct. So, 1,990 plus 35 would be adding 35 to 1,990. So, 1,990 plus 30 is 2,020, and then plus 5 makes 2,025. So, the answer should be 2,025. Wait, but I'm not a math expert, so maybe I should double check that. 1,990 is the same as 1990, right? Yeah, 1,000 plus 990 is 1,990. Adding 35, so 1,990 plus 35 equals 2,025. Yeah, that makes sense. </think> The sum of 1,990 and 35 is calculated as follows: 1,990 + 35 2,025. **Answer:** 2,025 ``` ## Modifying Parameters Refer to [Qwen Documentation](./llm_qwen.html)。"},"/maixpy/doc/en/mllm/tts_melotts.html":{"title":"Running the MeloTTS Model on MaixPy MaixCAM","content":" title: Running the MeloTTS Model on MaixPy MaixCAM update: date: 2025 08 15 author: lxowalle version: 1.0.0 content: Initial version ## Introduction MeloTTS is a high quality multilingual text to speech library jointly developed by MIT and MyShell.ai. Currently, it supports the mellotts zh model, which can synthesize both Chinese and English speech. However, English synthesis is not yet optimal. The default output audio is PCM data with a sample rate of 44100 Hz, single channel, and 16 bit depth. > Sample rate: The number of times sound is sampled per second. > > Channels: The number of audio channels captured per sample. Single channel means mono audio, and dual channel means stereo (left and right channels). To reduce AI inference complexity, single channel audio is generally used. > > Bit depth: The data range captured per sample. A 16 bit depth usually represents each sample as a 16 bit signed integer. Higher bit depth captures finer audio details. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [melotts maixcam2](https://huggingface.co/sipeed/melotts maixcam2) MaixCAM2 1G base Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy ```python from maix import nn, audio # Only MaixCAM2 supports this model. sample_rate 44100 p audio.Player(sample_rate sample_rate) p.volume(80) melotts nn.MeloTTS(model \"/root/models/melotts maixcam2/melotts zh.mud\", speed 0.8, language 'zh') pcm melotts.infer('你好', output_pcm True) p.play(pcm) ``` Notes： 1. Import the nn module first to create a MeloTTS model object: ```python from maix import nn ``` 2. Choose the model to load. currently, the melotts zh model is supported: `speed` sets the playback speed `language` sets the language type ```python melotts nn.MeloTTS(model \"/root/models/melotts/melotts zh.mud\", speed 0.8, language 'zh') ``` 3. Start inference: The text to infer here is 'hello' Set `output_pcm True` to return PCM data ```python pcm melotts.infer('hello', output_pcm True) ``` 4. Use the audio playback module to play the generated audio: Make sure the sample rate matches the model’s output Use `p.volume(80)` to control the output volume (range: 0–100) Play the PCM generated by MeloTTS with `p.play(pcm)` ```shell p audio.Player(sample_rate sample_rate) p.volume(80) p.play(pcm) ```"},"/maixpy/doc/en/mllm/llm_qwen.html":{"title":"Running Qwen LLM on MaixPy MaixCAM","content":" title: Running Qwen LLM on MaixPy MaixCAM update: date: 2025 05 28 author: neucrack version: 1.0.0 content: Added Qwen code and documentation ## Introduction to Qwen LLM In recent years, large language models (LLMs) have gained significant popularity, bringing great convenience to work and daily life. With LLMs, we can interact via dialogue, handling everything from casual chatting to professional guidance. Qwen (Tongyi Qianwen) is a series of open source large language models (LLMs) and multimodal models (LMMs) developed by Alibaba Cloud under Alibaba Group. It aims to advance the development of Artificial General Intelligence (AGI). Since its first release in 2023, Qwen has continued to evolve and has demonstrated outstanding performance in various natural language processing and multimodal tasks. For more detailed information, feel free to search or visit the [Qwen official site](https://qwen.readthedocs.io/zh cn/latest/). Qwen actually includes many different models. This document mainly introduces the usage of the large language model (LLM) Qwen. ## Basic Concepts * Token: After a user inputs text, the text is not directly passed to the model. Instead, it is encoded in a specific way into a list of vocabulary indexes. For example, a vocabulary generated from a dataset might be: ```txt ... 1568 hello ... 1876 world ... 1987 ! 1988 (space) ``` The preceding numbers are line numbers, so if we input `hello world !`, it will be encoded as the list `[1568, 1988, 1876, 1988, 1987]`. This is called a `token`. This approach significantly reduces the number of input characters. Of course, this is just a basic introduction; in practice, extra identifier tokens, etc., are also included, and different models may have different token vocabularies and algorithms. Similarly, model outputs are also in tokens, which the program then converts back into human readable text. * Context: This refers to the dialogue context — interactions between the user and the model. The utterances are remembered and related to each other. * 72B/32B/1.5B/0.5B: These indicate the scale of trainable parameters in the model, with B representing Billion (one billion). The larger the parameters, the better the performance, but they require more memory and run slower. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [Qwen2.5 0.5B Instruct maixcam2](https://huggingface.co/sipeed/Qwen2.5 0.5B Instruct maixcam2) MaixCAM2 4G [Qwen2.5 1.5B Instruct maixcam2](https://huggingface.co/sipeed/Qwen2.5 1.5B Instruct maixcam2) MaixCAM2 4G Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy ```python from maix import nn, err, log, sys model \"/root/models/Qwen2.5 1.5B Instruct/model.mud\" # model \"/root/models/Qwen2.5 0.5B Instruct/model.mud\" log.set_log_level(log.LogLevel.LEVEL_ERROR, color False) def show_mem_info(): print(\"memory info:\") for k, v in sys.memory_info().items(): print(f\"\\t{k:12}: {sys.bytes_to_human(v)}\") print(\"\") show_mem_info() qwen nn.Qwen(model) show_mem_info() def on_reply(obj, resp): print(resp.msg_new, end \"\") qwen.set_system_prompt(\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\") qwen.set_reply_callback(on_reply) msg \"你好，请介绍你自己\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) msg \"请计算 1990 + 35的值，并给出计算过程\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) qwen.clear_context() msg \"please calculate 1990 + 35\" print(\">>\", msg) resp qwen.send(msg) err.check_raise(resp.err_code) # print(resp.msg) ``` Result: ``` >> 你好，请介绍你自己 你好！我是Qwen，我是一个由阿里云开发的预训练语言模型。我的设计目的是尽可能多地模拟人类语言的复杂性和多样性。虽然我没有个人记忆或情感，但我可以生成连贯和有意义的文本。如果你有任何问题或需要帮助，请告诉我！ >> 请计算 1990 + 35的值，并给出计算过程 计算的过程如下： 1. 首先，将两个数相加，即 1990 + 35。 2. 将 1990 和 35 对齐数字位数，如下： 1 9 9 0 + 3 5 2 0 2 5 3. 按照从右向左加起来： 0 + 5 5 2 + 9 11（进一，写 2 进 10） 1 + 9 10（进一，写 0 进 10） 1 + 1 2 所以，1990 + 35 的结果是 2025。 >> please calculate 1990 + 35 1990 + 35 2025 ``` ### Context Due to limited resources, the context length is also limited. For example, the default model supports about 512 tokens, and at least 128 free tokens must remain to continue the dialogue. For instance, if the historical tokens reach 500 (which is less than 512 but not enough free tokens), further dialogue is not possible. When the context is full, you currently must call `clear_context()` to clear the dialogue and start a new one. Of course, this context length can be modified, but doing so requires re quantizing the model. Also, longer context length can slow down model performance. If needed, you can convert the model yourself as described below. ## Modifying Parameters The Qwen model allows certain parameters to be modified, which can change the model's behavior. Default values are typically set within the `model.mud` file, Of course, you can also set these values programmatically, for example: ```python qwen.post_config.temperature 0.9 ``` configs for example: ```ini [post_config] enable_temperature true temperature 0.9 enable_repetition_penalty false repetition_penalty 1.2 penalty_window 20 enable_top_p_sampling false top_p 0.8 enable_top_k_sampling true top_k 10 ``` These parameters are used to **control the text generation behavior** of the Qwen model (or other large language models) through sampling strategies. They affect the **diversity, randomness, and repetition** of the output. Below is an explanation of each parameter: * `enable_temperature true` * `temperature 0.9` * **Meaning**: Enables the \"temperature sampling\" strategy and sets the temperature value to 0.9. * **Explanation**: * Temperature controls **randomness**. Lower values (e.g., 0.1) result in more deterministic outputs (similar to greedy search), while higher values (e.g., 1.5) increase randomness. * A recommended range is typically between `0.7 ~ 1.0`. * A value of 0.9 means a moderate increase in diversity without making the output too chaotic. * `enable_repetition_penalty false` * `repetition_penalty 1.2` * `penalty_window 20` * **Meaning**: * Repetition penalty is disabled, so the value `repetition_penalty 1.2` has no effect. * If enabled, this mechanism reduces the probability of repeating tokens from the most recent `20` tokens. * **Explanation**: * Helps prevent the model from being verbose or getting stuck in repetitive loops (e.g., “hello hello hello…”). * A penalty factor > 1 suppresses repetition. A common recommended range is `1.1 ~ 1.3`. * `enable_top_p_sampling false` * `top_p 0.8` * **Meaning**: * Top p (nucleus) sampling is disabled. * If enabled, the model samples from **the smallest set of tokens whose cumulative probability exceeds p**, instead of from all tokens. * **Explanation**: * `top_p 0.8` means sampling from tokens whose cumulative probability just reaches 0.8. * More flexible than top k, as it adapts the candidate set dynamically based on the token distribution in each generation step. * `enable_top_k_sampling true` * `top_k 10` * **Meaning**: Enables top k sampling, where the model selects output tokens from the **top 10 most probable tokens**. * **Explanation**: * This is a way to constrain the sampling space and control output diversity. * `top_k 1` approximates greedy search (most deterministic), while `top_k 10` allows for a moderate level of diversity. ## Custom Quantized Models The models provided above are quantized specifically for MaixCAM2. If you need to quantize your own models, refer to: * [pulsar2 documentation](https://pulsar2 docs.readthedocs.io/zh cn/latest/appendix/build_llm.html): for quantization and compilation. **Note**: pulsar2 version must be `> 4.0`. * Original model: https://huggingface.co/Qwen/Qwen2.5 1.5B Instruct * More article: https://zhuanlan.zhihu.com/p/706645301 Default model conversion command: ```shell pulsar2 llm_build input_path Qwen2.5 0.1B Instruct output_path models/Qwen2.5 1.5B Instruct ax630c hidden_state_type bf16 prefill_len 128 kv_cache_len 1023 last_kv_cache_len 256 last_kv_cache_len 512 chip AX620E c 1 ``` You can modify it based on your own needs."},"/maixpy/doc/en/mllm/vlm_internvl.html":{"title":"MaixPy MaixCAM Running VLM InternVL Vision-Language Model","content":" title: MaixPy MaixCAM Running VLM InternVL Vision Language Model update: date: 2025 06 05 author: neucrack version: 1.0.0 content: Added InternVL code and documentation ## Introduction to InternVL VLM (Vision Language Model) refers to a vision language model that allows AI to generate text output based on both text and image input, such as describing the content of an image, meaning the AI has learned to interpret images. InternVL supports multiple languages, such as Chinese and English. MaixPy has integrated [InternVL2.5](https://huggingface.co/OpenGVLab/InternVL2_5 1B), which is based on Qwen2.5 with added image support. Therefore, some basic concepts are not detailed here. It is recommended to read the [Qwen](./llm_qwen.html) introduction first. For example, with this image, using the system prompt `你是由上海人工智能实验室联合商汤科技开发的书生多模态大模型，英文名叫InternVL, 是一个有用无害的人工智能助手。` and the user prompt `Describe the picture` on MaixCAM2 with InternVL2.5 1B (which is also the result of the code below): ![ssd\\_car.jpg](../../assets/ssd_car.jpg) ``` >> 请描述图中有什么 图中有一个红色双层巴士停在马路上，前面是一辆黑色的小轿车。一位穿黑色夹克的人站在巴士前面，脸上带着微笑。背景是城市建筑，有商店和多幅广告牌。路上的画面上有一个行人图案。 >> Describe the picture In the image, we see a vibrant street scene featuring a classic double decker bus in red with \"Things Get New Look!\" written on its side. It’s parked on the street, where a woman stands smiling at the camera. Behind the bus, a row of classic buildings with large windows lines the street, contributing to the urban atmosphere. A black van is parked nearby, and there are a few people and street signs indicating traffic regulations. The overall scene captures a typical day in a historic city. ``` This is the result with a casually set prompt. You can adjust the system and user prompts according to the actual situation. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [InternVL2.5 1B maixcam2](https://huggingface.co/sipeed/InternVL2.5 1B maixcam2) MaixCAM2 4G Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPY ```python from maix import nn, err, log, sys, image, display model \"/root/models/InternVL2.5 1B maixcam2/model.mud\" log.set_log_level(log.LogLevel.LEVEL_ERROR, color False) disp display.Display() def show_mem_info(): print(\"memory info:\") for k, v in sys.memory_info().items(): print(f\"\\t{k:12}: {sys.bytes_to_human(v)}\") print(\"\") show_mem_info() internvl nn.InternVL(model) show_mem_info() in_w internvl.input_width() in_h internvl.input_height() in_fmt internvl.input_format() print(f\"input size: {in_w}x{in_h}, format: {image.format_name(in_fmt)}\") def on_reply(obj, resp): print(resp.msg_new, end \"\") internvl.set_system_prompt(\"You are InternVL, a multimodal large model co developed by Shanghai AI Laboratory and SenseTime, a helpful and harmless AI assistant.\") internvl.set_reply_callback(on_reply) # load and set image img image.load(\"/maixapp/share/picture/2024.1.1/ssd_car.jpg\", format in_fmt) internvl.set_image(img, fit image.Fit.FIT_CONTAIN) # if size not match, will auto resize first disp.show(img) # set prompt msg \"请描述图中有什么\" print(\">>\", msg) resp internvl.send(msg) err.check_raise(resp.err_code) msg \"Describe the picture\" print(\">>\", msg) resp internvl.send(msg) err.check_raise(resp.err_code) # print(resp.msg) ``` Result: ``` >> 请描述图中有什么 图中有一个红色双层巴士停在马路上，前面是一辆黑色的小轿车。一位穿黑色夹克的人站在巴士前面，脸上带着微笑。背景是城市建筑，有商店和多幅广告牌。路上的画面上有一个行人图案。 >> Describe the picture In the image, we see a vibrant street scene featuring a classic double decker bus in red with \"Things Get New Look!\" written on its side. It’s parked on the street, where a woman stands smiling at the camera. Behind the bus, a row of classic buildings with large windows lines the street, contributing to the urban atmosphere. A black van is parked nearby, and there are a few people and street signs indicating traffic regulations. The overall scene captures a typical day in a historic city. ``` This loads an image from the system and asks the model to describe what’s in the image. Note that this model **does not support context**, meaning each call to the `send` function is a brand new conversation and does not remember the content from previous `send` calls. Additionally, the default model supports image input resolution of `364 x 364`. So when calling `set_image`, if the resolution doesn't match, it will automatically call `img.resize` to resize the image using the method specified by `fit`, such as `image.Fit.FIT_CONTAIN`, which resizes while maintaining the original aspect ratio and fills the surrounding space with black. `set_system_prompt` is the system prompt and can be modified to improve accuracy in your application scenario. Note that the length of the input text after being tokenized is limited. For example, the default 1B model supports 256 tokens, and the total tokens for input and output should not exceed 1023. ## Modifying Parameters Refer to [Qwen Documentation](./llm_qwen.html)。 ## Custom Quantized Model The model provided above is a quantized model for MaixCAM2. If you want to quantize your own model, refer to: * [Pulsar2 Documentation](https://pulsar2 docs.readthedocs.io/zh cn/latest/appendix/build_llm.html) * Original model: https://huggingface.co/OpenGVLab/InternVL2_5 1B * More articles: [https://zhuanlan.zhihu.com/p/4118849355](https://zhuanlan.zhihu.com/p/4118849355)"},"/maixpy/doc/en/mllm/asr_whisper.html":{"title":"Running the Whisper Model on MaixPy MaixCAM","content":" title: Running the Whisper Model on MaixPy MaixCAM update: date: 2026 01 05 author: lxowalle version: 1.0.0 content: Added Whisper documentation ## Whisper Model Overview Whisper is a general purpose speech recognition model open sourced by OpenAI, designed for tasks such as multilingual speech recognition and speech translation. Currently, the Whisper model ported to MaixCAM2 is the `base` version. It supports input WAV audio files with mono channel and 16 kHz sample rate, and can recognize Chinese and English. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [whisper base maixcam2](https://huggingface.co/sipeed/whisper base maixcam2) MaixCAM2 1G base Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy Currently, only the base size Whisper model is supported. It accepts mono, 16 kHz WAV audio files and supports Chinese and English recognition. Below is a simple example demonstrating how to use Whisper for speech recognition: ```python from maix import nn whisper nn.Whisper(model \"/root/models/whisper base maixcam2/whisper base.mud\") wav_path \"/maixapp/share/audio/demo.wav\" res whisper.transcribe(wav_path) print('res:', res) ``` Notes: 1. First, import the nn module to create a Whisper model object: ```python from maix import nn ``` 2. Select the model to load. Currently, only the base size Whisper model is supported: ```python whisper nn.Whisper(model \"/root/models/whisper base maixcam2/whisper base.mud\") ``` 3. Prepare a mono, 16 kHz WAV audio file and run inference. The recognition result will be returned directly: ```python wav_path \"/maixapp/share/audio/demo.wav\" res whisper.forward(wav_path) print('whisper:', res) ``` 4. Output result: ```shell whisper: 开始愉快的探索吧 ``` By default, the model recognizes Chinese. To recognize English, specify the `language` parameter when initializing the object: ```python whisper nn.Whisper(model \"/root/models/whisper base/whisper base maixcam2.mud\", language \"en\")"},"/maixpy/doc/en/mllm/vlm_smolvlm.html":{"title":"MaixPy MaixCAM Running SmolVLM Visual Language Model","content":" title: MaixPy MaixCAM Running SmolVLM Visual Language Model update: date: 2025 12 03 author: lxowalle version: 1.0.0 content: Added SmolVLM code and documentation ## Introduction to SmolVLM VLM (Vision Language Model) refers to models that can take text + image input and output text, such as describing the content in an image—essentially enabling the AI to “see.” SmolVLM currently supports English only. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [smolvlm 256m instruct maixcam2](https://huggingface.co/sipeed/smolvlm 256m instruct maixcam2) MaixCAM2 1G Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy ```python from maix import nn, err, log, sys, image, display model \"/root/models/smolvlm 256m instruct maixcam2/model.mud\" log.set_log_level(log.LogLevel.LEVEL_ERROR, color False) disp display.Display() smolvlm nn.SmolVLM(model) in_w smolvlm.input_width() in_h smolvlm.input_height() in_fmt smolvlm.input_format() print(f\"input size: {in_w}x{in_h}, format: {image.format_name(in_fmt)}\") def on_reply(obj, resp): print(resp.msg_new, end \"\") smolvlm.set_system_prompt(\"Your a helpful assistant.\") smolvlm.set_reply_callback(on_reply) # load and set image img image.load(\"/maixapp/share/picture/2024.1.1/ssd_car.jpg\", format in_fmt) smolvlm.set_image(img, fit image.Fit.FIT_CONTAIN) # if size not math, will auto resize first disp.show(img) msg \"Describe the picture\" print(\">>\", msg) resp smolvlm.send(msg) err.check_raise(resp.err_code) ``` Output: ``` >> Describe the picture The image depicts a prominent bus stop, specifically in the middle, where a young woman is captured and standing on the sidewalk. The bus, which appears to be a double decker bus, is prominently displayed in the center of the image. The bus is red with bold white text and design elements on its side. The text on the bus reads \"THING'S GET MORE EXCITING.\" Below this text is a small image of the bus logo. The bus is parked next to another bus, both in a city background. The background on which the bus is parked is not clearly discernible due to the perspective, but it looks urban due to the buildings and street signs visible. The woman in the image is looking towards the bus on the street, possibly waiting to board or simply admiring the scene. She is wearing a black coat, and her hair is short and dark. The bus itself has a red roof, and its windows are visible. The bus’s front is also visible, but it is not as prominent as the bus’s front side. In the background, there are buildings and a large glass window. The sky is not visible, but it is bright, as indicated by the light reflection on the windows. The street is wide and seems to be a busy urban street, possibly with cars and other vehicles. The bus stop itself seems to be in an area that is busy. There are traffic signs visible, and the sidewalk looks well maintained. The street is wide enough for a bus to pass by at a distance, though it is not very wide. The overall environment appears modern and functional. This vivid depiction of the bus stop and the surrounding environment provides a clear and detailed view of the scene. ``` Additionally, the default model supports an image input resolution of `512×512`, so when calling `set_image`, if the image resolution does not match, it will automatically call `img.resize` to scale it. The scaling method is controlled by the `fit` parameter. For example, `image.Fit.FIT_CONTAIN` preserves the original aspect ratio and fills the padding with black when the aspect ratio differs from the required resolution. ## Custom Quantized Model Some model parameters can be modified. Refer to [the Qwen documentation](./llm_qwen.html) for details. ## Custom Quantized Model The model provided above is a quantized model for MaixCAM2. If you want to quantize your own model, refer to: * [Pulsar2 Documentation](https://pulsar2 docs.readthedocs.io/zh cn/latest/appendix/build_llm.html) * Original model: https://huggingface.co/HuggingFaceTB/SmolVLM 256M Instruct"},"/maixpy/doc/en/mllm/basic.html":{"title":"MaixPy MaixCAM Large Model User Guide","content":" title: MaixPy MaixCAM Large Model User Guide update: date: 2026 01 05 author: lxowalle version: 1.0.0 content: Added large model usage documentation ## Introduction This document provides an overview of how to obtain and use large models, including models for text to image, image to image, speech to text, text to speech, and chat. List of supported large models: Supported Model Supported Platform Memory Requirement Description [lcm lora sdv1 5 maixcam2](./dlm_lora_sdv1_5.html) MaixCAM2 4G Text to Image / Image to Image [lcm lora sdv1 5 320x320 maixcam2](./dlm_lora_sdv1_5.html) MaixCAM2 4G Text to Image / Image to Image [sensevoice maixcam2](./asr_sensevoice.html) MaixCAM2 1G Speech to Text [whisper basic maixcam2](asr_whisper.html) MaixCAM2 1G Speech to Text [melotts maixcam2](tts_melotts.html) MaixCAM2 1G Text to Speech [smolvlm 256m instruct maixcam2](vlm_smolvlm.html) MaixCAM2 1G Vision Language Model [InternVL2.5 1B maixcam2](vlm_internvl.html) MaixCAM2 4G Vision Language Model [ Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448 maixcam2](vlm_qwen3.html) MaixCAM2 4G Vision Language Model [deepseek r1 distill qwen 1.5B maixcam2](llm_deepseek.html) MaixCAM2 4G Language Model [Qwen2.5 1.5B Instruct maixcam2](llm_qwen.html) MaixCAM2 4G Language Model [Qwen2.5 0.5B Instruct maixcam2](llm_qwen.html) MaixCAM2 4G Language Model ## Download Methods Currently, two download methods are provided: `cloud storage download` and `HuggingFace download`. ### Cloud Storage Download [Download Images (Baidu Netdisk)](https://pan.baidu.com/s/1r4ECNlaTVxhWIafNBZOztg) Extraction code:`vjex` [Download Images (MEGA)](https://mega.nz/folder/01IEDZQb#3ktByGkFMn_x6jDxMLbK4w) From the `List of supported large models` above, locate the required model and download it from the cloud storage. For example, for the `lcm lora sdv1 5 maixcam2` model, download a file similar to `lcm lora sdv1 5 maixcam2 202601051759.zip`, The suffix `202601051759` indicates the model packaging time. ### HuggingFace Download > 注： > > 1. Downloading from HuggingFace requires a stable network environment. Poor connectivity may result in interrupted downloads > 2. The following methods can also be executed directly in the terminal of the target platform (e.g., MaixCAM2). 1. Download via Command Line Install the download tool: ```shell pip install huggingface_hub ``` Set the download endpoint. The default is `https://huggingface.co` For users in China, it is recommended to use `https://hf mirror.com` ```shell # Linux/MacOS export HF_ENDPOINT https://hf mirror.com # or 'https://huggingface.co' # Windows ## cmd set HF_ENDPOINT https://hf mirror.com ## PowerShell $env:HF_ENDPOINT \"https://hf mirror.com\" ``` Example for downloading the `lcm lora sdv1 5 maixcam2` model. To download another model, replace `lcm lora sdv1 5 maixcam2` with the desired model name. ```shell # Download model (new) hf download sipeed/lcm lora sdv1 5 maixcam2 local dir /root/models # Download model (legacy) huggingface cli download sipeed/lcm lora sdv1 5 maixcam2 local dir /root/models ``` 2. Download Using Python Install the `huggingface_hub` package: ```python pip install huggingface_hub ``` Example for downloading the `lcm lora sdv1 5 maixcam2` model. To download other models, replace the `model_name` variable accordingly. ```python # This scripy is used to install models from huggingface # Only support MaixCAM2 platform import os os.environ['HF_ENDPOINT'] 'https://hf mirror.com' # or 'https://huggingface.co' from huggingface_hub import snapshot_download from huggingface_hub.utils import tqdm CAPTURE_PROGRESS False class Tqdm(tqdm): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) if CAPTURE_PROGRESS: print(f\"[INIT] {self.desc} {self.n}/{self.total} ({self.n/self.total*100:.2f}%)\") else: print('') def update(self, n 1): super().update(n) if CAPTURE_PROGRESS: print(f\"[UPDATE] {self.desc} {self.n}/{self.total} ({self.n/self.total*100:.2f}%)\") else: print('') def close(self): super().close() if CAPTURE_PROGRESS: print(f\"[CLOSE] {self.desc} {self.n}/{self.total} ({self.n/self.total*100:.2f}%)\") else: print('') model_name 'lcm lora sdv1 5 maixcam2' repo_id f'sipeed/{model_name}' local_dir f'/root/models/{model_name}' snapshot_download( repo_id repo_id, local_dir local_dir, # allow_patterns \"*.py\", tqdm_class Tqdm, ) ``` ## Uploading Models to the Board After downloading, you can upload the model to the board using the `scp` command. It is recommended to upload models to the `/root/models` directory. Using `lcm lora sdv1 5 maixcam2` as an example: 1. Confirm the Files to Upload The directory structure of the `lcm lora sdv1 5 maixcam2` model is: ```shell lcm lora sdv1 5 maixcam2 ├── lcm lora sdv1 5 maixcam2\t\t\t\t\t# The actual model files to upload ├── README.md ├── README_ZH.md └── launcher.py ``` The folder `lcm lora sdv1 5 maixcam2` under the parent directory is the actual model directory that must be uploaded. Make sure to upload the entire model directory. 2. Upload the Model to the Development Board Example of uploading the `lcm lora sdv1 5 maixcam2` model to the board using `scp`(assuming the board IP address is 192.168.10.100): > Notes: > > 1. It is recommended to upload models via the USB network interface for higher transfer speed. For details on obtaining the USB network IP address, see[Wired Connection](../README_MaixCAM2.html) ```shell scp r lcm lora sdv1 5 maixcam2/lcm lora sdv1 5 maixcam2 root@192.168.10.100:/root/models ``` ## Using the Models For instructions on how to use each model, please refer to the corresponding documentation, for example: [Qwen Large Language Model](./llm_qwen.html) [InternVL Vision Language Model](./vlm_internvl.html) [SmolVLM Vision Language Model](./vlm_smolvlm.html)"},"/maixpy/doc/en/mllm/dlm_lora_sdv1_5.html":{"title":"Running the LCM-LoRA-SDv1-5 Model on MaixPy MaixCAM","content":" title: Running the LCM LoRA SDv1 5 Model on MaixPy MaixCAM update: date: 2025 12 03 author: lxowalle version: 1.0.0 content: Added LCM LoRA SDv1 5 code and documentation ## Introduction to the LCM LoRA SDv1 5 Model LCM LoRA SDv1 5 is a model that supports text to image and image to image generation, based on the StableDiffusion 1.5 LCM project. With this model, you can generate conceptual images for artistic creation—simply provide a text description, and the model will generate an image based on it. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [lcm lora sdv1 5 maixcam2](https://huggingface.co/sipeed/lcm lora sdv1 5 maixcam2) MaixCAM2 4G Output 256x256 resolution [lcm lora sdv1 5 320x320 maixcam2](https://huggingface.co/sipeed/lcm lora sdv1 5 320x320 maixcam2) MaixCAM2 4G Output 320x320 resolution Refer to the [Large Model User Guide](./basic.html) to download the model. ### Running the Model with MaixPy > Note: MaixPy version `4.12.3` or above is required for support. This example demonstrates how to use MaixPy to run the LCM LoRA SDv1 5 model for both text to image and image to image generation. ```python from maix import sdv1_5 model sdv1_5.SDV1_5(\"/root/models/lcm lora sdv1 5 maixcam2/ax620e_models\") model.init(img2img True) model.refer(prompt \"A white dog.\", save_path \"/root/text2img.jpg\") model.refer(prompt \"Replace the dog with a cat.\", init_image_path \"/root/text2img.jpg\", seed 1, save_path \"/root/img2img.jpg\") model.deinit() ``` Output: ![](../../assets/ldm_sdv1.5_img2img.jpg) Explanation: 1. When performing text to image generation, you need to define a `prompt` describing the image you want. The prompt must be written in English, and the generated image will be saved to the path specified by `save_path`. 2. When performing image to image generation, you need to define a prompt describing the content you want to generate`init_image_path` specifies the initial image,`seed` controls randomness in image generation The output image will be saved to the path specified by save_path. 3. Since `sdv1_5` depends on several large Python packages, importing it may take some time. If you want the application to enter the UI quickly during development, you can use `importlib` and `threading` to load the module in the background. Example: ```python from maix import time import importlib, threading class ModuleLoader(): def __load_module(self): self.module importlib.import_module(self.module_name) self.load_ok True pass def __init__(self, module_name: str): self.module_name module_name self.module None self.load_ok False self.t threading.Thread(target self.__load_module) self.t.start() def try_get_module(self, block True): if self.load_ok: return self.module if block: while not self.load_ok: time.sleep_ms(100) return self.module else: return None sdv1_5_loader ModuleLoader(\"maix.sdv1_5\") while True: sdv1_5 sdv1_5_loader.try_get_module(block False) if sdv1_5 is not None: break time.sleep_ms(1000) print(sdv1_5) ``` ### Running the Model with Command Line Refer to `launcher.py` in the model directory to run the model. #### Text to Image ```shell cd lcm lora sdv1 5 maixcam2 python3 launcher.py isize 256 model_dir ax620e_models/ o \"ax620e_txt2img_axe.png\" prompt \"a white dog\" ``` Parameter description: ` isize`: Input image size, recommended value is 256 ` model_dir`: Model directory ` o`: Output image filename ` prompt`: Description text; the model generates an image based on this description #### Image to Image ```shell cd lcm lora sdv1 5 maixcam2 python3 launcher.py init_image ax620e_models/img2img init.png isize 256 model_dir ax620e_models/ seed 1 prompt \"Change to black clothes\" o \"ax620e_img2img_axe.png\" ``` Parameter description: ` init_image`: Initial image; the model generates a new image based on this ` isize`: Input image size, recommended value is 256 ` model_dir`: Model directory ` seed`: Random seed, controls randomness during image generation ` o`: Output image filename ` prompt`: Description text; the model generates an image based on this description"},"/maixpy/doc/en/mllm/vlm_qwen3.html":{"title":"MaixPy MaixCAM Running VLM Qwen3-VL Visual Language Model","content":" title: MaixPy MaixCAM Running VLM Qwen3 VL Visual Language Model update: date: 2025 11 27 author: lxowalle version: 1.0.0 content: Added Qwen3 VL code and documentation ## Introduction to Qwen3 VL `Qwen3 VL` is a visual language model from the Qwen series. Compared to the previous generation, it offers superior text comprehension and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamic understanding, and stronger agent interaction capabilities. [Qwen3 VL 2B](https://huggingface.co/Qwen/Qwen3 VL 2B Instruct) has been ported to MaixPy. ## Downloading the Model Supported models: Model Platform Memory Requirement Description [Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448 maixcam2](https://huggingface.co/sipeed/Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448 maixcam2) MaixCAM2 4G Refer to the [Large Model User Guide](./basic.html) to download the model. ## Running the Model with MaixPy ```python from maix import app, nn, err, image, display, time import requests import json model \"/root/models/Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448 maixcam2/model.mud\" disp display.Display() qwen3_vl nn.Qwen3VL(model) in_w qwen3_vl.input_width() in_h qwen3_vl.input_height() in_fmt qwen3_vl.input_format() print(f\"input size: {in_w}x{in_h}, format: {image.format_name(in_fmt)}\") def on_reply(obj, resp): print(resp.msg_new, end \"\") qwen3_vl.set_system_prompt(\"You are Qwen3VL. You are a helpful vision to text assistant.\") qwen3_vl.set_reply_callback(on_reply) # load and set image img image.load(\"/maixapp/share/picture/2024.1.1/ssd_car.jpg\", format in_fmt) qwen3_vl.set_image(img, fit image.Fit.FIT_CONTAIN) # if size not math, will auto resize first disp.show(img) while not app.need_exit(): print('wait model is ready') if qwen3_vl.is_ready(): break time.sleep(1) def example1(): print('') # set prompt msg \"请描述图中有什么\" print(\">>\", msg) resp qwen3_vl.send(msg) err.check_raise(resp.err_code) def example2(): print('') msg \"Describe the picture\" print(\">>\", msg) resp qwen3_vl.send(msg) err.check_raise(resp.err_code) example1() example2() del qwen3_vl # Must release vlm object ``` Result: ``` >> 请描述图中有什么 好的，这是一张在城市街道上拍摄的照片。以下是图中包含的详细信息： 这张照片的主体是一位站在红车前的女性，背景是城市街道和建筑。 **前景中的女性**：一位女性，她站在画面的前景中央。她有深色的头发，穿着一件深色的外套。她正看着镜头，似乎正准备拍照。 **背景中的红车**：在女性的后方，是一辆红色的“大众”（Volkswagen）汽车的前部。这辆车停在一条小巷或路边，它的前脸部分被遮挡，但可以清楚地看到它红色的车身和前大灯。 **背景中的建筑物**：在车辆后方是几座多层的建筑，看起来是城市中的居民楼或办公楼。这些建筑的外立面是浅色的，带有许多窗户，窗户的大小和排列方式不同。 **照片中的细节**：在图像的右上角，可以看到一辆黑色的汽车的后视镜，这可能是一辆停在远处的车辆。在图像的左上角，有一座建筑的窗户上有一个明显的“N”字母，这可能是一个窗户的标识或装饰。 **拍摄视角和构图**：这是一张从较低角度拍摄的风景照。摄影师可能使用了广角镜头，使建筑和街道的细节被放大。整体构图具有一定的对称性，以车辆和建筑为中心。 总的来说，这张照片展示了一个城市街景，其中一位女性在一辆红色的大众汽车前，而背景则是一排具有多个窗户的建筑。 >> Describe the picture A woman stands on the pavement in front of a red double decker bus in a city, likely London, given the distinctive bus and the architecture. She is wearing a black jacket and is looking towards the camera. The bus is parked on a street with a white painted line marking the curb. The background consists of buildings with classic architecture. ``` Here, an image is loaded from the system, and the model is asked to describe what is in the picture. Note that this model does not support context, meaning each call to the `send` function is a completely new conversation and does not remember the content from previous `send` calls. Additionally, the default model supports an input image resolution of `448 x 448`. Therefore, when calling `set_image`, if the resolution does not match, the `img.resize` method will be automatically called for scaling, using the method specified by `fit`. For example, `image.Fit.FIT_CONTAIN` means that when the aspect ratio of the input image does not match the expected resolution, the original aspect ratio is maintained during scaling, and the surrounding area is filled with black. `set_system_prompt` is the system prompt statement, which can be appropriately modified to improve accuracy in your application scenario. Note: For the model `Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448`, `P320` means that `the system prompt` and `user prompt` can only be filled with up to `320 tokens` in total. `CTX448` means the maximum total length for `the system prompt`, `user prompt`, and the model's reply combined is `448 tokens`. ### Calling the Model with HTTP ```python from maix import app, nn, err, image, display, time import requests import json model \"/root/models/Qwen3 VL 2B Instruct GPTQ Int4 AX630C P320 CTX448 maixcam2/model.mud\" disp display.Display() qwen3_vl nn.Qwen3VL(model) in_w qwen3_vl.input_width() in_h qwen3_vl.input_height() in_fmt qwen3_vl.input_format() print(f\"input size: {in_w}x{in_h}, format: {image.format_name(in_fmt)}\") def on_reply(obj, resp): print(resp.msg_new, end \"\") qwen3_vl.set_system_prompt(\"You are Qwen3VL. You are a helpful vision to text assistant.\") qwen3_vl.set_reply_callback(on_reply) # load and set image img image.load(\"/maixapp/share/picture/2024.1.1/ssd_car.jpg\", format in_fmt) qwen3_vl.set_image(img, fit image.Fit.FIT_CONTAIN) # if size not math, will auto resize first disp.show(img) while not app.need_exit(): print('wait model is ready') if qwen3_vl.is_ready(): break time.sleep(1) def example3(): print('') url \"http://127.0.0.1:12346\" headers { \"Content Type\": \"application/json\", } stream True data { \"model\": \"AXERA TECH/Qwen3 VL 2B Instruct GPTQ Int4\", \"stream\":stream, \"temperature\":0.7, \"repetition_penalty\":1, \"top p\":0.8, \"top k\":20, \"messages\": [{ \"role\":\"user\", \"content\": [{ \"type\":\"text\", \"text\":\"What is your name?\" }, { \"type\":\"image_url\", \"image_url\":\"images/demo.jpg\" }] }] } response requests.post(url + '/v1/chat/completions', headers headers, json data, stream stream) if not stream: print(response.status_code) print(response.text) else: if response.status_code 200: for line in response.iter_lines(): if line: line line.decode('utf 8') if line.startswith('data: '): data_str line[6:] if data_str.strip() '[DONE]': print(\"\\nStreaming finished\") break try: chunk json.loads(data_str) if 'choices' in chunk and len(chunk['choices']) > 0: delta chunk['choices'][0].get('delta', {}) if 'content' in delta: print(delta['content'], end '', flush True) except json.JSONDecodeError: continue else: print(f\"Request failed: {response.status_code}\") print(response.text) example3() del qwen3_vl # Must release vlm object ``` Result: ``` I am an AI assistant without a name. I am a virtual assistant capable of helping you answer questions, provide information, and engage in beneficial discussions. Streaming finished ``` Qwen3 VL supports an OpenAI style interface, allowing you to obtain the model's output results via HTTP protocol streaming. ## Custom Quantized Model The model provided above is a quantized model for MaixCAM2. If you want to quantize your own model, refer to: * [Pulsar2 Documentation](https://pulsar2 docs.readthedocs.io/zh cn/latest/appendix/build_llm.html) * Original model: https://huggingface.co/Qwen/Qwen3 VL 2B Instruct"},"/maixpy/doc/en/README_MaixCAM.html":{"title":"MaixCAM MaixPy Quick Start","content":" title: MaixCAM MaixPy Quick Start <style> #head_links table { width: 100%; display: table; } @media screen and (max width: 900px){ #head_links th, #head_links td { /* padding: 8px; */ font size: 0.9em; padding: 0.1em 0.05em; } } </style> ## Getting Started ### Prepare the TF Image Card and Insert it into the Device If the package you purchased includes a TF card, it already contains the factory image. If the TF card was not installed in the device at the factory, you will first need to carefully open the case (be careful not to tear the ribbon cables inside) and then insert the TF card. Additionally, since the firmware from the factory may be outdated, it is highly recommended to follow the instructions on [Upgrading and Flashing the System](./basic/os.html) to upgrade the system to the latest version. If you did not purchase a TF card, you need to flash the system onto a self provided TF card. Please refer to [Upgrading and Flashing the System](./basic/os.html) for the flashing method, and then install it on the board. ### Power On Use a `Type C` data cable to connect the `MaixCAM` device and power it on. Wait for the device to boot up and enter the function selection interface. ![maixcam_font](../../static/image/maixcam_font.png) If the screen does not display: * Please confirm that you purchased the bundled TF card. If you confirm that you have a TF card and it is inserted into the device, you can try <a href \"./basic/os\" target \"_blank\">updating to the latest system</a>. * If you did not purchase the TF card bundle, you need to follow the instructions in <a href \"./basic/os\" target \"_blank\">updating to the latest system</a> to flash the latest system onto the TF card. * Also, ensure that the screen and camera cables are not loose. The screen cable can easily come off when opening the case, so be careful. ### Connect to the Network For the first run, you need to connect to the network, as you will need it later to activate the device and use the IDE. If you don't have a router, you can use your phone to open a hotspot. Click `Settings` on the device and select `WiFi`. There are two ways to connect to the `WiFi` hotspot: * Scan the WiFi sharing code: * Use your phone to share the `WiFi` hotspot QR code, or go to [maixhub.com/wifi](https://maixhub.com/wifi) to generate a QR code. * Click the `Scan QR code` button, the camera screen will appear, scan the QR code generated previously to connect. * Search for hotspots: * Click the `Scan` button to start scanning the surrounding `WiFi`, you can click multiple times to refresh the list. * Find your WiFi hotspot. * Enter the password and click the `Connect` button to connect. Then wait for the `IP` address to be obtained, which may take `10` to `30` seconds. If the interface does not refresh, you can exit the `WiFi` function and re enter to view it, or you can also see the `IP` information in `Settings` > `Device Information`. ### Update the Runtime Libraries **This step is very important!!!** If this step is not done properly, other applications and functions may not work (e.g., they may crash). * First, ensure that you have completed the previous step of connecting to WiFi and have obtained an IP address to access the internet. * On the device, click `Settings`, and select `Install Runtime Libraries`. * After the installation is complete, you will see that it has been updated to the latest version. Then exit. If it shows `Request failed` or `请求失败` (Request failed), please first check if the network is connected. You need to be able to connect to the internet. If it still doesn't work, please take a photo and contact customer service for assistance. ### Use Built in Applications Many applications are built in, such as Find Blobs, AI Detector, Line Follower, etc. For example, Find Blobs: <video playsinline controls autoplay loop muted preload class \"pl 6 pb 4 self end\" src \"/static/video/self_learn_tracker.mp4\" type \"video/mp4\" style \"width:100%\"> Classifier Result video </video> Please explore other applications on your own. More applications will be updated in the future. For usage documentation and application updates, please see the [MaixHub App Store](https://maixhub.com/app). **Note: The applications only include a part of the functionality that MaixPy can achieve. Using MaixPy, you can create even more features.** ### Logging into the Terminal If you need to log into the terminal, the default username for `MaixCAM` is `root`, and the password is `root`. ## Use as a Serial Module > If you want to use the device as the main controller (or if you don't understand what a serial module is), you can skip this step. The built in applications can be used directly as serial modules, such as `Find Blobs`, `Find Faces`, `Find QR Codes`, etc. Note that the serial port can only directly connect to other microcontrollers. **If you want to communicate with a computer via a serial port, you must provide a USB to serial module yourself.** Usage: * Hardware connection: You can connect the device to the `Type C one to two mini board`(For MaixCAM Pro is 6Pin interface), which allows you to connect the device via serial to your main controller, such as `Arduino`, `Raspberry Pi`, `STM32`, etc. * Open the application you want to use, such as QR code recognition. When the device scans a QR code, it will send the result to your main controller via serial. > The serial baud rate is `115200`, the data format is `8N1`, and the protocol follows the [Maix Serial Communication Protocol Standard](https://github.com/sipeed/MaixCDK/blob/master/docs/doc/convention/protocol.md). You can find the corresponding application introduction on the [MaixHub APP](https://maixhub.com/app) to view the protocol. > If APP no serial output, you can also do it by yourself, follow function examples and [UART usage doc](./peripheral/uart.html) to add function and serial output. ## Preparing to Connect Computer and Device To enable communication between the computer (PC) and the device (MaixCAM), we need to ensure they are on the same local area network. There are two methods to achieve this: * **Method 1 (Highly Recommended)**: Wireless Connection. Connect the device to the same router or Wi Fi hotspot that the computer is connected to via Wi Fi. Go to the device's `Settings > WiFi Settings` and connect to your Wi Fi. (If you experience **screen lag or high latency** with Wi Fi, you can try Method 2 for a wired connection.) Here is the translation: * **Method Two**: Wired Connection. The device connects to the computer via a USB cable, and the device will emulate as a USB network adapter. This way, the device and the computer will be on the same local network through the USB connection. It is recommended to start with WiFi because although a wired connection offers stable transmission, it may encounter issues such as faulty cables, poor connection, or driver problems. If you encounter any issues, you can refer to the common problems in the [FAQ](./faq.html). .. details::Method Two: Driver Installation on Different Computer Systems: :open: true By default, there are two types of USB virtual network adapter drivers (NCM and RNDIS drivers) to meet the needs of different systems. You can also disable the unused virtual network adapter on the device under `Settings` > `USB Settings`: * **Windows**: All Windows systems will automatically install the RNDIS driver, while only Windows 11 will automatically install the NCM driver. As long as **one of the drivers works**, it is sufficient. * Open Task Manager > Performance, and you should see a virtual Ethernet with an IP address such as `10.131.167.100`, which is the computer's IP address. The device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. If you are using Windows 11, you will see two virtual network adapters; you can use either IP address. * Additionally, you can open `Device Manager` (search for `Device Manager` in the search bar). The RNDIS and NCM drivers should be correctly installed, as shown below: ![RNDIS ok](../../static/image/windows_rndis_ok.png) ![NCM ok](../../static/image/windows_ncm_ok.png) * **Linux**: No additional setup is required. Simply plug in the USB cable. Use `ifconfig` or `ip addr` to see the `usb0` and `usb1` network interfaces, and either IP address can be used. **Note**: The IP address you see, such as `10.131.167.100`, is the computer's IP address, and the device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. * **MacOS**: Check for the `usb` network adapter under `System Settings` > `Network`. **Note**: The IP address you see, such as `10.131.167.100`, is the computer's IP address, and the device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. ## Preparing the Development Environment * First, ensure that the computer and the device are on the same local network as per the previous step. * Download and install [MaixVision](https://wiki.sipeed.com/maixvision). * Connect the device and the computer using a Type C cable. Open MaixVision, click the `“Connect”` button in the lower left corner, and it will automatically search for the device. Wait for a moment until the device appears, then click the connection button next to the device to connect. If **no device is detected**, you can also manually enter the device's IP address in the **device**'s `Settings > Device Info`. You can also find solutions in the [FAQ](./faq.html). **After a successful connection, the function selection interface on the device will disappear, and the screen will turn black, releasing all hardware resources. If there is still an image displayed, you can disconnect and reconnect.** Here is a video example of using MaixVision: <video style \"width:100%\" controls muted preload src \"/static/video/maixvision.mp4\"></video> ## Run Examples Click `Example Code` on the left side of MaixVision, select an example, and click the `Run` button in the bottom left to send the code to the device for execution. For example: * `hello_maix.py`: Click the `Run` button, and you will see messages printed from the device in the MaixVision terminal, as well as an image in the upper right corner. * `camera_display.py`: This example will open the camera and display the camera view on the screen. ```python from maix import camera, display, app disp display.Display() # Construct a display object and initialize the screen cam camera.Camera(640, 480) # Construct a camera object, manually set the resolution to 640x480, and initialize the camera while not app.need_exit(): # Keep looping until the program exits (you can exit by pressing the function key on the device or clicking the stop button in MaixVision) img cam.read() # Read the camera view and save it to the variable img, you can print(img) to print the details of img disp.show(img) # Display img on the screen ``` * `yolov5.py` will detect objects in the camera view, draw bounding boxes around them, and display them on the screen. It supports detection of 80 object types. For more details, please see [YOLOv5 Object Detection](./vision/yolov5.html). You can try other examples on your own. > If you encounter image display stuttering when using the camera examples, it may be due to poor network connectivity, or the quality of the USB cable or the host's USB being too poor. You can try changing the connection method or replacing the cable, host USB port, or computer. ## Install Applications on the Device The above examples run code on the device, but the code will stop running when `MaixVision` is disconnected. If you want the code to appear in the boot menu, you can package it as an application and install it on the device. Click the `Install App` button in the bottom left corner of `MaixVision`, fill in the application information, and the application will be installed on the device. Then you will be able to see the application on the device. You can also choose to package the application and share your application to the [MaixHub App Store](https://maixhub.com/app). > The default examples do not explicitly write an exit function, so you can exit the application by pressing the function key on the device. (For MaixCAM, it is the user key.) If you want the program to start automatically on boot, you can set it in `Settings > Boot Startup`. More MaixVision usage refer to [MaixVision documentation](./basic/maixvision.html)。"},"/maixpy/doc/en/comm/modbus.html":{"title":"MaixCAM MaixPy Using Modbus Protocol","content":" title: MaixCAM MaixPy Using Modbus Protocol ## Introduction to Modbus Modbus is an application layer bus protocol that operates over UART or TCP as the transport layer. It allows multiple devices to connect to a single bus, enabling one to many communication. ## Differences Between Modbus and Maix Application Communication Protocol * **Maix Application Communication Protocol**: * **Communication Type**: One to one communication. * **Communication Mode**: Full duplex, allowing both parties to actively send messages for more real time interaction. * **Data Flexibility**: No restrictions on data length or type, supporting flexible data structures. * **MaixCAM MaixPy integrated**: By default some MaixCAM/MaixPy APP impleted this protocol, you can directly use it, and use same protocol is good for MaixCAM/MaixPy ecosystem health. * **Application Scenarios**: Suitable for one to one scenarios requiring high real time performance and bidirectional data transmission, such as AI inference result transmission and control command feedback. And communicate with MaixCAM MaixPy's applications. * **Modbus**: * **Communication Type**: A bus protocol supporting one to many communication. * **Communication Mode**: Only the master can initiate read/write operations. The slave's data updates are obtained by the master's polling mechanism, and slaves can essentially be regarded as sensors with multiple groups of registers. * **Data Type**: Data is organized into registers. Slaves have multiple readable/writable or read only registers for data exchange. * **Application Scenarios**: Suitable for industrial automation scenarios where data collection and monitoring of sensors or devices are needed, especially in master slave structured systems. ## Using Modbus with MaixCAM MaixPy MaixPy supports the Modbus protocol, including both master and slave modes, as well as RTU (UART) and TCP modes. The implementation is based on the open source project [libmodbus](https://libmodbus.org/). ## MaixCAM as a Modbus Slave When acting as a slave, MaixCAM can be seen as a module with several groups of readable/writable registers. The registers include the following types, which differ in value type and read/write permissions: * **`coils` Registers**: Boolean values, readable and writable. * **`discrete input` Registers**: Boolean values, readable but not writable. * **`input registers`**: 16 bit integer values, readable but not writable. * **`holding registers`**: 16 bit integer values, readable and writable. The address and length of each register group can be freely specified during slave initialization based on application requirements. Below are examples. For more code, refer to the source examples (`examples/protocol/comm_modbus_xxx.py`). ### RTU (UART): ```python from maix.comm import modbus from maix import app, err slave modbus.Slave( modbus.Mode.RTU, # Set Modbus mode to RTU \"/dev/ttyS0\", # Specify the UART port for communication with the master 0x00, 10, # Start address and number of registers for coils 0x00, 10, # Start address and number of registers for discrete input 0x00, 10, # Start address and number of registers for input registers 0x00, 10, # Start address and number of registers for holding registers 115200, 1, # Baud rate of 115200, default 8N1; the last `1` is the slave address for RTU 0, False # TCP port (irrelevant for RTU), debug flag ) \"\"\" Example: Coils register group Start address: 0x00 Number: 10 This means the coils register group ranges from 0x00 to 0x09, with each register storing a Boolean value. \"\"\" # Read all values from the input registers group old_ir slave.input_registers() print(\"Old input registers:\", old_ir) # Update values in the input registers group starting from index 2 # New register values: [0x00, 0x00, 0x22, 0x33, 0x44, 0x00, 0x00, 0x00, 0x00, 0x00] data: list[int] [0x22, 0x33, 0x44] slave.input_registers(data, 2) # Read and verify updated values new_ir slave.input_registers() print(\"New input registers:\", new_ir) while not app.need_exit(): # Wait for the master's read/write operation if err.Err.ERR_NONE ! slave.receive(2000): # Timeout: 2000ms continue # Determine the type of operation requested by the master rtype slave.request_type() if rtype modbus.RequestType.READ_HOLDING_REGISTERS: print(\"Master requested to read holding registers\") hr slave.holding_registers() print(\"Current holding registers:\", hr) # Update holding registers with new values hr [x + 1 for x in hr] slave.holding_registers(hr) # Automatically handle the master's request and update register values slave.reply() ``` ### TCP: ```python from maix.comm import modbus from maix import app, err slave modbus.Slave( modbus.Mode.TCP, # Set mode to TCP \"\", # Leave blank for TCP mode 0x00, 10, # Start address and number of registers for coils 0x00, 10, # Start address and number of registers for discrete input 0x00, 10, # Start address and number of registers for input registers 0x00, 10, # Start address and number of registers for holding registers 0, 1, # TCP port; the last parameter is the debug flag ) # The following code is identical to the RTU example old_ir slave.input_registers() print(\"Old input registers:\", old_ir) data: list[int] [0x22, 0x33, 0x44] slave.input_registers(data, 2) new_ir slave.input_registers() print(\"New input registers:\", new_ir) while not app.need_exit(): if err.Err.ERR_NONE ! slave.receive(2000): # Timeout: 2000ms continue rtype slave.request_type() if rtype modbus.RequestType.READ_HOLDING_REGISTERS: print(\"Master requested to read holding registers\") hr slave.holding_registers() print(\"Current holding registers:\", hr) hr [x + 1 for x in hr] print(\"Updated holding registers:\", hr) slave.holding_registers(hr) slave.reply() ``` As shown above, calling `slave.reply()` after receiving a read request from the master automatically replies with the requested data. The example also demonstrates how to modify the register values on the slave side. For detailed information on the Modbus API, refer to the [Modbus API Documentation](../../../api/maix/comm/modbus.html). ## MaixCAM MaixPy as Modbus Master As a master, MaixCAM can actively read and write data from/to slaves. Below is an example (refer to the source example `examples/protocol/comm_modbus_xxx.py` for more details): ```python from maix import pinmap, app, err, time, thread from maix.comm import modbus REGISTERS_START_ADDRESS 0x00 # Start address for registers REGISTERS_NUMBER 10 # Number of registers to read RTU_SLAVE_ID 1 # Slave ID RTU_BAUDRATE 115200 # Baud rate for UART communication def master_thread(*args): # Initialize UART1 for Modbus communication if pinmap.set_pin_function(\"A19\", \"UART1_TX\") ! err.Err.ERR_NONE: print(\"Failed to initialize UART1 TX!\") exit( 1) if pinmap.set_pin_function(\"A18\", \"UART1_RX\") ! err.Err.ERR_NONE: print(\"Failed to initialize UART1 RX!\") exit( 1) # Optional: Enable debugging for Modbus master # modbus.set_master_debug(True) # Create a Modbus master instance with RTU mode master modbus.MasterRTU( \"/dev/ttyS1\", # UART device for communication RTU_BAUDRATE # Baud rate ) while not app.need_exit(): # Read holding registers from the slave hr master.read_holding_registers( RTU_SLAVE_ID, # Slave ID REGISTERS_START_ADDRESS, # Starting address of registers REGISTERS_NUMBER, # Number of registers to read 2000 # Timeout in milliseconds ) # Check if the read operation was successful if len(hr) 0: continue # Print the read data print(\"Master read holding registers:\", hr) # Wait for 1 second before the next read time.sleep(1) # Start the master thread master_thread(None) ``` This example demonstrates using UART1 as the master to read register values from a slave device."},"/maixpy/doc/en/comm/maix_protocol.html":{"title":"MaixCAM MaixPy Maix Application Communication Protocol","content":" title: MaixCAM MaixPy Maix Application Communication Protocol ## Communication Protocol Overview In order for two devices to communicate stably, there are typically several layers, starting from the bottom: * **Hardware Layer**: For example, `UART` uses three wires: `TX`, `RX`, and `GND`. It can also be wireless, such as with WiFi. * **Transport Layer**: Uses transmission control protocols to ensure stable data transmission, such as the `UART` protocol, which specifies baud rate, stop bits, and parity bits to ensure correct data transmission. The `TCP` protocol works similarly. * **Application Layer**: The data obtained from the transport layer is stream data (simply put, a long string of data without punctuation). To help the application understand what the data means, the application typically defines its own communication protocol for the application layer to structure the transmitted content (you can think of this as adding punctuation to the stream of data to make it easier for the receiver to understand the sentence structure). For example: The application layer protocol specifies that a data packet begins with a `$` symbol. A sends two data packets to B: `$12345$67890`. After receiving it, B knows that A has sent two packets, `12345` and `67890`. Without this protocol, if A sends `12345` followed by `67890`, since the data is transmitted as a stream, B might receive `1234567890`, making it unclear whether one or two packets were sent. ## Character Protocol vs. Binary Protocol **Character Protocol**: In the previous example, we used `$` to mark the beginning of each packet. If we want to send the number `123`, we simply send the string `$123`, which is human readable. The receiver needs to convert the string `123` to an `int` type. For example, in C language: ```c int value; sscanf(buff, \"$%d\", &value); ``` **Binary Protocol**: In a character protocol, sending the number `123` takes 4 bytes, and the receiver has to parse the string to convert it into an integer type. In a binary protocol, we can reduce the number of bytes transmitted and make it easier for the receiver to handle. We can simply send `0x24 0x7B`. `0x24` is the hexadecimal representation of the `$` symbol (refer to the ASCII table), and `0x7B` is the hexadecimal representation of the decimal number `123`. This uses only two bytes to transmit the same information that required 4 bytes in the character protocol. The receiver can directly read the second byte `0x7B` and use its value, such as in C language: ```c uint8_t value buff[1]; ``` This is just a simple explanation to help you understand the two protocols. In practice, each has its advantages depending on the specific use case, and other factors, such as checksum values, may also be considered. You can explore more and learn about it in the Maix Communication Protocol Practice section below. ## Maix Application Communication Protocol The Maix Application Communication Protocol is an application layer protocol that uses UART or TCP as the transport layer. It defines how the two parties communicate and the format in which data is transmitted, making it easier to parse and identify information. It is a binary protocol that includes frame headers, data content, checksums, etc. The complete protocol definition is available in the [Maix Application Communication Protocol Standard](https://wiki.sipeed.com/maixcdk/doc/convention/protocol.html) (included in the MaixCDK documentation because MaixCDK also uses this protocol). If you have no prior experience with communication protocols, it might seem a bit difficult, but by reviewing the examples below a few times, you should be able to understand it. In `MaixPy`, the API is already encapsulated, making it very simple to use. For other microcontrollers or chips, you may need to implement this protocol yourself, and you can refer to the appendix of the [Maix Application Communication Protocol Standard](https://wiki.sipeed.com/maixcdk/doc/convention/protocol.html) to check for any corresponding implementations. For example, if we are performing object detection and want to send the detected object information (such as type and coordinates) via UART to another device (e.g., STM32 or Arduino microcontroller), here’s how it can be done. Full Example: [MaixPy/examples/protocol/comm_protocol_yolov5.py](https://github.com/sipeed/MaixPy/tree/main/examples/protocol/comm_protocol_yolov5.py) First, we need to detect the objects, based on the `yolov5` detection example. Here, we’ll skip other details and focus on how the detection results look: ```python while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` You can see that `objs` is a list of detection results. Here, we draw bounding boxes on the screen, but we can also send these results via UART. We don't need to manually initialize the UART; we can directly use the built in `maix.comm, maix.protocol` modules. By calling `comm.CommProtocol`, the UART is automatically initialized, with a default baud rate of `115200`. Communication protocol settings can be configured in the device's `System Settings > Communication Protocol`. There might also be other communication methods, such as `TCP`, with the default set to `UART`. You can check the current setting by using `maix.app.get_sys_config_kv(\"comm\", \"method\")`. ```python from maix import comm, protocol, app from maix.err import Err import struct def encode_objs(objs): ''' Encode objects info to bytes body for protocol 2B x(LE) + 2B y(LE) + 2B w(LE) + 2B h(LE) + 2B idx ... ''' body b\"\" for obj in objs: body + struct.pack(\"<hhHHH\", obj.x, obj.y, obj.w, obj.h, obj.class_id) return body APP_CMD_ECHO 0x01 # Custom command 1, for testing, not used here, just reserved APP_CMD_DETECT_RES 0x02 # Custom command 2, send detected object information # You can define more custom commands for your application p comm.CommProtocol(buff_size 1024) while not app.need_exit(): # ... objs detector.detect(img, conf_th 0.5, iou_th 0.45) if len(objs) > 0: body encode_objs(objs) p.report(APP_CMD_DETECT_RES, body) # ... ``` In the `encode_objs` function, we pack the detected object information into a `bytes` type data. Then, using the `p.report` function, we send the result. The `body` content is simply defined as `2B x(LE) + 2B y(LE) + 2B w(LE) + 2B h(LE) + 2B idx ...`, where: * For each object, we send 10 bytes (2 bytes for each of the `x`, `y`, `w`, `h`, and `class_id` coordinates). The `x` coordinate is represented as a short integer, encoded in little endian (LE) format. * The loop encodes all objects and concatenates them into one byte array (`bytes`). When calling the `report` function, the protocol header, checksum, and other details are automatically added at the lower level, resulting in a complete data frame ready to be received. On the receiving end, you can decode the data according to the protocol. If the receiver is also using MaixPy, you can directly use: ```python while not app.need_exit(): msg p.get_msg() if msg and msg.is_report and msg.cmd APP_CMD_DETECT_RES: print(\"Received objects:\", decode_objs(msg.get_body())) p.resp_ok(msg.cmd, b'1') ``` If the device is something like an STM32 or Arduino, you can refer to the C language functions in the appendix of the [Maix Application Communication Protocol Standard](https://wiki.sipeed.com/maixcdk/doc/convention/protocol.html) for encoding and decoding."},"/maixpy/doc/en/basic/auto_start.html":{"title":"MaixPy/MaixCAM Application Auto-Start at Boot","content":" title: MaixPy/MaixCAM Application Auto Start at Boot Packaged applications can be set to automatically start when the device boots up, bypassing the application menu and directly launching the specified application. ## Method One for Setting Application Auto Start First, package and install the application, then go to `Settings > Auto Start` on your device to select the application you want to auto start. To cancel auto start, you can also adjust it here. ## Method Two for Setting Application Auto Start Run the Python script to set up, and modify the `new_autostart_app_id` variable in the script to the `app_id` you want to set. All installed `app_id`s will be printed out when you run the script, so you can run it once to find the desired `app_id`, modify the variable, and then run it again. To cancel the autostart setting, set it to `None`. This script can also be found in the `MaixPy` examples under `examples/tools` as `set_autostart.py`: ```python import configparser, os def parse_apps_info(): info_path \"/maixapp/apps/app.info\" conf configparser.ConfigParser() conf.read(info_path) version conf[\"basic\"][\"version\"] apps {} for id in list(conf.keys()): if id in [\"basic\", \"DEFAULT\"]: continue apps[id] conf[id] return apps def list_apps(): apps parse_apps_info() print(f\"APP num: {len(apps)}\") for i, (id, info) in enumerate(apps.items()): name_zh info.get(\"name[zh]\", \"\") print(f\"{i + 1}. [{info['name']}] {name_zh}:\") print(f\" id: {id}\") print(f\" exec: {info['exec']}\") print(f\" author: {info['author']}\") print(f\" desc: {info['desc']}\") print(f\" desc_zh: {info.get('desc', 'None')}\") print(\"\") def get_curr_autostart_app(): path \"/maixapp/auto_start.txt\" if os.path.exists(path): with open(path, \"r\") as f: app_id f.readline().strip() return app_id return None def set_autostart_app(app_id): path \"/maixapp/auto_start.txt\" if not app_id: if os.path.exists(path): os.remove(path) return with open(path, \"w\") as f: f.write(app_id) if __name__ \"__main__\": # new_autostart_app_id \"settings\" # change to app_id you want to set new_autostart_app_id None # remove autostart list_apps() print(\"Before set autostart appid:\", get_curr_autostart_app()) set_autostart_app(new_autostart_app_id) print(\"Current autostart appid:\", get_curr_autostart_app()) ``` ## Method Three for Setting Application Auto Start You can also modify the `/maixapp/auto_start.txt` file in your device to set it up. For methods on file transfer, refer to the previous documentation. * First, determine the `id` of the application you want to set. This is set when you package the application; if it's not an application you packaged yourself, you can install it on the device and check the folder names under the device's `/maixapp/apps/` directory, which are the application names (or you can download and check the device's `/maixapp/apps/app.info` file, where the application `id` is indicated inside the `[]` brackets). * Then write the `id` into the `/maixapp/auto_start.txt` file. (You can create the file locally on your computer, and then transfer it to the device using `MaixVision`.) * To cancel, delete the `/maixapp/auto_start.txt` file on the device. ## Other Methods Since MaixCAM runs on a Linux based system, if you're familiar with Linux, you can also configure system startup by directly editing startup scripts: * For **MaixCAM/MaixCAM Pro**, edit `/etc/rc.local` or startup scripts under `/etc/init.d`. * For **MaixCAM2**, which uses **systemd** for startup management, add a service file under `/etc/systemd/system`, then enable it with `systemctl enable xxxx.service`. You can refer to `launcher.service`, which is the default launcher program. > ⚠️ However, note that this approach **prevents MaixVision from stopping the running application during connection**, which may cause **resource conflicts** (e.g., screen or camera already in use), and **MaixVision might fail to run programs** properly. In contrast, the first two application auto start methods allow MaixVision to **gracefully stop the running app** when connecting to the device. Therefore, this method is **more suitable for background processes** that don’t require access to the screen or camera. If you're not familiar with Linux, this approach is **not recommended**, as it can easily lead to resource conflicts involving the screen or camera."},"/maixpy/doc/en/peripheral/pwm.html":{"title":"MaixCAM MaixPy Using PWM","content":" title: MaixCAM MaixPy Using PWM update: date: 2025 08 08 author: Neucrack version: 1.1.0 content: Refactored document, easier for beginners to understand ## Prerequisites Please first learn how to use the [pinmap](./pinmap.html) module to set pin functions. To enable a pin for `PWM` functionality, first use `pinmap` to set the corresponding pin function to `PWM`. ## Introduction to PWM Using `PWM`, you can output a square wave from a pin. By setting an appropriate period and duty cycle (the proportion of low level in the entire period), it can serve different purposes, such as: * Controlling servo motor direction. * Controlling the rotation speed of a brushless motor. * Adjusting light brightness (PWM dimming). For more basic knowledge about PWM, there are many good tutorials online. This article will not go into detail—please search and learn on your own. ## Choosing the Right PWM to Use First, we need to know which pins and PWM channels the device provides, as shown in the table: Device Model Pin Diagram Pin Multiplexing Description MaixCAM ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) On the silkscreen, for example, `A19` is the pin name and `PWM7` is the function name MaixCAM Pro ![maixcam_pro_io](/static/image/maixcam_pro_io.png) The first name, such as `A19`, is the pin name; the corresponding `PWM7` is the function name MaixCAM2 ![maixcam2_io](https://wiki.sipeed.com/hardware/assets/maixcam/maixcam2_pins.jpg) The first name, such as `B25`, is the pin name; the corresponding `PWM6` is the function name Note that pins may be used for other purposes by default; it’s best to avoid those pins. See the [pinmap](./pinmap.html) documentation for details. For example: * `MaixCAM/MaixCAM Pro`: Since `WiFi` uses all pins of `SDIO1`, `PWM4~9` are not recommended for use. * `MaixCAM2`: By default, 4 PWM pins can be used directly. The lighting LED can also be controlled by `PWM6`, but note that `PWM6` can only be used for either `A30` or the lighting LED at one time, not both. ## Using PWM in MaixPy to Control LED Brightness Using the persistence of human vision, keeping the LED always on will make it brightest, while switching it on and off rapidly can control brightness—the longer the off time proportion, the dimmer it appears. ```python from maix import pwm, time, pinmap, sys, err # get pin and pwm number according to device id device_id sys.device_id() if device_id \"maixcam2\": pin_name \"B25\" # LED light pwm_id 6 else: pin_name \"A18\" # A18 pin pwm_id 6 # set pinmap err.check_raise(pinmap.set_pin_function(pin_name, f\"PWM{pwm_id}\"), \"set pinmap failed\") SERVO_PERIOD 100000 # 100kHz 0.01ms out pwm.PWM(pwm_id, freq SERVO_PERIOD, duty 0, enable True) for i in range(100): print(i) out.duty(i) time.sleep_ms(100) for i in range(100): print(100 i) out.duty(100 i) time.sleep_ms(100) ``` ## Using PWM in MaixPy to Control a Servo Controlling the servo angle is done by adjusting the duty cycle—different duty cycles correspond to different angles. Refer to the servo’s documentation. Here, we control the servo to rotate from the minimum angle to the maximum angle and then back to the minimum: ```python from maix import pwm, time, pinmap, err SERVO_PERIOD 50 # 50Hz 20ms SERVO_MIN_DUTY 2.5 # 2.5% > 0.5ms SERVO_MAX_DUTY 12.5 # 12.5% > 2.5ms # get pin and pwm number according to device id device_id sys.device_id() if device_id \"maixcam2\": pin_name \"A31\" pwm_id 7 else: pin_name \"A19\" pwm_id 7 # set pinmap err.check_raise(pinmap.set_pin_function(pin_name, f\"PWM{pwm_id}\"), \"set pinmap failed\") def angle_to_duty(percent): return (SERVO_MAX_DUTY SERVO_MIN_DUTY) * percent / 100.0 + SERVO_MIN_DUTY out pwm.PWM(pwm_id, freq SERVO_PERIOD, duty angle_to_duty(0), enable True) for i in range(100): out.duty(angle_to_duty(i)) time.sleep_ms(100) for i in range(100): out.duty(angle_to_duty(100 i)) time.sleep_ms(100) ``` ## More Examples See [MaixPy examples](https://github.com/sipeed/MaixPy/tree/main/examples/peripheral/pwm). ## API Documentation For more APIs, see the [PWM API documentation](https://wiki.sipeed.com/maixpy/api/maix/peripheral/pwm.html)"},"/maixpy/doc/en/peripheral/i2c.html":{"title":"MaixCAM MaixPy Using I2C","content":" title: MaixCAM MaixPy Using I2C update: date: 2025 08 08 author: Neucrack version: 1.1.0 content: Refactored the documentation to be more understandable for beginners ## Prerequisites Please learn how to use the [pinmap](https://www.google.com/search?q ./pinmap.md) module to set pin functions first. To enable an pin for `I2C` functionality, first use `pinmap` to set the function of the corresponding pin to `I2C`. ## I2C Introduction `I2C` uses only two pins, `SCL` and `SDA`, to achieve bus communication. This allows one master to connect to multiple slaves, enabling convenient one to many communication. It is commonly used for: * Reading sensor data, such as temperature and humidity, IMU, and touchscreens. * Controlling devices, such as setting camera parameters. * Communication between two devices. There are many good tutorials on the basics of I2C online. This article will not go into detail; please search and learn on your own. ## Choosing the Right I2C to Use First, we need to know which pins and I2C interfaces the device has, as shown in the figure: Device Model Pinout Diagram Pin Multiplexing Description MaixCAM The silkscreen on the board, e.g., `A15`, is the pin name, and `I2C5_SCL` is the function name MaixCAM Pro The first name, e.g., `A15`, is the pin name, and `I2C5_SCL` is the corresponding function name MaixCAM2 The first name, e.g., `A1`, is the pin name, and `I2C6_SCL` is the corresponding function name It's important to note that pins might have other default uses. It's best to avoid these pins. Please refer to the [pinmap](https://www.google.com/search?q ./pinmap.md) documentation for details. For example: * For `MaixCAM / MaixCAM Pro`, the `I2C1` and `I2C3` pins overlap with the WiFi module (SDIO1), so their use is not recommended. There is also an `I2C5`, which is a software simulated driver at the low level. It is recommended to use it as the underlying driver has been configured, and it works the same as a hardware `I2C`. * The `I2C6 / I2C7` pins on `MaixCAM2` are idle and can be used once `pinmap` is configured. ## Using I2C in MaixPy First, use `pinmap` to set the pin function to `I2C`, then initialize the `I2C` object: ```python from maix import i2c, pinmap, sys, err # get pin and i2c number according to device id device_id sys.device_id() if device_id \"maixcam2\": scl_pin_name \"A1\" scl_i2c_name \"I2C6_SCL\" sda_pin_name \"A0\" sda_i2c_name \"I2C6_SDA\" i2c_id 6 else: scl_pin_name \"A15\" scl_i2c_name \"I2C5_SCL\" sda_pin_name \"A27\" sda_i2c_name \"I2C5_SDA\" i2c_id 5 # set pinmap err.check_raise(pinmap.set_pin_function(scl_pin_name, scl_i2c_name), \"set pin failed\") err.check_raise(pinmap.set_pin_function(sda_pin_name, sda_i2c_name), \"set pin failed\") bus i2c.I2C(i2c_id, i2c.Mode.MASTER) slaves bus.scan() print(\"find slaves:\") for s in slaves: print(f\"{s}[0x{s:02x}]\") ``` ## More Examples See [MaixPy examples](https://github.com/sipeed/MaixPy/tree/main/examples/peripheral/i2c). ## API Documentation For more API information, see [i2c API documentation](https://wiki.sipeed.com/maixpy/api/maix/peripheral/i2c.html). ## Using Other Libraries Additionally, since it is a standard Linux driver, besides using the APIs provided by MaixPy, you can also use general Linux libraries like `smbus / smbus2`. Steps to use: * Run `pip install smbus` in the development board's terminal to install the library. * Use `pinmap` to map the pin functions. * Call `smbus` APIs to read from and write to `i2c`. ```python from maix import i2c, pinmap, sys, err import smbus # get pin and i2c number according to device id device_id sys.device_id() if device_id \"maixcam2\": scl_pin_name \"A1\" scl_i2c_name \"I2C6_SCL\" sda_pin_name \"A0\" sda_i2c_name \"I2C6_SDA\" i2c_id 6 else: scl_pin_name \"A15\" scl_i2c_name \"I2C5_SCL\" sda_pin_name \"A27\" sda_i2c_name \"I2C5_SDA\" i2c_id 5 # set pinmap err.check_raise(pinmap.set_pin_function(scl_pin_name, scl_i2c_name), \"set pin failed\") err.check_raise(pinmap.set_pin_function(sda_pin_name, sda_i2c_name), \"set pin failed\") bus smbus.SMBus(i2c_id) ```"},"/maixpy/doc/en/peripheral/key.html":{"title":"Using Key Events in MaixCAM MaixPy","content":" title: Using Key Events in MaixCAM MaixPy update: date: 2025 01 08 version: v1.0 author: neucrack content: Added documentation ## Introduction MaixCAM is equipped with built in buttons, including the user/ok button and the power button. By default, the user/ok button acts as an exit button in MaixPy. Pressing it automatically calls `maix.app.set_exit_flag(True)`. If your program relies on `maix.app.need_exit()` to determine when to exit, pressing the button will cause the program to terminate. Additionally, you can customize the button's behavior. Once customized, the default behavior of setting the exit flag will be disabled. ## Reading Key Events in MaixCAM MaixPy ### Default Behavior: Exit Flag Set When Button is Pressed The default behavior works without explicit function calls: ```python from maix import app, time cout 0 while not app.need_exit(): time.sleep(1) print(cout) cout + 1 ``` Pressing the button will exit the loop. ### Reading Key Events and Customizing Callback Functions You can use the `key.Key` class to handle key events. The callback function takes two parameters: `key_id`: The key's ID. Refer to [maix.key.Keys API Documentation](/api/maix/peripheral/key.html#Keys). For custom drivers, the ID depends on the driver. `state`: The key's state. Refer to [maix.key.State API Documentation](/api/maix/peripheral/key.html#State). Example: ```python from maix import key, app, time def on_key(key_id, state): ''' this func is called in a single thread ''' print(f\"key: {key_id}, state: {state}\") # key.c or key.State.KEY_RELEASED key_obj key.Key(on_key) while not app.need_exit(): time.sleep(1) ``` A more complete example: ```python from maix import image, key, app, display class App: def __init__(self): self.key_obj key.Key(self.on_key) self.disp display.Display() self.key_id 0 self.state 0 def on_key(self, key_id, state): ''' This function is called in a single thread ''' print(f\"key: {key_id}, state: {state}\") # e.g., key.c or key.State.KEY_RELEASED self.key_id key_id self.state state def run(self): while not app.need_exit(): img image.Image(self.disp.width(), self.disp.height(), image.Format.FMT_RGB888) msg f\"key: {self.key_id}, state: {self.state}\" img.draw_string(0, 10, msg, image.Color.from_rgb(255, 255, 255), 1.5) self.disp.show(img) App().run() ``` ## Customizing Key Drivers MaixPy is based on a Linux system, allowing you to write custom key drivers. You can add your custom key devices to the `key_devices` field in `/boot/board`. Multiple devices are supported and can be separated by commas, e.g.: `key_devices:/dev/my_keys1,/dev/my_keys2`. ### Driver Development Methods 1. **Kernel Driver** Follow the standard Linux driver development process, using a cross compilation toolchain to compile a `.ko` driver file. Load the driver automatically at boot via `/etc/init.d`. 2. **User Space Driver** For buttons connected to GPIO pins, you can write a user space driver using the GPIO API. Use `/dev/uinput` to export a device file. This approach requires a program to continuously read GPIO inputs. The program can be integrated into your main application or run as a standalone background process, offering flexibility."},"/maixpy/doc/en/peripheral/hid.html":{"title":"Introduction to Using MaixCAM MaixPy USB HID (as device)","content":" title: Introduction to Using MaixCAM MaixPy USB HID (as device) ## 简介 MaixPy currently supports simulate as USB HID device like keyboards, mice, and touchscreens, and the following is a guide on how to use maixPy to control your PC via HID. > If you want to use USB as host to connect HID devices into MaixCAM, set USB mode to HOST mode in `settings`. ## Preparation > MaixPy firmware version should be > 4.5.1. You must enable the HID device before operating HID, there are two ways: 1. Open the `Settings` application that comes with MaixCAM, click `USB Settings` in turn > tick the required HID devices, such as `Keyboard`, `Mouse`, `Touchscreen`, and then click `Confirm` , then restart MaixCAM. 2. Through the `Examples/tools/maixcam_switch_usb_mode.py` in MaixVision, modify the HID devices that need to be switched on in the `device_list`, run it and restart MaixCAM. Note: Since only 4 USB devices are supported, only 4 devices can be started at the same time among `ncm`, `rndis`, `keyboard`, `mouse`, `touchpad`, choose according to the actual demand, among them, `ncm` and `rndis` are the USB network protocol devices, you can turn them off if you don't need them, by default, they are turned on. ## Write a keyboard in MaixPy. You need to enable `HID Keyboard` to run it. The following example sends keyboard event to pc. ```python from maix import hid, time keyboard hid.Hid(hid.DeviceType.DEVICE_KEYBOARD) # Refer to the `Universal Serial Bus HID Usage Tables` section of the [USB HID Documentation](https://www.usb.org) for key numbers. keys [21, 22, 23, 24, 25, 0] # means [r, s, t, u, v, 0], 0 means release key. for key in keys: keyboard.write([0, 0, key, 0, 0, 0, 0, 0]) ``` After creating the `hid` object, key events can be sent using the `write` method. A key event is represented by an 8 byte array, where: **Byte 1**: Indicates the status of modifier keys like `ctrl`, `shift`, `alt`, etc. Each bit represents a specific modifier key:`bit0: left ctrl`,`bit1: left shift`,`bit2: left alt`,`bit3: left GUI (e.g., Windows key)`,`bit4: right ctrl`,`bit5: right shift`,`bit6: right alt`,`bit7: right GUI` **Byte 2**: Reserved byte. **Byte 3**: Primary key value. A value of 0 means the key is released. Key codes are referenced from the \"Universal Serial Bus HID Usage Tables\" section of the [USB HID documentation](https://www.usb.org). **Bytes 4~8**: Additional keys, used to press multiple keys at once. A value of 0 means the key is released. For specific usage, refer to the example code above. ## Write a mouse in MaixPy. You need to enable `HID Mouse` to run it. The following example moves the mouse 5 pixels every 100ms. ```python from maix import hid, time mouse hid.Hid(hid.DeviceType.DEVICE_MOUSE) button 0 # button state, 0 means release, 1 means left button pressed, 2 means right button pressed, 4 means wheel button pressed x_oft 0 # offset relative to current position, value range is 127~127 y_oft 0 # offset relative to current position, value range is 127~127 wheel_move 0 # The distance the wheel has moved, the range of values is 127~127 count 0 while True: x_oft + 5 y_oft + 5 mouse.write([button, x_oft, y_oft, wheel_move]) time.sleep_ms(100) count + 1 if count > 50: break ``` ## Write a touchpad in MaixPy. The `HID Touchpad` needs to be enabled to run. In the following example, move the touchscreen 150 units every 100ms. Note that the coordinate system of the touchscreen is absolute, not relative, and that you need to map the actual size of the screen to the interval [1, 0x7FFF], the coordinates (1,1) means the upper left corner, the coordinates (0x7FFF,0x7FFF) means the lower right corner. ```python from maix import hid, time touchpad hid.Hid(hid.DeviceType.DEVICE_TOUCHPAD) def touchpad_set(button, x_oft, y_oft, wheel_move): touchpad.write([button, # button state, 0 means release, 1 means left button pressed, 2 means right button pressed, 4 means wheel button pressed x_oft & 0xff, (x_oft >> 8) & 0xff, # Absolute position, the leftmost is 1, the rightmost is 0x7fff, 0 means no operation, the value range is 0 to 0x7fff. y_oft & 0xff, (y_oft >> 8) & 0xff, # Absolute position, the topmost is 1, the bottom is 0x7fff, 0 means no operation, the value range is 0 to 0x7fff wheel_move]) # wheel move distance, value range is 127~127 button 0 x_oft 0 y_oft 0 wheel_move 0 count 0 while True: x_oft + 150 y_oft + 150 touchpad_set(button, x_oft, y_oft, wheel_move) time.sleep_ms(100) count + 1 if count > 50: break ```"},"/maixpy/doc/en/peripheral/pinmap.html":{"title":"MaixCAM MaixPy Pinmap Usage Introduction","content":" title: MaixCAM MaixPy Pinmap Usage Introduction update: date: 2024 06 11 author: iawak9lkm version: 1.0.0 content: Initial version of the document date: 2025 08 08 author: Neucrack version: 1.1.0 content: Refactored the document for better understanding by beginners ## What is a Pin A pin is a physical hardware pin exposed from the chip/development board. They are visible and tangible around the chip package. In English, we refer to them as `Pin`. ## What is an On chip Peripheral These are components built into the chip, other than the CPU cores. The “external” here is relative to the `CPU core` (internal), as opposed to “off chip modules” which are external to the entire chip. For example, the following is the internal architecture diagram of the `MaixCAM/MaixCAM Pro` chip: ![](../../assets/maixcam_cpu_arch.jpg) We can see that the core consists of two RISC V cores and one 8051 core. In addition, there are many peripherals such as `GPIO`, `UART`, and `H.264 codec`, all of which are considered on chip peripherals. Importantly, **peripherals here do not necessarily require pins to be brought out of the chip** — for example, the `H.264 codec` is also a peripheral but does not need external pins for interaction. Also note that here `GPIO` is a peripheral function within the chip and is not the same as the physical pins (`Pin`) exposed from the chip. ## What is Pin Multiplexing / Pin Mapping For peripherals that require interaction with the outside world, such as `GPIO` or `I2C`, * `GPIO` can control pin input/output, * `I2C` can communicate with other chips via two pins (`SDA/SCL`). The simplest design is one pin for each function: for example, in the architecture diagram above, `GPIO` has 54 pins, and `I2C` has 5 sets, requiring a total of `54 + 2 × 5 64` pins. The more functions that require pins, the more pins need to be exposed from the chip, which increases chip size. In reality, it’s rare to need all 54 `GPIO`s and all 5 `I2C`s simultaneously. We can add a pin multiplexing circuit between peripherals and pins, so a single pin can switch between `GPIO` and `I2C`. This way, fewer pins can support more functions. For example, with 50 pins, 10 of them could be configured either as `GPIO` or as `I2C`. This is pin multiplexing, usually called `pinmux` in English. Due to hardware design limits, most chips allow a pin to be mapped to only a few fixed functions. For example, the `MaixCAM Pro` pin mapping is shown below: ![maixcam\\_pro\\_io](/static/image/maixcam_pro_io.png) Take the `A17` pin on the right: it supports three functions — `GPIOA17 / UART0_RX / PWM5`. We can select which function to use. If the default is `UART0_RX` but we need `PWM5`, we configure the pinmux accordingly. In MaixPy, the `maix.peripheral.pinmap` module (or simply `maix.pinmap`) is used to query and set pin multiplexing (pin function mapping). > Other benefits of pin multiplexing: > > * **Save pin count**: SoCs integrate many modules (CPU, GPU, memory controllers, I/O interfaces, communication modules, etc.). Assigning separate pins to each function would require huge pin counts, increasing packaging complexity and cost. Pinmux allows one pin to serve multiple purposes, reducing pin numbers significantly. > * **Reduce packaging & manufacturing cost**: Fewer pins allow for smaller chip packages, lowering material and manufacturing costs. Smaller packages also save PCB space, enabling more compact designs. > * **Increase design flexibility**: Pinmux provides flexibility for different application scenarios by enabling different pin functions via software configuration. > * **Simplify PCB layout**: Fewer pins make PCB routing easier, reducing layers and vias, which lowers production difficulty and cost. > * **Optimize performance**: Choosing optimal pin functions can shorten signal paths and reduce noise/interference, improving overall system reliability. ## Using Pinmap in MaixPy ### Pin Function Diagrams Different boards expose different pins. Below are pin mapping diagrams for each device. For detailed mappings, refer to the schematic or the chip manual’s Pinmux section: Device Model Pin Diagram Description Full Schematic & Datasheet MaixCAM ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) Silkscreen like `A19` is the pin name; `GPIOA19/PWM7` is the function name See [Hardware Docs](https://wiki.sipeed.com/hardware/zh/maixcam/index.html) MaixCAM Pro ![maixcam\\_pro\\_io](/static/image/maixcam_pro_io.png) First name like `A19` is the pin name; `GPIOA19/PWM7` is the function name See [Hardware Docs](https://wiki.sipeed.com/hardware/zh/maixcam/maixcam_pro.html) MaixCAM2 ![maixcam2\\_io](https://wiki.sipeed.com/hardware/assets/maixcam/maixcam2_pins.jpg) First name like `A2` is the pin name; `GPIOA2/SPI1_CS0` is the function name See [Hardware Docs](https://wiki.sipeed.com/hardware/zh/maixcam/maixcam2.html) > Note: For MaixCAM2, in the schematics and chip documentation, you may encounter notations like `GPIO1_A25`. To make things look cleaner, we define it as `B25`, which is equivalent. For example, `GPIO0_A2` corresponds to `GPIOA2` in MaixPy, and `GPIO3_A2` corresponds to `C2` in MaixPy. Specifically, the `n` in `GPIOn` maps as follows: `n 0` corresponds to `A` in MaixPy, `n 1` corresponds to `B`. The `A25` part of `GPIO1_A25` corresponds to `25` in MaixPy, with the `A` dropped. ### Mapping Pin Functions in MaixPy In MaixPy, use `maix.pinmap.set_pin_function` to set a pin function. Example — set `A17` pin on `MaixCAM/MaixCAM Pro`: ```python from maix import pinmap pinmap.set_pin_function(\"A17\", \"GPIOA17\") ``` Here `A17` is the pin name, and `GPIOA17` is the on chip peripheral function. This changes the default `UART0_RX` to `GPIO`. Now, even if data is sent to `UART0`, the chip won’t receive it because the pin is in `GPIO` mode. After setting to `GPIO`, follow the [GPIO usage](./gpio.html) docs to output high/low levels or read input. ### Get All Functions of a Pin ```python from maix import pinmap funcs pinmap.get_pin_functions(\"A17\") print(funcs) ``` Print all pins and their functions: ```python from maix.peripheral import pinmap print(\"All pins of MaixCAM:\") print(pinmap.get_pins()) print(\"All pin's functions:\") for pin in pinmap.get_pins(): funcs pinmap.get_pin_functions(pin) print(f\"{pin:10s}: {', '.join(funcs)}\") ``` ### Query Current Function of a Pin Note: support varies by board type: * **MaixCAM / MaixCAM Pro**: Mapping info is stored in an array, not read directly from hardware, so you must set it first for accuracy ([source code here](https://github.com/sipeed/MaixCDK/blob/main/components/peripheral/port/maixcam/maix_pinmap.cpp)). * **MaixCAM2**: Reads directly from hardware, so results are accurate. ```python from maix import pinmap func pinmap.get_pin_function(\"A17\") print(func) ``` ### More Examples See more in [MaixPy Examples](https://github.com/sipeed/MaixPy/tree/main/examples/peripheral/pinmap). ## API Documentation See the detailed [Pinmap API Docs](../../../api/maix/peripheral/pinmap.html). ## Default Pin Functions & Notes Device Model Pin Diagram Default Function Pins to Note MaixCAM ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) Refer to `MaixCAM Pro` 1. `UART0` is system log + default serial port<br>WiFi (SDIO1 + A26)<br>2. `A14` is system status LED; after setting to `GPIO`, it can be used as normal output<br>3. User button already has a system `key` driver; not recommended to read via `GPIO`<br>4. IO is `3.3V` — do not connect `5V` directly MaixCAM Pro ![maixcam\\_pro\\_io](/static/image/maixcam_pro_io.png) 1. Refer to silkscreen, e.g. `29` GPIO, `RX` UART<br>2. `6pin` defaults to UART and I2C 1. Same as `MaixCAM`<br>2. `B3` drives a lighting LED, active high MaixCAM2 ![maixcam2\\_io](https://wiki.sipeed.com/hardware/assets/maixcam/maixcam2_pins.jpg) 1. Refer to silkscreen, e.g. `A4` GPIO, `U2R` UART<br>2. `6pin` defaults to UART and I2C 1. `B25` drives a lighting LED, active high<br>2. `A6` drives system status LED; can be used as GPIO output after init<br>3. IO is `3.3V` — do not connect `5V` directly "},"/maixpy/doc/en/peripheral/uart.html":{"title":"MaixCAM MaixPy UART Serial Port Usage Introduction","content":" title: MaixCAM MaixPy UART Serial Port Usage Introduction update: date: 2024 03 07 author: Neucrack version: 1.0.0 content: Initial version date: 2024 08 01 author: Neucrack version: 1.1.0 content: Optimized documentation with more details date: 2025 08 08 author: Neucrack version: 1.2.0 content: Added MaixCAM2 support ## Prerequisite Knowledge Please first learn to use the [pinmap](./pinmap.html) module to set pin functions. To use a pin for `UART` functionality, you must first set its function to `UART` using `pinmap`. ## Serial Port Overview A serial port is a communication method that includes both hardware and communication protocol definitions. * Hardware includes: * 3 pins: `GND`, `RX`, and `TX`. Communication between two devices is **cross connected** for `RX` and `TX`, meaning one device’s `TX` connects to the other’s `RX`, and both `GND` pins are connected together. * A controller, usually inside the chip, also called a `UART` peripheral. A chip usually has one or more `UART` controllers, each with corresponding pins. * Serial communication protocol: To ensure proper communication, a protocol defines timing, baud rate, parity bits, etc. The baud rate is the most commonly used parameter. Through the board’s serial port, you can communicate with other microcontrollers or SoCs. For example, MaixCAM can perform human detection and send the detected coordinates to an STM32/Arduino via the serial port. ## Choosing the Appropriate I2C to Use First, we need to know which pins and I2C interfaces are available on the device, as shown below: Device Model Pin Diagram Pin Multiplexing Description MaixCAM ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) The board’s silkscreen shows the pin name (e.g., `A19`) and function name (e.g., `UART1_TX`). MaixCAM Pro ![maixcam\\_pro\\_io](/static/image/maixcam_pro_io.png) The first label (e.g., `A19`) is the pin name, corresponding to the function name (e.g., `UART1_TX`). MaixCAM2 ![maixcam2\\_io](https://wiki.sipeed.com/hardware/assets/maixcam/maixcam2_pins.jpg) The first label (e.g., `A21`) is the pin name, corresponding to the function name (e.g., `UART4_TX`). Note: Pins may be used for other purposes by default. It’s best to avoid these pins—see the [pinmap](./pinmap.html) documentation. ### Notes for MaixCAM/MaixCAM Pro Serial Port Usage: * By default, a `UART0` serial port is routed from the USB port. You can use the matching Type C adapter board to directly access the serial pins, or you can use the onboard `A16 (TX)` and `A17 (RX)` pins, which are equivalent to the USB exposed serial pins. * When using the USB exposed serial port on MaixCAM, note that the Type C plug’s orientation affects the adapter board’s `RX` and `TX` pins (swapped if reversed; silkscreen matches when the **Type C female port faces forward**). If communication fails, try flipping the Type C connector. * **`UART0` on MaixCAM prints boot logs** during startup and prints `serial ready` when boot completes. If communicating with a microcontroller, ignore this initial output. Boot logs can also help diagnose startup issues. * The `TX` pin of `UART0` is also a boot mode detection pin. It **must not be pulled low during power on**, or the device won’t boot. If using a 3.3 V to 5 V level shifter, ensure it doesn’t pull `TX` low by default (use a level shifting chip or keep it floating). If the board won’t start, check if `TX` is being pulled low. * If `UART0` causes issues, consider using another UART such as `UART1`. * `UART0` is also the system’s default `maix protocol` port. ### Notes for MaixCAM2: * `MaixCAM2` has multiple serial ports: `UART0 / UART1 / UART2 / UART3 / UART4`—don’t mix them up. * `UART0` is the system terminal and log port. ### Baud Rate Limitations Not all baud rates are supported. Unless necessary, use `115200` (universally supported). Other baud rates may have high error rates or be unsupported. Common tested baud rates (contributions welcome): * `MaixCAM / MaixCAM Pro`: `115200`. * `MaixCAM2`: `115200`. Theoretical max: `4000000 bits/s`. Formula: `baud uart_clk / (fractional_div * 16)` Default `uart_clk`: `200000000`. Integer part: `uart_clk / (baud * 16)`. Fractional part: `round((uart_clk % (baud * 16)) * 16 / (baud * 16)) / 16`. Example: For `115200`, divisor `108.5`, precision `0.0064%`. ## Serial Port Hardware Wiring For two devices to communicate, connect three pins: `GND`, `RX`, `TX`. Connect `TX` of one to `RX` of the other, and connect both `GND`s together. ## Using Serial Port in MaixPy Once the two boards are connected (crossed `RX`/`TX`, common `GND`), you can use the software. Basic MaixPy code: ```python from maix import uart serial_dev uart.UART(\"/dev/ttyS0\", 115200) serial_dev.write_str(\"Hello MaixPy\") ``` `/dev/ttyS0` is the serial device. Use `print(uart.list_devices())` to list all devices. For pins that are already mapped to UART, you can use them directly. For others, set their function via `pinmap` before creating the `UART` object: ```python from maix import uart, pinmap, time, sys, err # ports uart.list_devices() # list all UARTs device_id sys.device_id() if device_id \"maixcam2\": pin_function { \"A21\": \"UART4_TX\", \"A22\": \"UART4_RX\" } device \"/dev/ttyS4\" else: pin_function { \"A16\": \"UART0_TX\", \"A17\": \"UART0_RX\" } device \"/dev/ttyS0\" for pin, func in pin_function.items(): err.check_raise(pinmap.set_pin_function(pin, func), f\"Failed set pin{pin} function to {func}\") serial_dev uart.UART(device, 115200) serial_dev.write_str(\"Hello MaixPy\") ``` ## Connecting Serial Port to a Computer * **Why doesn’t a serial device appear on my computer when I plug in USB?** The board’s USB port is for USB functions (e.g., USB network adapter), not USB to UART. For terminal access, use SSH. * **How to communicate between the computer and board via UART?** Use a USB to UART adapter (e.g., [this one](https://item.taobao.com/item.htm?spm a1z10.5 c s.w4002 24984936573.13.73cc59d6AkB9bS&id 610365562537)). Connect USB to the PC and UART to the board. * **How to view boot logs or interact with the board via UART terminal?** SSH is recommended for terminal interaction. For serial terminal access: * **MaixCAM/MaixCAM Pro**: Connect USB to UART adapter to `UART0` (`A16` TX, `A17` RX). In `/boot/uEnv.txt`, comment or remove the `consoledev` line to enable UART0 terminal, then reboot. You’ll see boot logs and have terminal access. * **MaixCAM2**: Connect USB to UART adapter to `UART0` (`U0T`/`U0R`). You’ll see boot logs and have terminal access after boot. ## Sending Data There are mainly two functions for sending data: `write_str` and `write`. The `write_str` function is used to send strings, while `write` is used to send byte streams, i.e., `str` and `bytes` types, which can be converted to each other. For example: * `\"A\"` can be converted to `b\"A\"` using the `encode()` method, and vice versa, `b\"A\"` can be converted back to `\"A\"` using the `decode()` method. * `str` cannot display some invisible characters, such as the ASCII value `0`, which is generally `\\0` in strings and serves as a terminator. In `bytes` type, it can be stored as `b\"\\x00\"`. * This is more useful for non ASCII encoded strings. For example, the Chinese character `好` in `UTF 8` encoding is represented by three bytes `\\xe5\\xa5\\xbd`. We can use `\"好\".encode(\"utf 8\")` to get `b\"\\xe5\\xa5\\xbd\"`, and `b'\\xe5\\xa5\\xbd'.decode(\"utf 8)` to get `\"好\"`. So if we need to send byte data, we can use the `write()` method to send it. For example: ```python bytes_content b'\\x01\\x02\\x03' serial.write(bytes_content) ``` Therefore, for the `str` type, you can use `serial.write(str_content.encode())` instead of `write_str` to send it. If you have other data types that you want to convert into a **string to send**, you can use `Python string formatting` to create a string. For example, to send `I have xxx apple`, where `xxx` is an integer variable, you can do: ```python num 10 content \"I have {} apple\".format(num) content2 f\"I have {num} apple\" content3 \"I have {:04d} apple\".format(num) content4 f\"I have {num:d} apple\" print(content) print(content2) print(content3) print(content4) print(type(content)) serial.write_str(content) ``` Additionally, you can encode the data into a **binary stream to send**. For example, the first 4 bytes are hexadecimal `AABBCCDD`, followed by an `int` type value, and finally a `0xFF` at the end. You can use `struct.pack` to encode it (if this is unclear, you can read the explanation later): ```python from struct import pack num 10 bytes_content b'\\xAA\\xBB\\xCC\\xDD' bytes_content + pack(\"<i\", num) bytes_content + b'\\xFF' print(bytes_content, type(bytes_content)) serial.write(bytes_content) ``` Here, `pack(\"<i\", num)` encodes `num` as an `int` type, which is a 4 byte signed integer. The `<` symbol indicates little endian encoding, with the low byte first. Here, `num 10`, the 4 byte hexadecimal representation is `0x0000000A`, and little endian encoding puts the low byte `0x0A` first, resulting in `b'\\x0A\\x00\\x00\\x00'`. > Here, we use `i` to encode `int` type data as an example. Other types, such as `B` for `unsigned char`, etc., can also be used. More `struct.pack` formatting options can be searched online with `python struct pack`. In this way, the final data sent is `AA BB CC DD 0A 00 00 00 FF` as binary data. ## Receiving Data Use the `read` method to read data directly: ```python while not app.need_exit(): data serial.read() if data: print(data) time.sleep_ms(1) ``` Similarly, the data obtained by the `read` method is also of the `bytes` type. Here, `read` reads a batch of data sent by the other party. If there is no data, it returns `b''`, which is an empty byte. Here, `time.sleep_ms(1)` is used to sleep for `1ms`, which frees up the CPU so that this thread does not occupy all CPU resources. `1ms` does not affect the program's efficiency, especially in multithreading. In addition, the `read` function has two parameters: * `len`: Represents the maximum length you want to receive. The default is ` 1`, meaning it will return as much as there is in the buffer. If you pass a value `>0`, it means it will return data up to that length. * `timeout`: * The default `0` means it will return immediately with whatever data is in the buffer. If `len` is ` 1`, it returns all data; if a length is specified, it returns data not exceeding that length. * `<0` means it waits until data is received before returning. If ` len` is ` 1`, it waits until data is received and returns (blocking read for all data); if a length is specified, it waits until it reaches `len` before returning. * `>0` means it will return after this time, regardless of whether data is received. It may seem complex, but here are some common parameter combinations: * `read()`: Which is `read( 1, 0)`, reads the data received in the buffer, usually a batch of data sent by the other party. It returns immediately when the other party has stopped sending (within one character's sending time). * `read(len 1, timeout 1)`: Blocking read for a batch of data, waits for the other party to send data and returns only when there is no more data within one character's sending time. * `read(len 10, timeout 1000)`: Blocking read for 10 characters, returns when 10 characters are read or 1000ms has passed without receiving any data. ## Setting a Callback Function for Receiving Data In MCU development, a serial port interrupt event usually occurs when data is received. MaixPy has already handled the interrupt at the bottom layer, so developers don't need to handle the interrupt themselves. If you want to call a callback function upon receiving data, you can use `set_received_callback` to set the callback function: ```python from maix import uart, app, time def on_received(serial : uart.UART, data : bytes): print(\"received:\", data) # send back serial.write(data) device \"/dev/ttyS0\" serial uart.UART(device, 115200) serial.set_received_callback(on_received) serial0.write_str(\"hello\\r\\n\") print(\"sent hello\") print(\"wait data\") while not app.need_exit(): time.sleep_ms(100) # sleep to make CPU free ``` When data is received, the set callback function will be called in **another thread**. Since it's called in another thread, unlike an interrupt function, you don't have to exit the function quickly. You can handle some tasks in the callback function before exiting, but be aware of common multithreading issues. If you use the callback function method to receive data, do not use the `read` function to read it, or it will read incorrectly. ## Using Other Serial Ports Each pin may correspond to different peripheral functions, which is also known as pin multiplexing. As shown below, each pin corresponds to different functions. For example, pin `A17` (silkscreen identification on the board) corresponds to `GPIOA17`, `UART0_RX`, and `PWM5` functions. The default function is `UART0_RX`. ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) ![maixcam_pro_io](/static/image/maixcam_pro_io.png) By default, you can directly use `UART0` as shown above. For other serial port pins, they are not set to the serial peripheral function by default, so you need to set the mapping to use other serial ports. Use `pinmap.set_pin_function` to set it. Let's take `UART1` as an example. First, set the pin mapping to choose the serial port function, then use the device number `/dev/ttyS1`. Note that `uart.list_devices()` will not return manually mapped serial ports by default, so you can directly pass the parameters manually: ```python from maix import app, uart, pinmap, time pinmap.set_pin_function(\"A18\", \"UART1_RX\") pinmap.set_pin_function(\"A19\", \"UART1_TX\") device \"/dev/ttyS1\" serial1 uart.UART(device, 115200) ``` ## Application Layer Communication Protocol ### Concept and Character Protocol Serial ports only define the hardware communication timing. To let the receiver understand the meaning of the character stream sent by the sender, an application communication protocol is usually established. For example, if the sender needs to send coordinates containing two integer values `x, y`, the following protocol is established: * **Frame Header**: When I start sending the `$` symbol, it means I'm about to start sending valid data. > **Content**: Designing a start symbol is because serial communication is stream based. For example, sending `12345` twice may result in receiving `12345123` at some moment. The `45` from the second frame has not been received. We can determine a complete data frame based on start and end symbols. * The value range of `x, y` is 0~65535, i.e., an unsigned short integer (`unsigned short`). I'll first send `x` then `y`, separated by a comma, such as `10,20`. * **Frame Tail**: Finally, I'll send a `*` to indicate that I've finished sending this data. In this way, sending a data packet looks like `$10,20*` as a string. The other party can receive and parse it using C language: ```c // 1. Receive data // 2. Determine if the reception is complete based on the frame header and tail, and store the complete frame data in the buff array // 3. Parse a frame of data uint16_t x, y; sscanf(buff, \"$%d,%d*\", &x, &y); ``` Thus, we have defined a simple character communication protocol with a certain degree of reliability. However, since we usually use parameters like `115200 8 N 1` for serial ports, where `N` means no parity check, we can add a **checksum** to our protocol at the end. For example: * Here, we add a checksum value after `x, y`, ranging from 0 to 255. It is the sum of all previous characters modulo 255. * Taking `$10,20` as an example, in `Python`, you can simply use the `sum` function: `sum(b'$10,20') % 255 > 20`, and send `$10,20,20*`. * The receiver reads the checksum `20`, calculates it in the same way as `$10,20`, and if it is also `20`, it means no transmission error occurred. Otherwise, we assume a transmission error and discard the packet to wait for the next one. In MaixPy, encoding a character protocol can be done using Python's string formatting feature: ```python x 10 y 20 content \"${},{}*\".format(x, y) print(content) ``` ### Binary Communication Protocol The character protocol above has a clear characteristic of using visible characters to transmit data. The advantage is simplicity and human readability. However, it uses an inconsistent number of characters and larger data volumes. For example, `$10,20*` and `$1000,2000*` have varying lengths, with `1000` using 4 characters, which means 4 bytes. We know an unsigned short integer (`uint16`) can represent values ranging from `0~65535` using only two bytes. This reduces the transmission data. We also know visible characters can be converted to binary via ASCII tables, such as `$1000` being `0x24 0x31 0x30 0x30 0x30` in binary, requiring 5 bytes. If we directly encode `1000` in binary as `0x03E8`, we can send `0x24 0x03 0xE8` in just 3 bytes, reducing communication overhead. Additionally, `0x03E8` is a 2 byte representation with `0xE8` as the low byte, transmitted first in little endian encoding. The opposite is big endian encoding. Both are fine as long as both parties agree on one. In MaixPy, converting a number to bytes is simple with `struct.pack`. For example, `0x03E8` (decimal `1000`): ```python from struct import pack b pack(\"<H\", 1000) print(b) ``` Here, `<H` indicates little endian encoding, with `H` denoting a `uint16` data type, resulting in `b'\\xe8\\x03'` as bytes. Similarly, binary protocols can have a frame header, data content, checksum, frame tail, or a frame length field instead of a frame tail, based on preference. ### Built in MaixPy Communication Protocol MaixPy also includes a built in communication protocol. Using this protocol, it is possible to implement application switching, application control, and data retrieval via serial communication or even TCP. For example, the coordinates detected by an AI detection application after identifying an object can be parsed using this protocol. ## Other Tutorials * [【MaixPy/MaixCAM】Visual Tool MaixCAM Beginner Tutorial 2](https://www.bilibili.com/video/BV1vcvweCEEe/?spm_id_from 333.337.search card.all.click) Watch the serial port explanation section * [How to Communicate via Serial Port between Visual Module and STM32](https://www.bilibili.com/video/BV175vWe5EfV/?spm_id_from 333.337.search card.all.click&vd_source 6c974e13f53439d17d6a092a499df304) * [[MaixCam] Experience 2: UART Serial Communication](https://blog.csdn.net/ButterflyBoy0/article/details/140577441) * For more, search online for resources."},"/maixpy/doc/en/peripheral/spi.html":{"title":"MaixCAM MaixPy SPI Serial Peripheral Interface Usage Guide","content":" title: MaixCAM MaixPy SPI Serial Peripheral Interface Usage Guide update: date: 2024 06 11 author: iawak9lkm version: 1.0.0 content: Initial version of the document date: 2025 08 08 author: Neucrack version: 1.1.0 content: Refactored document, easier for beginners to understand ## Prerequisites Please first learn how to use the [pinmap](./pinmap.html) module to set pin functions. To enable a pin for `SPI` functionality, first use `pinmap` to set the corresponding pin function to `SPI`. ## Introduction to SPI Earlier we introduced `I2C`, which enables one to many bus communication with only two wires. However, it has limitations, such as relatively low speed (typically `200k/400k`). `SPI` (Serial Peripheral Interface) is also a one to many bus communication method, but it is faster and requires `4` wires for communication: * `MISO`: Master In Slave Out — this pin sends data in slave mode and receives data in master mode. * `MOSI`: Master Out Slave In — this pin sends data in master mode and receives data in slave mode. * `SCK`: Serial clock, output by the master and input to the slave. * `NSS`/`CS`: Slave Select — chip select pin that allows the master to communicate individually with a specific slave device, avoiding conflicts on the data lines. Common uses include: * Reading and writing Flash memory. * Communication between two devices. * Protocol conversion, such as SPI to Ethernet. * LCD display drivers. * Outputting specific square waves, e.g., WS2812 LEDs. Apart from using GPIO control, SPI’s square wave output capability can also be used to output specific waveforms. In terms of protocol behavior, SPI generally works as follows: * SPI supports one master and multiple slaves. The master selects the slave to communicate with via the chip select pin. In most cases, a slave device needs only one chip select pin, while the master’s number of chip select pins equals the number of devices. When the chip select signal for a specific slave is enabled, that slave responds to all requests from the master; other slaves ignore all bus data. * SPI has four modes, determined by polarity (CPOL) and phase (CPHA) settings. Polarity affects the clock signal level when the SPI bus is idle. 1. CPOL 1: Idle level is high. 2. CPOL 0: Idle level is low. Phase determines which clock edge is used to sample data. 1. CPHA 0: Sampling starts at the first clock edge. 2. CPHA 1: Sampling starts at the second clock edge. Combining polarity and phase yields the four SPI modes: Mode CPOL CPHA 0 0 0 1 0 1 2 1 0 3 1 1 * SPI usually supports both full duplex and half duplex communication. * SPI has no defined maximum transfer rate, no addressing scheme, no acknowledgment mechanism, and no flow control rules. SPI is a very common communication interface, and through SPI, SoCs can control various peripheral devices. ## Choosing the Right SPI to Use First, we need to know which pins and SPI interfaces the device provides, as shown in the table: Device Model Pin Diagram Pin Multiplexing Description MaixCAM ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) On the silkscreen, `A24` is the pin name, `SPI4_CS` is the function name MaixCAM Pro ![maixcam_pro_io](/static/image/maixcam_pro_io.png) The first name, such as `A24`, is the pin name; `SPI4_CS` is the function name MaixCAM2 ![maixcam2_io](https://wiki.sipeed.com/hardware/assets/maixcam/maixcam2_pins.jpg) The first name, such as `B21`, is the pin name; `SPI2_CS1` is the function name Note that pins may be used for other purposes by default; it’s best to avoid those pins. See the [pinmap](./pinmap.html) documentation for details. For example: * `MaixCAM/MaixCAM Pro`: Due to SPI peripheral limitations, they can only be used as SPI masters. MaixCAM's SPI currently does not support changing the active level of the hardware CS pin; all hardware SPI CS pins are active low. If you need another CS active level, configure it in the SPI API using a software CS pin and its active level. SPI4 is a software simulated SPI, with a tested maximum speed of 1.25 MHz; usage is the same as hardware SPI. * `MaixCAM2`: Has two hardware SPI interfaces by default; SPI2’s default function is SPI. SPI1’s pins require multiplexing setup first; see `pinmap` for details. ## Using SPI in MaixPy To use SPI in MaixPy, first configure `pinmap`, then create an `SPI` object to communicate. Here’s an example: connect the SPI’s `MOSI` and `MISO` together to perform a full duplex loopback test. ```python from maix import spi, pinmap, sys, err # get pin and SPI number according to device id device_id sys.device_id() if device_id \"maixcam2\": pin_function { \"B21\": \"SPI2_CS1\", \"B19\": \"SPI2_MISO\", \"B18\": \"SPI2_MOSI\", \"B20\": \"SPI2_SCK\" } spi_id 2 else: pin_function { \"A24\": \"SPI4_CS\", \"A23\": \"SPI4_MISO\", \"A25\": \"SPI4_MOSI\", \"A22\": \"SPI4_SCK\" } spi_id 4 for pin, func in pin_function.items(): err.check_raise(pinmap.set_pin_function(pin, func), f\"Failed set pin{pin} function to {func}\") spidev spi.SPI(spi_id, spi.Mode.MASTER, 1250000) ### Example of full parameter passing, fully documention see API documentation. # spidev spi.SPI(id 4, # SPI ID # mode spi.Mode.MASTER, # SPI mode # freq 1250000, # SPI speed # polarity 0, # CPOL 0/1, default is 0 # phase 0, # CPHA 0/1, default is 0 # bits 8, # Bits of SPI, default is 8 # hw_cs 1, # use default hardware cs. # soft_cs \"\", # If you want use soft cs, set GPIO name, # # e.g. GPIOA19(MaixCAM), GPIOA2(MaixCAM2) # # you should set pinmap first by yourself. # cs_active_low true # cs pin active low voltage level b bytes(range(0, 8)) res spidev.write_read(b, len(b)) if res b: print(\"loopback test succeed\") else: print(\"loopback test failed\") print(f\"send:{b}\\nread:{res}\") ```` ## More Examples See [MaixPy examples](https://github.com/sipeed/MaixPy/tree/main/examples/peripheral/spi). ## API Documentation For more APIs, see the [SPI API documentation](https://wiki.sipeed.com/maixpy/api/maix/peripheral/spi.html)"},"/maixpy/doc/en/peripheral/adc.html":{"title":"Using ADC in MaixCAM MaixPy","content":" title: Using ADC in MaixCAM MaixPy update: date: 2024 06 11 author: iawak9lkm version: 1.0.0 content: Initial document ## Supported Devices Device Supported MaixCAM2 ❌ MaixCAM / MaixCAM Pro ✅ ## ADC Introduction An ADC, which can also be called an analog to digital converter, converts an input voltage signal into an output digital signal. As the ADC converted digital signal itself does not have practical significance, only represents a relative size. Therefore, any ADC needs a reference analog as a conversion standard, the reference standard is generally the largest convertible signal size. The digital output of the ADC indicates the size of the input signal relative to the reference signal. ADC peripherals generally have two main parameters: resolution and reference voltage. * Resolution: The resolution of an ADC is expressed as the number of bits in a binary (or decimal) number. It describes the ability of the A/D converter to discriminate the input signal. Generally speaking, an A/D converter with n bit output can distinguish 2^n different levels of input analog voltage, and the minimum value of input voltage that can be distinguished is 1/(2^n) of the full scale input. For a given maximum input voltage, the more output bits, the higher the resolution. * Reference Voltage: The ADC peripheral reference voltage is the voltage that is compared to a known voltage during AD conversion to find the value of the unknown voltage. The reference voltage can be thought of as the highest upper limit voltage and can be reduced to improve resolution when the signal voltage is low. With the board's ADC, it is possible to capture external voltages and have the board verify that the voltages are up to snuff or perform specific tasks when specific voltages are detected (e.g., the ADC detects multiple buttons). ## Using ADC in MaixPy Using ADC with MaixPy is easy: ```python from maix.peripheral import adc from maix import time a adc.ADC(0, adc.RES_BIT_12) raw_data a.read() print(f\"ADC raw data:{raw_data}\") time.sleep_ms(50) vol a.read_vol() print(f\"ADC vol:{vol}\") ``` Use ADC0 to read the raw conversion data from it, or read the voltage data directly from it. See the ADC [API documentation](../../../api/maix/peripheral/adc.html) for a detailed description of the ADC API. ## Some notes on MaixCAM's ADC MaixCAM elicits an IO that connects to the ADC, this IO is GPIO B3（For MaixCAM Pro, B3 connected light LED, so ADC can't directly use）. ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/intro/RV_Nano_3.jpg) This IO is ADC by default and does not require additional configuration. MaixCAM's ADC peripheral has a sampling accuracy of 12 bits, which means that the sampling output range is from 0 to 4095. The sampling accuracy is 1/4096 of the reference voltage. The MaixCAM's ADC peripheral cannot scan at a frequency higher than 320K/s, which is the reason for the additional wait time between ADC samples in the previous example. The MaixCAM's ADC peripheral has an internal reference voltage of 1.5V, which may vary slightly in actual use.Since the typical internal reference voltage is 1.5 V, the ADC range of Soc is 0 to 1.5 V. Since the ADC range of this range is small, MaixCAM has designed a voltage divider circuit for the ADC peripheral to increase the ADC range. The reference voltage Vin_max of this voltage divider circuit is about 4.6~5.0V, due to the error of resistor resistance in the circuit, the impedance of ADC external device, and the deviation of internal reference voltage. A higher precision default value has been chosen in the API, and there is generally no need to pass this parameter. ![](https://wiki.sipeed.com/hardware/zh/lichee/assets/RV_Nano/peripheral/adc.png) If you need high ADC accuracy, you can calculate the reference voltage for this voltage divider circuit by following the steps below: * You need to first measure to get the actual input voltage of ADC_PIN, which we call Vin. * Then you need to measure to get the actual input voltage at ADC1, which we call Vadc. The location of resistor R10 can be found in this BOM file. * You need to keep the same voltage input to ADC_PIN as in step 1 and then execute these commands in the shell: ```shell echo 1 > /sys/class/cvi saradc/cvi saradc0/device/cv_saradc cat /sys/class/cvi saradc/cvi saradc0/device/cv_saradc ``` This gives you the raw measured value of the ADC, which we call adc_data. * You need to know the resistance values of the resistors R6 and R10 in the picture, record them.Typically, the MaixCAM has a resistance value of 10KΩ (10 000Ω) for R6 and 5.1KΩ (5 100Ω) for R10. * Finally, you need to pass the results from the above steps to these python codes to get the range [0,Vin_max] of the ADC_PIN port. ```python def maixcam_get_vin_max(Vin:float, Vadc:float, adc_data:int, r6:int, r10:int, adc_max:int 4095): Vref (Vadc/adc_data)*(adc_max+1) r3 Vadc*r6/(Vin Vadc) Vin_max (Vref/r3)*(r6+r3) return Vin_max Vin 3.3\t\t# step 1 Vadc 1.06\t\t# step 2 adc_data 2700\t# step 3 r6 10000\t\t# step 4 r10 5100\t\t# step 4 if __name__ '__main__': print(maixcam_get_vin_max(Vin, Vadc, adc_data, r6, r10)) ``` Now pass the result to the third parameter of `adc.ADC()` and you will get a highly accurate ADC."},"/maixpy/doc/en/peripheral/gpio.html":{"title":"MaixCAM MaixPy Using GPIO","content":" title: MaixCAM MaixPy Using GPIO ## Introduction Using GPIOs allows you to control a pin to output a high or low voltage level or to read a signal from it. This is a very common way to read signals or output control signals. **Note:** The pins on the `MaixCAM` are `3.3V` tolerant. **Do not** apply `5V` to them. ## Prerequisites Please learn how to use the [pinmap](https://www.google.com/search?q ./pinmap.md) module to set pin functions first. To enable a pin for `GPIO` functionality, first use `pinmap` to set the function of the corresponding pin to `GPIO`. ## Choosing the Right GPIO to Use First, you need to know which pins on your device are available as GPIOs, as shown in the figure: Device Model Pinout Diagram Pin Multiplexing Description MaixCAM The silkscreen on the board, e.g., `A19`, is the pin name, and `GPIOA19/PWM7` is the function name MaixCAM Pro The first name, e.g., `A19`, is the pin name, and `GPIOA19/PWM7` is the corresponding function name MaixCAM2 The first name, e.g., `A2`, is the pin name, and `GPIOA2/SPI1_CS0` is the corresponding function name, that is, just add `GPIO` prefix for IO name to use `GPIO` funtion Note that pins may have other default uses. It's best to avoid these pins. Please refer to the [pinmap](https://www.google.com/search?q ./pinmap.md) documentation for details. ## Circuit Considerations Be aware that the voltage tolerance and load capacity of the pins are limited. You need to be careful when designing circuits to avoid basic mistakes, like asking \"why can't the pin directly power a motor?\" * **Pin Voltage Tolerance**: Unless otherwise specified, the pins operate at `3.3v`. Do not connect an external `5v` voltage. * **Pin Input/Output Current**: The input/output current of the chip pins is limited. They are generally only used for control signals. For devices with high current requirements, please use a conversion circuit. **Example**: To control an LED, a simple circuit is as follows: The `LED` is directly powered by a high level output from the pin. This is the most intuitive way to use it, but you must be very careful. The maximum output and input current of a chip pin (the driving capability) is limited and is usually described in the chip's datasheet. Here, the current is `3.3v/(LED+resistor resistance)` \\< `0.64mA`, so it can be driven directly. However, if your circuit's current is too high, it may fail to drive the device or even cause the chip to malfunction. The **correct approach** is to connect an external conversion circuit so the pin only acts as a control signal. This can be done using a transistor, optocoupler, or relay. This document won't go into details; please do your own research. ## GPIO Output Mode As shown in the LED circuit diagram, we only need to provide a high voltage level to the `A14` (`MaixCAM2` is `A6`) pin for the LED to turn on: ```python from maix import gpio, pinmap, time, sys, err pin_name \"A6\" if sys.device_id() \"maixcam2\" else \"A14\" gpio_name \"GPIOA6\" if sys.device_id() \"maixcam2\" else \"GPIOA14\" err.check_raise(pinmap.set_pin_function(pin_name, gpio_name), \"set pin failed\") led gpio.GPIO(gpio_name, gpio.Mode.OUT) led.value(0) while 1: led.toggle() time.sleep_ms(500) ``` * First, get the pin and function name based on the board model. * Use `pinmap` to set the pin's function to `GPIO`. * `err.check_raise` is used to check the return value of `set_pin_function`. If there's an error, it raises an exception to prevent mistakes. * Initialize the `GPIO` object and set it to output mode. * The output value is toggled every `0.5s`, causing the LED to blink. For more API information, please see the [GPIO API documentation](https://wiki.sipeed.com/maixpy/api/maix/peripheral/gpio.html). ## GPIO Input Mode ```python from maix import gpio, pinmap, time, err err.check_raise(pinmap.set_pin_function(\"A19\", \"GPIOA19\"), \"set pin failed\") led gpio.GPIO(\"GPIOA19\", gpio.Mode.IN) while 1: print(led.value()) time.sleep_ms(1) # sleep to make cpu free ``` ## Using the Illumination LED on MaixCAM Pro Both `MaixCAM / MaixCAM Pro` and `MaixCAM2` have a small LED connected to pins `A14` and `IOA6`. Additionally, the `MaixCAM Pro` and `MaixCAM2` also have an onboard **illumination LED** connected to pins `B3` and `IOA25`, respectively, which turns on with a high voltage and off with a low voltage: ```python from maix import gpio, pinmap, time, sys, err pin_name \"B25\" if sys.device_id() \"maixcam2\" else \"B3\" gpio_name \"B25\" if sys.device_id() \"maixcam2\" else \"GPIOB3\" err.check_raise(pinmap.set_pin_function(pin_name, gpio_name), \"set pin failed\") led gpio.GPIO(gpio_name, gpio.Mode.OUT) led.value(0) while 1: led.toggle() time.sleep_ms(500) ``` ## More Examples See [MaixPy examples](https://github.com/sipeed/MaixPy/tree/main/examples/peripheral/gpio). ## API Documentation For more API information, see the [GPIO API documentation](https://wiki.sipeed.com/maixpy/api/maix/peripheral/gpio.html)."},"/maixpy/doc/en/peripheral/wdt.html":{"title":"Using Watchdog Timer in MaixCAM MaixPy","content":"# Using Watchdog Timer in MaixCAM MaixPy ## Introduction To prevent program issues, a watchdog timer (WDT) is often used to automatically restart the system when the program encounters a problem. The principle is that there is a countdown timer that we need to periodically reset within the program logic (also called \"feeding the dog\"). If our program gets stuck and fails to reset the countdown timer, the hardware will trigger a system reboot when the timer reaches 0. ## Using WDT in MaixPy ```python from maix import wdt, app, time w wdt.WDT(0, 1000) while not app.need_exit(): w.feed() # Here, sleep operation is our task # 200 ms is normal; if it exceeds 1000 ms, it will cause a system reset time.sleep_ms(200) ``` This code sets up a watchdog timer that requires feeding every 1000 ms. If the program fails to feed the watchdog within this period, the system will reset."},"/maixpy/doc/en/gui/i18n.html":{"title":"MaixPy MaixCAM i18n (Internationalization) Multi-Language Implementation","content":" title: MaixPy MaixCAM i18n (Internationalization) Multi Language Implementation ## Introduction to i18n (Internationalization) i18n is an abbreviation for internationalization, which aims to switch languages according to the user's region or preference. Commonly used languages, such as Chinese and English, have corresponding region codes (LCID). For example, the region code for Chinese is `zh`, English is `en`, and Japanese is `ja`. There are also secondary region codes, like Simplified Chinese corresponding to `zh cn`. Generally, implementing `zh` is sufficient. For region codes, you can refer to [Windows Locale Codes](https://www.science.co.il/language/Locale codes.php) or check [Wikipedia](https://en.wikipedia.org/wiki/Language_localisation). ## Using i18n in MaixPy MaixCAM The general user process is as follows: * Initially, users can select the system language in the system settings, with the factory default being `en` (English). * Then, the program can get the current system locale using `maix.i18n.get_locale()`. * The program displays the corresponding language strings based on the system locale. For applications, the tricky part is the third step, which involves looking up the corresponding strings based on the locale settings. Here are two methods to achieve this, depending on your needs: Full example code in [MaixPy](https://github.com/sipeed/MaixPy) `examples/gui/i18n`. ### Using a Dictionary Directly Without Translation Files If your program only has a few strings, you can manually specify the translation dictionary: ```python from maix import i18n trans_dict { \"zh\": { \"hello\": \"你好\" }, \"en\": { } } trans i18n.Trans(trans_dict) tr trans.tr trans.set_locale(\"zh\") print(tr(\"hello\")) print(tr(\"my friend\")) ``` Here, `trans.set_locale(\"zh\")` temporarily sets the language to Chinese. Running this will print `你好` and `my friend`, since there is no translation for `my friend`, it returns as is. ### Automatically Scanning and Generating a Dictionary, and Loading from Translation Files This method is more suitable for scenarios with many strings to translate. In the previous method, we manually specified string translations, which is convenient for simple scenarios. However, if there are too many strings, manually editing the dictionary can easily result in omissions. Therefore, we need the program to automatically find the strings that need translation and generate translation files, which we only need to translate. In MaixPy, the `maix.i18n.Trans` class is provided to load translation files in multiple languages. By calling its `tr()` function and passing in the text to be translated, you can get the translation. For example: ```python from maix import i18n, err trans i18n.Trans() tr trans.tr e trans.load(\"locales\") err.check_raise(e, \"load translation yamls failed\") print(tr(\"hello\")) ``` Here, the translation files are loaded from the `locales` folder in the current directory, and the system prints `hello` according to the language settings, such as `你好` for Chinese. **Translation Files**: Since translation files are used here, how are these files created? First, we need to know which text needs translation, which are the strings called by the `tr` function. So we just need to search for all strings that use the `tr` function in the source code to find all the strings that need translation. The usage process is as follows: * Create a project folder to store the code entry `main.py`, and open this project folder with `MaixVision` for easy operation. * Write `main.py`, using the `tr` function to call the strings that need translation. * MaixPy provides a scanning tool. First, make sure `maixtool` is installed (`pip install maixtool U` on the computer terminal to install or upgrade). * Then, in the directory, use the computer terminal to execute `maixtool i18n d . r` to scan for strings that need translation and generate a `locales` directory containing translation files for Chinese and English. For more languages, execute `maixtool i18n h` for help. * The generated files are key value pairs, for example, in `zh.yaml`, `hello: hello` means the Chinese translation of `hello` is `hello`. This is incorrect and needs manual translation, changing `hello: hello` to `hello: 你好`. Make sure to use a text editor that supports `UTF 8` encoding, especially on Windows, avoid changing the file to `GBK` encoding to prevent errors. You can use MaixVision or VsCode for editing. * Then run the project, or package the project into an installation package, remember to include the `locales` directory. * If the source code is updated later, execute the `maixtool` command again to update the files. It will update the previously translated files. If you are worried about accidental overwriting, you can back up the files first and then delete the backup after confirming everything is correct. This way, your program will change the language according to the system settings. You can also manually call `trans.set_locale(\"zh\")` to temporarily switch the language for debugging. ## Displaying Translations on the Interface The previous examples used the `print` function to display translations. If you want to display them on the interface, you need font support. For English, it is supported by default, but for languages with large font libraries like Chinese, it is not supported by default. For example: ```python from maix import i18n, image, display, app, time trans_dict { \"zh\": { \"hello\": \"你好\" }, \"en\": { } } trans i18n.Trans(trans_dict) tr trans.tr trans.set_locale(\"zh\") disp display.Display() img image.Image(disp.width(), disp.height()) img.draw_string(10, 10, tr(\"hello\"), image.COLOR_WHITE, scale 2) disp.show(img) while not app.need_exit(): time.sleep_ms(100) ``` Running this will show a bunch of `?` because there is no Chinese font library. For the `image` module, you can load a font library. The system has a built in Chinese font library, or you can use your own font library: ```python from maix import i18n, image, display, app, time trans_dict { \"zh\": { \"hello\": \"你好\" }, \"en\": { } } trans i18n.Trans(trans_dict) tr trans.tr trans.set_locale(\"zh\") disp display.Display() image.load_font(\"sourcehansans\", \"/maixapp/share/font/SourceHanSansCN Regular.otf\", size 24) image.set_default_font(\"sourcehansans\") img image.Image(disp.width(), disp.height()) img.draw_string(10, 10, tr(\"hello\"), image.COLOR_WHITE, scale 2) disp.show(img) while not app.need_exit(): time.sleep_ms(100) ```"},"/maixpy/doc/en/video/jpeg_streaming.html":{"title":"MaixCAM MaixPy Video Stream JPEG Streaming / Sending Images to Server","content":" title: MaixCAM MaixPy Video Stream JPEG Streaming / Sending Images to Server update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial document date: 2024 05 20 author: lxowalle version: 1.0.1 content: update JPEG HTTP usage ## Introduction Sometimes it is necessary to send images to a server, or to push video from a webcam to a server, so here are two ways to do it. One of the simplest methods is to compress images into `JPEG` format and send them one by one to the server. Note, this is a very basic method and not a formal way to stream video. It is also not suitable for high resolution, high frame rate video streams, as it involves sending images one by one. For more efficient video streaming, please use the `RTSP` or `RTMP` modules discussed later. Set up an HTTP server, so that the PC side can be accessed directly through the browser. ## Methods for pushing streams as a client ```python from maix import image import requests # create image img image.Image(640, 480, image.Format.FMT_RGB) # draw something img.draw_rect(60, 60, 80, 80, image.Color.from_rgb(255, 0, 0)) # convert to jpeg jpeg img.to_format(image.Format.FMT_JPEG) # image.Format.FMT_PNG # get jpeg bytes jpeg_bytes jpeg.to_bytes() # faster way, borrow memory from jpeg object, # but be careful, when jpeg object is deleted, jpeg_bytes object MUST NOT be used, or program will crash # jpeg_bytes jpeg.to_bytes(copy False) # send image binary bytes to server url \"http://192.168.0.123:8080/upload\" res requests.post(url, data jpeg_bytes) print(res.status_code) print(res.text) ``` As you can see, the image is first converted into `JPEG` format, and then the binary data of the `JPEG` image is sent to the server via `TCP`. ## Methods for pushing streams as a server ```python from maix import camera, time, app, http html \"\"\"<!DOCTYPE html> <html> <head> <title>JPG Stream</title> </head> <body> <h1>MaixPy JPG Stream</h1> <img src \"/stream\" alt \"Stream\"> </body> </html>\"\"\" cam camera.Camera(320, 240) stream http.JpegStreamer() stream.set_html(html) stream.start() print(\"http://{}:{}\".format(stream.host(), stream.port())) while not app.need_exit(): t time.ticks_ms() img cam.read() jpg img.to_jpeg() stream.write(jpg) print(f\"time: {time.ticks_ms() t}ms, fps: {1000 / (time.ticks_ms() t)}\") ``` Steps: 1. Import the image, camera and http modules: ```python from maix import image, camera, http ``` 2. Initialize the camera: ```python cam camera.Camera(320, 240) ``` 3. Initialize Stream Object ```python stream http.JpegStreamer() stream.start() ``` `http.JpegStreamer()` is used to create a `JpegStreamer` object, which will start an `http server` that will be used to publish `jpeg` image streams to clients. `stream.start()` is used to start the `http server`. 4. Custom html styles (optional) ```python html \"\"\"<!DOCTYPE html> <html> <head> <title>JPG Stream</title> </head> <body> <h1>MaixPy JPG Stream</h1> <img src \"/stream\" alt \"Stream\"> </body> </html>\"\"\" stream.set_html(html) ``` `html xxx` is the `html` code that can be used to customise the style of your web page. Note that the core code is `<img src ‘/stream’ alt ‘Stream’>`, be sure not to miss this line of code. `stream.set_html(html)` is used to set the custom `html` code, this step is optional. The default browsing address is `http://device_ip:8000`. 5. Getting images from the camera and pushing streams ```python while 1: img cam.read() jpg img.to_jpeg() stream.write(jpg) ``` `img cam.read()` gets an image from the camera, when initialised as `cam camera.Camera(320, 240)` the `img` object is an RGB image with a resolution of 320x240. `jpg img.to_jpeg()` converts the image to `jpeg` format `stream.write(jpg)` writes the image format to the server and the `http` server will send this image to the `http` client. 6. 6. Done, after running the code above, you can see the video stream directly through your browser, the default address is `http://device_ip:8000`. Open your browser and take a look!"},"/maixpy/doc/en/video/play.html":{"title":"MaixPy Playback Video","content":" title: MaixPy Playback Video update: date: 2024 08 19 author: lxowalle version: 1.0.0 content: Initial document ## Introduction This document provides instructions for using the Play Video feature. `MaixPy` supports playing `h264`, `mp4` and `flv` video formats, note that currently only `avc` encoded `mp4` and `flv` files are supported. Additionally, due to hardware encoder limitations, if you encounter issues decoding the video during playback, try re encoding it with `ffmpeg` and then play it again. Refer to the following command: ```shell ffmpeg i input_video.mp4 c:v libx264 x264opts \"bframes 0\" c:a aac strict experimental output_video.mp4 ``` ## Play `MP4` video An example of playing an `mp4` video, the path to the video file is `/root/output.mp4`. ```python from maix import video, display, app, time disp display.Display() d video.Decoder('/root/output.mp4') print(f'resolution: {d.width()}x{d.height()} bitrate: {d.bitrate()} fps: {d.fps()}') d.seek(0) while not app.need_exit(): t time.ticks_ms() ctx d.decode_video() if not ctx: d.seek(0) continue img ctx.image() disp.show(img) wait_ms (ctx.duration_us() // 1000) (time.ticks_ms() t) wait_ms wait_ms if wait_ms > 0 else 0 time.sleep_ms(wait_ms) ``` Steps: 1. Import the module and initialise the camera ```python from maix import video, display, app disp display.Display() ``` `disp display.Display()` is used to initialise the display to show the decoded image 2. Initialise the `Decoder` module ```python d video.Decoder('/root/output.mp4') ``` `d video.Decoder(‘/root/output.mp4’)` is used to initialise the decoder and set the path to the video file that needs to be played. If you need to play `flv` files, you can fill in the path of the file with `flv` suffix, such as `{your_file_path}.flv`, if you need to play `h264` files, you can fill in the path of the file with `h264` suffix, such as `{your_file_path}.h264` 3. Set the decoding location ```python d.seek(0) ``` can be used to set the position of the video to be played, in seconds. 4. Get the decoded image ```python ctx d.decode_video() img ctx.image() ``` Each call returns a frame context, and you can obtain img through `ctx.image()`. Currently the decoded output only supports the NV21 format. 5. Display the decoded image ```python disp.show(img) ``` When displaying images, you can use `ctx.duration_us()` to obtain the duration of each frame. It can be used to control the playback speed, and the unit is microseconds. 6. Done, see [API documentation](https://wiki.sipeed.com/maixpy/api/maix/video.html) for more usage of `Decoder`."},"/maixpy/doc/en/video/record.html":{"title":"MaixCAM MaixPy Video Record","content":" title: MaixCAM MaixPy Video Record update: date: 2024 05 20 author: lxowalle version: 1.0.0 content: Initial document ## Introduction This document provides instructions on how to use the video recording feature ## Example 1 An example of recording a video in `h265` format. ```python from maix import video, image, camera, app, time cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) e video.Encoder(width cam.width(), height cam.height()) f open('/root/output.h265', 'wb') record_ms 5000 start_ms time.ticks_ms() while not app.need_exit(): img cam.read() frame e.encode(img) print(frame.size()) f.write(frame.to_bytes()) if time.ticks_ms() start_ms > record_ms: app.set_exit_flag(True) ``` 步骤： 1. import module and Initialize the camera ```python from maix import video, image, camera, app, time cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) ``` `camera.Camera()` is used to initialise the camera, here the camera resolution is initialised to `640x480`, currently the `Encoder` only supports the `NV21` format, so set the image format to `image.Format.FMT_YVU420SP`. 2. Initialise the `Encoder` module ```python e video.Encoder() ``` The `video.Encoder()` module currently only supports processing `image.Format.FMT_YVU420SP` format images, which supports `h265` and `h264` encoding, and defaults to `h265` encoding. If you want to use `h264` encoding, then you can change the initialisation parameter to ` video.Encoder(type video.VideoType.VIDEO_H264_CBR)`. Note that only one encoder can exist at the same time 3. Encoding the camera image ```python img cam.read() frame e.encode(img) ``` `img cam.read()` read camera image and save to `img` `frame e.encode(img)` encode `img` and save result to `frame` 4. Save the encoded result to file ```python f open('/root/output.h265', 'wb') f.write(frame.to_bytes(False)) ``` `f open(xxx)` opens and creates a file `f.write(frame.to_bytes(False))` converts the encoding result `frame` to type `bytes` and then calls `f.write()` to write the data to the file 5. Timed 2s exit ```python record_ms 5000 start_ms time.ticks_ms() while not app.need_exit(): if time.ticks_ms() start_ms > record_ms: app.set_exit_flag(True) ``` Here is the application logic for the timed exit, see the code for yourself 6. Done ## Example 2 An example of recording a video in `h265` format. ```python from maix import video, time, image, camera, app cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) e video.Encoder(width cam.width(), height cam.height(), capture True) e.bind_camera(cam) f open('/root/output.h265', 'wb') record_ms 5000 start_ms time.ticks_ms() while not app.need_exit(): frame e.encode() img e.capture() print(frame.size()) f.write(frame.to_bytes(True)) if time.ticks_ms() start_ms > record_ms: app.set_exit_flag(True) ``` Similar to example 1, the difference is that the `Encoder` object's `bind_camera` method is called, and the `Encoder` takes the initiative to get the camera image, which has the advantage of using the hardware features to increase the encoding speed. ``` e video.Encoder(capture True) e.bind_camera(cam) frame e.encode() img e.capture() ``` `e video.Encoder(capture True)` enables the `capture` parameter to allow encoding to capture encoded images when encoding `e.bind_camera(cam)` binds the camera to the `Encoder` object `frame e.encode()` Instead of passing in `img` when encoding, fetch the image from the camera internally `img e.capture()` captures the encoded image from the `Encoder` object, which can be used for image processing ## Convert to MP4 format If you want to record video in `mp4` format, you can record `H265` video first, and then use the `ffmpeg` tool in the system to convert to `mp4` format. ```python import os # Pack h265 to mp4 # /root/output.h265 is the h265 file path # /root/output.mp4 is the mp4 file path os.system('ffmpeg loglevel quiet i /root/output.h265 c:v copy c:a copy /root/output.mp4 y') ```"},"/maixpy/doc/en/video/uvc_streaming.html":{"title":"MaixCAM MaixPy Video Streaming UVC Streaming / As a UVC camera to display custom image","content":" title: MaixCAM MaixPy Video Streaming UVC Streaming / As a UVC camera to display custom image update: date: 2024 12 20 author: taorye version: 1.0.0 content: 初版文档 ## Introduction `MaixCAM` as a `UVC camera`, where `UVC` stands for `USB video(device) class`. Here, two methods are provided to display custom content: Refresh the target image using the `show` method of `maix.uvc.UvcStreamer` (supports YUYV and MJPEG). Refresh the target image by registering a custom image refresh callback function with `maix.uvc.UvcServer` (only supports MJPEG). Much more complex than the method above. ## Example **First, you need to enable the `UVC` function in the `USB settings` section of the `Settings` app.** After connecting the USB cable: Windows users can go to Settings → Bluetooth & devices → Cameras, and see the UVC Camera device. Clicking on it will preview a static image of a small cat. Linux users need to download the `guvcview` software, select the resolution of 320x240 and the format as MJPG. You will see two cats with some garbled characters in between (this happens because the actual resolution of the cat image is 224x224, and the software automatically tries to fill the remaining space with another thing). Simply use the correct resolution normally. Note: The version of guvcview used in Ubuntu 22 and earlier systems is `2.0.7`, which is known that the colors are displayed incorrectly, with a strong green tint. To fix it, please upgrade to a higher version. The version currently in use by the author is `2.2.1`. Ubuntu/Debian users can try to find a relevant PPA (Personal Package Archive) to install a newer version of guvcview. **Note:** Once the `UVC` function is enabled, due to Linux's implementation of the `UVC Gadget`, a user program is still required to handle `UVC` device events. Otherwise, the entire `USB` functionality will pause and wait, affecting other simultaneously enabled `Gadget` features like `Rndis` and `NCM`, which may cause network disconnection. Therefore, for users who also need other `USB` functionalities, it is recommended to use the `UvcStreamer` method when developing UVC display functionality based on `MaixPy`. Otherwise, ensure that the `MaixCAM` device has other network access methods, such as `WIFI`, to ensure proper development and debugging. <video controls autoplay src \"../../assets/maixcam pro_uvcdemo.mp4\" type \"video/mp4\"> Your browser does not support video playback. </video> ### UvcStreamer This method does not affect normal USB functionality. The underlying principle is to split the task into two processes. The official implementation uses a `server` process to handle `UVC` device events and encapsulates an easy to use, unified image refresh interface `show(img)` for users. You can treat it as a simple `display` linear logic operation. **Reference example source code path:** `MaixPy/examples/vision/streaming/uvc_stream.py` ### **Example Source (Usage Instructions):** 1. **Initialize the UvcStreamer object** ```python uvcs uvc.UvcStreamer() ``` (Optional) Switch to MJPEG streaming mode (YUYV default) ```python uvcs.use_mjpg(1) ``` 2. Refresh the image (automatically handles the format, medium performance loss for MJPEG, and high loss for YUYV) ```python uvcs.show(img) ``` ### UvcServer This approach offers high performance with a single process implementation, but USB functionality will only be available when the process is running. Therefore, when stopping this process, it's important to note that the enabled `Rndis` and `NCM` functionalities will temporarily become inactive, causing a network disconnection. **Reference example source code path:** `MaixPy/examples/vision/streaming/uvc_server.py` **Also packaged as an app source code path:** `MaixCDK/projects/app_uvc_camera/main/src/main.cpp` ### **Example Source (Usage Instructions):** 1. **Initialize the UvcServer object (requires a callback function for image refresh)** A helper function `helper_fill_mjpg_image` is provided to assist in placing more general `Image` objects into the `UVC` buffer. ```python cam camera.Camera(640, 360, fps 60) # Manually set resolution # 手动设置分辨率 def fill_mjpg_img_cb(buf, size): img cam.read() return uvc.helper_fill_mjpg_image(buf, size, img) uvcs uvc.UvcServer(fill_mjpg_img_cb) ``` The reference implementation will `fill_mjpg_img_cb` only trigger a buffer refresh when it returns `0`. Therefore, it is recommended to use the helper function in the last line: `return uvc.helper_fill_mjpg_image(buf, size, img)` 2. Start the UVC, which launches a background thread, non blocking operation: ```python uvcs.run() ``` 3. Stop the UVC when it's no longer needed. This will restore the background process from the `UvcStreamer` method implementation to ensure normal USB functionality. Currently, there is a **BUG** in the MaixPy framework where it forcibly terminates processes upon exit, preventing the functions after the `while not app.need_exit():` loop from being executed, meaning the `stop()` method may not run as expected. Therefore, for users who require **normal USB functionality**, it is recommended to switch to the `UvcStreamer` method or use the original C++ API from **MaixCDK**. **Reference example:** `MaixCDK/examples/uvc_demo/main/src/main.cpp` ```python uvcs.stop() ```"},"/maixpy/doc/en/video/webrtc_streaming.html":{"title":"MaixCAM MaixPy Video Streaming WebRTC Push Streaming","content":" title: MaixCAM MaixPy Video Streaming WebRTC Push Streaming update: date: 2025 12 12 author: 916BGAI version: 1.0.0 content: Initial documentation <br/> > WebRTC streaming requires a browser that supports the WebRTC protocol. Please use the latest version of Chrome for testing. ## Introduction This document explains how to push camera video using WebRTC. ## Usage ```python from maix import time, webrtc, camera, image cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) server webrtc.WebRTC() server.bind_camera(cam) server.start() print(server.get_url()) while True: time.sleep(1) ``` Steps: 1. Import the `time`, `webrtc`, `camera`, and `image` modules ```python from maix import time, webrtc, camera, image ``` 2. Initialize the camera ```python # Initialize the camera with output resolution 640x480 and NV21 format cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) ``` Note: The WebRTC module currently only supports NV21 format, so the camera must be configured to output NV21. 3. Initialize and start the WebRTC server ```python server webrtc.WebRTC() server.bind_camera(cam) server.start() ``` `server webrtc.WebRTC()` creates a `WebRTC` instance. `server.bind_camera(cam)` binds a `Camera` instance to the WebRTC server. After binding, the original `Camera` object should no longer be used directly. `server.start()` starts the WebRTC streaming service. 4. Print the WebRTC stream URL ```python print(server.get_url()) ``` `server.get_url()` returns the playback URL for the WebRTC stream. 5. Done — after running the script above, open the printed URL in your browser to view the camera stream."},"/maixpy/doc/en/video/rtsp_streaming.html":{"title":"MaixCAM MaixPy Video Streaming RTSP Push Streaming","content":" title: MaixCAM MaixPy Video Streaming RTSP Push Streaming update: date: 2024 05 20 author: lxowalle version: 1.0.0 content: Initial documentation ## Introduction This document provides methods for pushing streaming camera image via RTSP ## How to use ```python from maix import time, rtsp, camera, image server rtsp.Rtsp() cam camera.Camera(2560, 1440, image.Format.FMT_YVU420SP) server.bind_camera(cam) server.start() print(server.get_url()) while True: time.sleep(1) ``` Steps: 1. Import the image、camera、image and rtsp modules: ```python from maix import time, rtsp, camera, image ``` 2. Initialize the camera: ```python cam camera.Camera(2560, 1440, image.Format.FMT_YVU420SP) # Initialise camera, output resolution 2560x1440 NV21 format ``` Note that the RTSP module currently only supports the NV21 format, so the camera needs to be configured to output in NV21 format. 3. Initialise and start the Rtsp object ```python server rtsp.Rtsp() server.bind_camera(cam) server.start() ``` ``server rtsp.Rtsp()`` used to create an ``Rtsp`` object `server.bind_camera(cam)` is used to bind a `Camera` object, after which the original `Camera` object can no longer be used. `server.start()` is used to start the `rtsp` push stream. 4. Print the URL of the current RTSP stream ``python print(server.get_url()) `` ``server.get_url()`` is used to get the ``playback address`` of ``RTSP``. 6. Finished, after running the above code, you can play the video stream through [VLC](https://www.videolan.org/vlc/) software, the tested version of `VLC` is `3.0.20`. The default playback address is `rtsp://device ip:8554/live`. ## Support for Streaming Audio MaixPy supports streaming both audio and video simultaneously. By binding a `Recorder` object, > Note: This method is supported in MaixPy v4.7.9 and later versions The following is an example code: ```python from maix import time, rtsp, camera, image, audio cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) audio_recorder audio.Recorder() server rtsp.Rtsp() server.bind_camera(cam) server.bind_audio_recorder(audio_recorder) server.start() print(server.get_url()) while True: time.sleep(1) ``` In the code above, an `audio_recorder` object is created using `audio.Recorder()`, and the `bind_audio_recorder()` method of the `Rtsp` server is used to bind this object. You can use `ffplay rtsp://<device ip>:8554/live` or `vlc 3.0.20` to receive the audio and video stream."},"/maixpy/doc/en/video/rtmp_streaming.html":{"title":"MaixCAM MaixPy Video Streaming RTMP Push Streaming","content":" title: MaixCAM MaixPy Video Streaming RTMP Push Streaming update: date: 2024 05 31 author: lxowalle version: 1.0.0 content: initial document ## Introduction This document provides methods for pushing H264 video streams via RTMP ## How to use The following example shows pushing an h264 video stream to `rtmp://192.168.0.30:1935/live/stream` ```python from maix import camera, time, rtmp, image cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) # rtmp://192.168.0.30:1935/live/stream host '192.168.0.30' port 1935 app 'live' stream 'stream' bitrate 1000_000 r rtmp.Rtmp(host, port, app, stream, bitrate) r.bind_camera(cam) r.start() while True: time.sleep(1) ``` Steps: 1. Import the camera、rtmp、time and image modules: ```python from maix import camera, time, rtmp, image ``` 2. Initialize the camera: ```python cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) # Initialise camera, output resolution 640x480 NV21 format ``` Note that the RTMP module currently only supports the NV21 format, so the camera needs to be configured to output in NV21 format. 3. Initialise and start the Rtmp object ```python r rtmp.Rtmp(host, port, app, stream, bitrate) r.bind_camera(cam) r.start() ``` `r rtmp.Rtmp(host, port, app, stream, bitrate)` is used to create an `Rtmp` object, where `host` refers to the ip address or domain of the rtmp server, `app` refers to the name of the application that is open to the rtmp server, and `stream` refers to the name of the rtmp stream, which can also be used as the key for pushing the stream `r.bind_camera(cam)` is used to bind a `Camera` object, the original `Camera` object can not be used after binding. `r.start()` is used to start the `rtmp` stream. 4. Done ## Support for Streaming Audio MaixPy supports streaming both audio and video simultaneously. By binding a `Recorder` object, > Note: This method is supported in MaixPy v4.7.9 and later versions The following is an example code: ```python from maix import camera, time, app, rtmp, image, audio cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) audio_recorder audio.Recorder() host \"192.168.0.63\" port 1935 app_name \"live\" stream_name \"stream\" client rtmp.Rtmp(host, port, app_name, stream_name) client.bind_camera(cam) client.bind_audio_recorder(audio_recorder) client.start() print(f\"rtmp://{host}:{port}/{app_name}/{stream_name}\") while not app.need_exit(): time.sleep(1) ``` In the code above, an `audio_recorder` object is created using `audio.Recorder()`, and the `bind_audio_recorder()` method of the `Rtmp` server is used to bind this object. Stream the audio data while pushing the video stream. ## Push streaming test to Bilibili ### Launch bilibili live stream 1. Click on Live Streaming ![](../../../static/image/bilibili_click_live.png) 2. Click on Live Streaming Settings ![](../../../static/image/bilibili_click_live_setting.png) 3. Find the live streaming address ![](../../../static/image/bilibili_check_live_link.png) 4. Scroll down, select a category, and click Start Live! ![](../../../static/image/bilibili_live_start.png) 5. Get the push stream address ![](../../../static/image/bilibili_check_rtmp_url.png) server address: `rtmp://live push.bilivideo.com/live bvc` key：`?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1` Push stream address: `rtmp://live push.bilivideo.com/live bvc/?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1` ### Run the RTMP client ```python from maix import camera, time, rtmp, image cam camera.Camera(640, 480, image.Format.FMT_YVU420SP) # rtmp://live push.bilivideo.com/live bvc/?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1 host 'live push.bilivideo.com' port 1935 app 'live bvc' stream '?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1' bitrate 1000_000 r rtmp.Rtmp(host, port, app, stream, bitrate) r.bind_camera(cam) r.start() while True: time.sleep(1) ``` Above get bilibili's push stream address as `rtmp://live push.bilivideo.com/live bvc/?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1` Can be detached: 1. server address is `live push.bilivideo.com` 2. port is `1935`, if there is no port number, the default is `1935` 3. application name is `live bvc` 4. stream name is `?streamname live_xxxx&key 1fbfxxxxxxxxxxxxxffe0&schedule rtmp&pflag 1` Run the code and you will be able to see the `maixcam` screen in the live stream, if you find that the live stream is not displayed, try to close the live stream first, then reopen it and run the code again. Try it~!"},"/maixpy/doc/en/modules/tmc2209.html":{"title":"MaixPy tmc2209 单串口驱动使用介绍","content":" title: MaixPy tmc2209 单串口驱动使用介绍 update: date: 2024 08 21 author: iawak9lkm version: 1.0.0 content: 初版文档 ## Introduction to TMC2209 TMC2209 is a stepper motor driver chip produced by the German company Trinamic. It is designed specifically for 2 phase stepper motors, featuring low power consumption, high efficiency, and excellent noise suppression capabilities. TMC2209 supports currents up to 2.8A, making it suitable for various applications such as 3D printers, CNC machines, robots, etc. ## Using TMC2209 to Drive Stepper Motors in MaixPy * Ensure that your stepper motor is a 2 phase 4 wire type, and confirm the step angle of your motor (step_angle), the microstepping resolution you need (micro_step), and the distance the load moves per revolution of the motor (screw_pitch or round_mm). This information will help us configure the driver parameters later. * Generally, TMC2209 driver boards on the market have the following pins (if you find it troublesome, you can purchase our TMC2209 driver board, link [not yet available,敬请期待]): ``` EN VM MS1 GND MS2 2B RX 2A TX 1A NC 1B STEP VDD DIR GND ``` `EN`: EN is the enable pin. Connect this pin to `GND` to enable TMC2209 hardware wise. `MS1`: MS1 is one of the microstepping selection pins, used in conjunction with the MS2 pin to set the microstepping mode of the stepper motor. `MS2`: MS2 is one of the microstepping selection pins, used in conjunction with the MS1 pin to set the microstepping mode of the stepper motor. **This driver program only uses the UART mode of TMC2209. In this mode, the two microstep selection pins are respectively `AD0` (originally `MS1`) and `AD1` (originally `MS2`). The level states of these two pins together form the UART address of the TMC2209, ranging from 0x00 to 0x03.** `TX`: TX is the serial communication transmit pin, used for communication with an external microcontroller via UART. `RX`: RX is the serial communication receive pin, used for communication with an external microcontroller via UART. When using both `RX` and `TX` on TMC2209, ensure there is a 1K ohm resistor between the `RX` of the TMC2209 driver board and the `TX` of the main control chip. Otherwise, communication data anomalies may occur. `NC`: NC is the no connect pin, indicating that this pin does not need to be connected during normal use. `STEP`: STEP is the step signal input pin. Each pulse received advances the stepper motor by one step angle. Since this driver is purely UART driven, this pin does not need to be connected and can be left floating. `DIR`: DIR is the direction signal input pin, used to control the rotation direction of the stepper motor. When DIR is high, the motor rotates clockwise; when DIR is low, the motor rotates counterclockwise. Since this driver is purely UART driven, this pin does not need to be connected and can be left floating. `VM`: VM is the power input pin, connected to the positive terminal of the stepper motor's power supply. `GND`: GND is the ground pin, connected to the negative terminal of the power supply. `2B`, `2A`, `1B`, `1A`: These pins are the phase output pins of the stepper motor, connected to the two phases of the motor's coils. `VDD`: VDD is the logic power input pin, providing power to the internal logic circuits of the chip. * Using TMC2209 Driver in MaixPy As an example, let's consider a stepper motor with a step angle of 18, a microstep resolution of 256, and a screw pitch of 3mm: ```python from maix import pinmap, ext_dev, err, time port \"/dev/ttyS1\" uart_addr 0x00 uart_baudrate 115200 step_angle 18 micro_step 256 screw_pitch 3 speed 6 use_internal_sense_resistors True run_current_per 100 hold_current_per 100 if port \"/dev/ttyS1\": ret pinmap.set_pin_function(\"A19\", \"UART1_TX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ret pinmap.set_pin_function(\"A18\", \"UART1_RX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) slide ext_dev.tmc2209.ScrewSlide(port, uart_addr, uart_baudrate, step_angle, micro_step, screw_pitch, speed, use_internal_sense_resistors, run_current_per, hold_current_per) def reset_callback() > bool: if 2 > 1: # An event occurs (e.g., a sensor is triggered), # indicating that the slide has moved to the boundary and the motor needs to stop. print(\"Reset finish...\") return True # Not occurred, no need to stop the motor. return False def move_callback(per:float) > bool: # per is the percentage of the current distance moved by move() # out of the total distance required for the current move(), ranging from 0 to 100. print(f\"Slide moving... {per}\") if per > 50: # Example: Stop moving when 50% of the total distance for the current move() has been covered. print(f\"{per} > 50%, stop.\") return True return False slide.reset(reset_callback) slide.move(screw_pitch*2, 1, move_callback) slide.move( screw_pitch) while True: slide.move(screw_pitch*2) slide.move( (screw_pitch*2)) time.sleep_ms(100) ``` First, ensure that UART1 is enabled using `pinmap` in the program. Then create a `ScrewSlide` object, using the internal reference resistor by default, and defaulting to 100% of the motor's running current and 100% of the motor's holding current. These parameters may need to be adjusted according to your motor. Next, the routine declares a reset callback function and a move callback function, which are respectively passed into the `reset()` function and `move()` function. The `reset()` and `move()` functions call the callback functions periodically to confirm whether the motor needs to be stopped immediately (when the callback function returns True). Both the `move()` and `reset()` functions are blocking functions, and they will only stop the motor and return when the callback function returns True (or when the specified length of movement is completed in the case of `move()`). ## Using tmc2209 Driver for Stepper Motors with Constant Load in MaixPy **!!!Screw stepper motors with constant load should not be considered as stepper motors with constant load, because screw stepper motors have limit devices to ensure the direction of motion of the load on the rod is known, and the screw stepper motor often collides with the limit device during operation, causing the motor load to not be constant. Other cases can be deduced by analogy to know whether it is a stepper motor with constant load.** In some application scenarios, the load on the stepper motor is constant throughout, and only increases when it hits an edge and stalls. In such cases, you can use the `Slide` class instead of the `ScrewSlide` class, where `Slide` has stall detection functionality. Using `ScrewSlide` is also feasible, it does not have stall detection but is more flexible. Please choose between these two classes based on the usage scenario; this section only discusses the `Slide` class. * Implementation Principle The TMC2209 has an internal register `SG_RESULT`, which stores data proportional to the remaining torque of the motor. If the motor load is constant, the variation in the register value is very small. When the motor stalls, the register value will rapidly decrease and maintain a lower value. By finding the running average value and stall average value of this register for the constant load motor, you can measure whether the motor is stalling at any given moment. * Obtaining the Average Value of the `SG_RESULT` Register The `maix.ext_dev.tmc2209` module provides a function to obtain and save this average value, `maix.ext_dev.tmc2209.slide_scan`. example: ```python from maix import ext_dev, pinmap, err port \"/dev/ttyS1\" uart_addr 0x00 uart_baudrate 115200 step_angle 1.8 micro_step 256 round_mm 60 speed 60 use_internal_sense_resistors True run_current_per 100 hold_current_per 100 if port \"/dev/ttyS1\": ret pinmap.set_pin_function(\"A19\", \"UART1_TX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ret pinmap.set_pin_function(\"A18\", \"UART1_RX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ext_dev.tmc2209.slide_scan(port, uart_addr, uart_baudrate, step_angle, micro_step, round_mm, speed, True, True, run_current_per, hold_current_per, conf_save_path './slide_scan_example.bin', force_update False) ``` After configuring the serial port and driver parameters, call `slide_scan`. The last parameter of `slide_scan`, `force_update`, determines the behavior when the configuration file already exists: > If `force_update` is True, the old configuration will be overwritten with the new configuration. > > If `force_update` is False, the running average value will be updated to the average of the new and old values, and the stall average value will be updated to the larger of the new and old stall average values (for example, if a slide has left and right boundaries, and the left boundary stall average value is less than the right boundary stall average value, meaning the right boundary is more prone to stalling than the left boundary, the easiest stalling average value will be saved). After running this program, the stepper motor will continue to rotate forward until it encounters a stall. Wait about 300ms, then stop the program. The program will record the running average value of the `SG_RESULT` register and the stall average value to `conf_save_path`. Subsequently, the `Slide` class can load this configuration file to stop the motor when a stall is detected. * Verifying the Configuration File Values You may wonder if this configuration is actually usable. The `maix.ext_dev.tmc2209` module provides a function to test this configuration file, `slide_test`. First, ensure the motor is in a stalled state, then modify the parameters to match those used when calling `slide_scan`, and execute the following code. example ```python from maix import ext_dev, pinmap, err port \"/dev/ttyS1\" uart_addr 0x00 uart_baudrate 115200 step_angle 1.8 micro_step 256 round_mm 60 speed 60 use_internal_sense_resistors True run_current_per 100 hold_current_per 100 if port \"/dev/ttyS1\": ret pinmap.set_pin_function(\"A19\", \"UART1_TX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ret pinmap.set_pin_function(\"A18\", \"UART1_RX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ext_dev.tmc2209.slide_test(port, uart_addr, uart_baudrate, step_angle, micro_step, round_mm, speed, True, True, run_current_per, hold_current_per, conf_save_path './slide_scan_example.bin') ``` The motor will stop rotating instantly upon encountering a stall, and the program will end accordingly. The stall stop logic for `Slide.move()` and `Slide.reset()` is the same. * Using `Slide` The approach to using `Slide` is essentially the same as using `ScrewSlide`, except that `Slide` removes the callback function and adds stall stop logic. If a configuration file is not passed when using `Slide`, it can still be used. The stall detection threshold is the average at the start of motor operation multiplied by `Slide.stop_default_per()`/100. The motor stops when the recent average operation number is lower than this value. You can obtain and modify this value through `Slide.stop_default_per()`. ```python from maix import pinmap, ext_dev, err, time port \"/dev/ttyS1\" uart_addr 0x00 uart_baudrate 115200 step_angle 1.8 micro_step 256 round_mm 60 speed 60 use_internal_sense_resistors True run_current_per 100 hold_current_per 100 if port \"/dev/ttyS1\": ret pinmap.set_pin_function(\"A19\", \"UART1_TX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) ret pinmap.set_pin_function(\"A18\", \"UART1_RX\") if ret ! err.Err.ERR_NONE: print(\"Failed in function pinmap...\") exit( 1) slide ext_dev.tmc2209.Slide(port, uart_addr, uart_baudrate, step_angle, micro_step, round_mm, speed, cfg_file_path \"./slide_conf.bin\") slide.reset() slide.move(60) slide.move( 60) ``` ## Notes **This driver is implemented purely through UART, offering the advantage of requiring fewer pins to drive up to 4 motors with relatively high precision. The downside is that it is not suitable for applications requiring extremely high precision.** Known Issues: * Do not use UART0 of MaixCAM as the driver's serial port, as it may cause MaixCAM to fail to boot properly. **!!! If you find any bugs, we welcome you to submit a PR to report them.** ## Disclaimer This motor driver program (hereinafter referred to as the \"Program\") is developed by [Sipeed] based on the BSD 3 open source license repository [janelia arduino/TMC2209](https://github.com/janelia arduino/TMC2209). The Program is intended for learning and research purposes only and is not guaranteed to work under all environmental conditions. Users assume all risks associated with the use of this Program. [Sipeed] shall not be liable for any losses or damages arising from the use or inability to use the Program, including but not limited to direct, indirect, incidental, special, punitive, or consequential damages. Users should conduct sufficient testing and validation to ensure that the Program meets their specific requirements and environment before using it in practical applications. [Sipeed] makes no express or implied warranties regarding the accuracy, reliability, completeness, or suitability of the Program. Users are responsible for complying with all applicable laws and regulations when using the Program and ensuring that they do not infringe upon the legal rights of any third parties. [Sipeed] shall not be liable for any consequences resulting from users' violation of laws or infringement of third party rights. The interpretation of this disclaimer is reserved by [Sipeed], who also reserves the right to modify this disclaimer at any time."},"/maixpy/doc/en/modules/thermal_cam.html":{"title":"Using Thermal Infrared Image Sensors with MaixCAM and MaixPy","content":" title: Using Thermal Infrared Image Sensors with MaixCAM and MaixPy ## Demo Single thermal imaging effect: ![](../../assets/thermal002.jpg) Thermal imaging and visible light fusion effect (left: cold drink, right: a working mini PC): ![](../../assets/thermal001.png) This APP has been integrated into the MaixCAM Release image. Install the supported module and launch the `Thermal Camera` APP to use it. [APP Installation Link](https://maixhub.com/app/87) [Source Code Link](https://github.com/sipeed/MaixCDK/tree/main/projects/app_thermal_camera) ## Supported List * PMOD_Thermal32 [Purchase Inquiry Link](https://wiki.sipeed.com/en/store.html) ### PMOD_Thermal32 ![](../../assets/thermal003.jpg) PMOD_Thermal32 is an industry standard, fully calibrated 32*24 pixel thermal infrared array sensor. Module parameters: Parameter Name Value Resolution 32x24 Temperature Range 40～450℃ Frame Rate 1~30fps Interface I2C After installing the PMOD_Thermal32 module, you can use the API provided by MaixPy to obtain the temperature data matrix, pseudo color images, and the minimum, maximum, and center temperature values and their corresponding coordinates from the latest frame data. For details, see [Module API Documentation](../../../api/maix/ext_dev/mlx90640.html). You can also refer to our demo APP to write your application code."},"/maixpy/doc/en/modules/bluetooth.html":{"title":"MaixCAM Bluetooth Instructions","content":" title: MaixCAM Bluetooth Instructions update: date: 2025 04 08 author: lxowalle version: 1.0.0 content: Initial document date: 2026 01 12 author: lxowalle version: 1.0.1 content: Updated method for using Bluetooth ## Instructions ​\tBluetooth is a common short range wireless communication technology mainly used to establish low power, point to point or local network connections between two or more devices. It operates in the 2.4GHz frequency band and was originally designed to replace wired data cables for small data transmissions between devices. In modern life, Bluetooth has become a widely used technology in our daily activities. For example, when driving, we can connect a phone to the car’s Bluetooth system to make hands free calls or play music; Bluetooth headsets free us from the constraints of wired earphones when listening to music or making calls; in smart homes, Bluetooth enables interaction with smart locks, lighting, temperature and humidity sensors, and more, creating a convenient and efficient living environment. ## Usage Method > Note: Supported from MaixPy v4.12.4 onwards ### Enable Bluetooth The Bluetooth function is not enabled by default. Run the following command to enable Bluetooth. ```shell bluetoothctl power on ``` You can also run the following command to enable Bluetooth automatically on each boot. ```shell echo \"bluetoothctl power on\" >> /etc/rc.local ``` ### Install Dependencies Run the following command to install the `bleak` package. ```shell pip install bleak ``` ### Run Bluetooth Programs Scan for nearby Bluetooth devices ```python import asyncio from bleak import BleakScanner async def main(): devices await BleakScanner.discover() for d in devices: print(d) asyncio.run(main()) ``` Connect Bluetooth ```python import asyncio from bleak import BleakClient address \"24:21:32:e3:01:87\" MODEL_NBR_UUID \"1A2A\" async def main(address): async with BleakClient(address) as client: model_number await client.read_gatt_char(MODEL_NBR_UUID) print(f\"Model Number: {model_number.decode()}\") asyncio.run(main(address)) ``` Here, we only cover the basic usage of `bleak` for Bluetooth. If you have more development requirements, please refer to [here](https://bleak.readthedocs.io/en/latest/index.html#). ## Usage Method (Legacy, Not Recommended) > Note: `MaixPy v4.12.3` and earlier versions are supported ### Preparation ​\tMaixCAM and MaixCAM Pro come with a built in AIC8800D Wi Fi/Bluetooth dual mode chip. However, due to limited IO resources, the main controller is not connected to Bluetooth by default. To enable Bluetooth, you need to solder a 0 ohm resistor between the following pins and the Bluetooth path: `GPIOA18`, `GPIOA19`, `GPIOA28`, and `GPIOA29`. The position for soldering the 0 ohm resistors on MaixCAM is shown below: ![](../../assets/maixcam_enable_ble.png) The position for soldering the 0 ohm resistors on MaixCAM Pro is shown below: ![](../../assets/maixcam_pro_enable_ble.png) > Note: The following section demonstrates basic Bluetooth usage via the command line. For more advanced development needs, we encourage you to explore further on your own! ## Enabling Bluetooth ```shell hciattach n /dev/ttyS1 any 1500000 & hciconfig hci0 up ``` ## Connecting a Bluetooth Mouse Here, we use the `bluetoothctl` tool to configure Bluetooth: ```shell bluetoothctl\t\t\t# Start bluetoothctl ## Enter the following commands in the bluetoothctl terminal ## power on # Turn on Bluetooth agent on # Enable agent default agent # Set as default agent scan on # Start scanning for devices # After finding the target Bluetooth MAC address during scan pair {device MAC address} # Pair with the device trust {device MAC address} # Trust the device connect {device MAC address} # Connect to the device # Exit after successful connection exit ``` Verify mouse data: ```shell # Run hcidump to observe all HCI messages printed in the terminal hcidump # Run btmon to observe captured HCI events btmon ``` ## Other reference * [使用MaixCAM的蓝牙功能 · 硬件篇](https://maixhub.com/share/58) * [使用MaixCAM的蓝牙功能 · 软件篇](https://maixhub.com/share/62)"},"/maixpy/doc/en/modules/fp5510.html":{"title":"MaixPy FP5510 Instructions","content":" title: MaixPy FP5510 Instructions update: date: 2024 12 02 author: lxowalle version: 1.0.0 content: Initial document ## Overview The **FP5510** is a single 10 bit DAC with a 120mA output current voice coil motor, specifically designed for autofocus operations. It is commonly used in cameras, smartphones, and other electronic devices requiring focus adjustments. ## Using FP5510 in MaixPy MaixPy supports the operation of the `FP5510` through the `FP5510` object. Example Code: ```python from maix.ext_dev import fp5510 fp fp5510.FP5510() fp.set_pos(value) position fp.get_pos() print(f'set position to {position}') ``` Use the `fp5510.FP5510()` method to construct an object for controlling the `fp5510`. Typically, the FP5510 may have slave addresses of `0x0e` or `0x0c`. You can specify the slave address via the `slave_addr` parameter, e.g.: > **Note**: If the address of fp5510 is found to change between 0x0e and 0x0c, it may be because the `FP5510` shares a reset pin with the `camera`. When the reset pin is enabled, the FP5510 address is `0x0c`. When the reset pin is disabled, the FP5510 address changes to `0x0e`. ```python fp fp5510.FP5510(slave_addr 0x0c) ``` Use the `set_pos` method of the `FP5510` class to set the position of the voice coil motor. The range is [0, 1023]. For example: ```python fp.set_pos(500) ``` Use the `get_pos` method of the `FP5510` class to get the position of the voice coil motor. For example: ```python position fp.get_pos() print(f'set position to {position}') ```"},"/maixpy/doc/en/modules/pmu.html":{"title":"MaixCAM MaixPy Power Management Unit","content":" title: MaixCAM MaixPy Power Management Unit update: date: 2024 11 08 author: 916BGAI version: 1.0.0 content: Initial document <br/> >! Warning !!! >Setting incorrect voltages may damage the `MaixCAM Pro`. Do not modify the voltages of `DCDC2~DCDC5`, `ALDO1~ALDO4`, and `BLDO1~BLDO2` unless you fully understand the purpose and consequences of the adjustment. ## Introduction The `MaixCAM Pro` is equipped with the `AXP2101` Power Management Unit (PMU), providing multi channel power outputs, charging management, and system protection functions. The `AXP2101` supports linear charging and features `5` `DC DC` channels and 11 `LDO` channels, meeting a variety of power requirements. It also comes with multiple channel `ADCs` for real time monitoring of voltage and temperature, and integrates protection features such as over voltage, over current, and over temperature to ensure system stability and safety. > The MaixCAM does not have an onboard power management unit. If power management functionality is required, please connect an external power management unit. ## Using Power Management Unit in MaixPy Operate the AXP2101 device using the PMU module. Example: ```python from maix import time, app from maix.ext_dev import pmu p pmu.PMU(\"axp2101\") # Get battery percent print(f\"Battery percent: {p.get_bat_percent()}%\") # Set the max battery charging current p.set_bat_charging_cur(1000) print(f\"Max charging current: {p.get_bat_charging_cur()}mA\") # Set DCDC1 voltage (!!! Do not modify the voltage of other channels, # as it may damage the device.) old_dcdc1_voltage p.get_vol(pmu.PowerChannel.DCDC1) print(f\"Old DCDC1 voltage: {old_dcdc1_voltage}mV\") p.set_vol(pmu.PowerChannel.DCDC1, 3000) new_dcdc1_voltage p.get_vol(pmu.PowerChannel.DCDC1) print(f\"New DCDC1 voltage: {new_dcdc1_voltage}mV\") # Get all channel voltages channels [ pmu.PowerChannel.DCDC1, pmu.PowerChannel.DCDC2, pmu.PowerChannel.DCDC3, pmu.PowerChannel.DCDC4, pmu.PowerChannel.DCDC5, pmu.PowerChannel.ALDO1, pmu.PowerChannel.ALDO2, pmu.PowerChannel.ALDO3, pmu.PowerChannel.ALDO4, pmu.PowerChannel.BLDO1, pmu.PowerChannel.BLDO2 ] print(\" All channel voltages: \") for channel in channels: print(f\"{channel.name}: {p.get_vol(channel)}\") print(\" \") # Poweroff (Important! Power will be cut off immediately) # p.poweroff() while not app.need_exit(): time.sleep_ms(1000) ``` > You can also use the `AXP2101` module to configure the power management unit. The usage is similar to the `PMU` module, and you can refer to the example script [axp2101_example.py](https://github.com/sipeed/MaixPy/blob/main/examples/ext_dev/pmu/pmu_axp2101/axp2101_example.py) for guidance. Initialize the `PMU` object, and call `get_bat_percent()` to get the current battery level. Call `set_bat_charging_cur()` to set the maximum charging current. Calling `poweroff()` will immediately cut off power to the device. Please ensure that data in memory is synced to disk before using this function. You can use the `set_vol()` and `get_vol()` methods to set and read the voltage of the `DC DC` and `LDO` channels, respectively. Currently, the following channels of the `AXP2101` are supported for voltage configuration: `DCDC1~DCDC5`, `ALDO1~ALDO4`, and `BLDO1~BLDO2`. >! Warning !!! >Setting incorrect voltages may damage the `MaixCAM Pro`. Do not modify the voltages of `DCDC2~DCDC5`, `ALDO1~ALDO4`, and `BLDO1~BLDO2` unless you fully understand the purpose and consequences of the adjustment. If you need to test, please use the `DCDC1` channel. For detailed information on the PMU API, please refer to the [PMU API Documentation](../../../api/maix/ext_dev/pmu.html)"},"/maixpy/doc/en/modules/spilcd.html":{"title":"MaixCAM MaixPy SPI LCD Screen","content":" title: MaixCAM MaixPy SPI LCD Screen update: date: 2024 12 02 author: 916BGAI version: 1.0.0 content: Initial document ## Introduction `MaixCAM` is equipped with three hardware SPI interfaces, allowing you to connect and drive an LCD screen via the SPI interface. > Currently, only hardware SPI is supported for driving the LCD screen, and it requires modification to the Linux kernel. Software SPI is not supported. > **Note：** Reading this document requires a certain level of knowledge in kernel compilation, kernel configuration, and kernel driver development. ## Using the ST7789 Screen This section uses the LCD screen driven by `ST7789` as an example. ### Get the LicheeRV Nano Build Source Code The base system used by `MaixCAM` is [https://github.com/sipeed/LicheeRV Nano Build](https://github.com/sipeed/LicheeRV Nano Build). First, pull the latest source code and follow the instructions in the [README](https://github.com/sipeed/LicheeRV Nano Build/blob/main/README.md) to build the system. ### Modify the Linux Kernel First, modify the kernel configuration to enable `FB_TFT` support. You can execute `menuconfig_kernel` in the root directory of LicheeRV Nano Build, then use the text based menu interface to configure it. The configuration option is located at: `Device Drivers > Staging drivers > Support for small TFT LCD display modules` Select the driver for the screen you are using; in this case, choose the `ST7789` driver, and compile it as a kernel module: `<M> FB driver for the ST7789 LCD Controller` > Alternatively, you can directly modify the configuration file `build/boards/sg200x/sg2002_licheervnano_sd/linux/sg2002_licheervnano_sd_defconfig` > by adding `CONFIG_FB_TFT y` and `CONFIG_FB_TFT_ST7789 m`. ### Modify the Device Tree Modify the device tree file `build/boards/sg200x/sg2002_licheervnano_sd/dts_riscv/sg2002_licheervnano_sd.dts`. ```c &spi2 { \tstatus \"okay\"; /delete node/ spidev@0; st7789: st7789@0{ \t\tcompatible \"sitronix,st7789\"; \t\treg <0>; \t\tstatus \"okay\"; \t\tspi max frequency <80000000>; \t\tspi cpol; \t\tspi cpha; \t\trotate <90>; \t\tfps <60>; \t\trgb; \t\tbuswidth <8>; \t\tdc <&porte 20 GPIO_ACTIVE_HIGH>; \t\treset <&porte 21 GPIO_ACTIVE_LOW>; \t\tdebug <0>; \t}; }; ``` This example uses `SPI2`. Since the Wi Fi module reuses the `SPI2` pins for `SDIO`, we need to modify the pin multiplexing. The method for modification is shown in the example below. After modification, the Wi Fi functionality will be unavailable. After modifying the device tree, recompile the image and generate the MaixCAM compatible image following the instructions in the [Compiling a System for MaixCAM](https://wiki.sipeed.com/maixpy/doc/en/pro/compile_os.html) guide. ### Test the Screen maixpy example: ```python from maix import pinmap, display, image, app import subprocess try: result subprocess.run(['lsmod'], capture_output True, text True, check True) if \"aic8800_bsp\" in result.stdout: subprocess.run(['rmmod', 'aic8800_fdrv'], check True) subprocess.run(['rmmod', 'aic8800_bsp'], check True) else: print(f\"aic8800 module is not currently loaded, skipping remove.\") except Exception as e: print(e) pinmap.set_pin_function(\"P18\", \"SPI2_CS\") pinmap.set_pin_function(\"P22\", \"SPI2_MOSI\") pinmap.set_pin_function(\"P23\", \"SPI2_SCK\") pinmap.set_pin_function(\"P20\", \"GPIOP20\") pinmap.set_pin_function(\"P21\", \"GPIOP21\") try: result subprocess.run(['lsmod'], capture_output True, text True, check True) if \"fb_st7789\" in result.stdout: print(f\"module is already loaded, skipping loading.\") else: subprocess.run(['insmod', '/mnt/system/ko/fb_st7789.ko'], check True) print(f\"load fb_st7789 success.\") except Exception as e: print(e) disp display.Display(device \"/dev/fb0\") print(\"display init done\") print(f\"display size: {disp.width()}x{disp.height()}\") y 0 while not app.need_exit(): img image.Image(disp.width(), disp.height(), image.Format.FMT_RGB888) img.draw_rect(0, y, image.string_size(\"Hello, MaixPy!\", scale 2).width() + 10, 80, color image.Color.from_rgb(255, 0, 0), thickness 1) img.draw_string(4, y + 4, \"Hello, MaixPy!\", color image.Color.from_rgb(255, 255, 255), scale 2) disp.show(img) y (y + 1) % disp.height() ``` First, remove the aic8800 driver module to prevent it from occupying the SDIO bus, which could interfere with the screen driver. Modify the pin multiplexing, mapping the corresponding pins to SPI functions. For detailed instructions on how to modify the pin multiplexing using pinmap, refer to [Using PINMAP in MaixCAM](https://wiki.sipeed.com/maixpy/doc/en/peripheral/pinmap.html). Then, use `insmod` to load the screen driver module. Check the system logs to confirm that the driver has been successfully loaded. You can also find the generated `fb0` device in the /dev directory. ```bash [ 1029.909582] fb_st7789: module is from the staging directory, the quality is unknown, you have been warned. [ 1029.911792] fb_st7789 spi2.0: fbtft_property_value: buswidth 8 [ 1029.911814] fb_st7789 spi2.0: fbtft_property_value: debug 0 [ 1029.911828] fb_st7789 spi2.0: fbtft_property_value: rotate 90 [ 1029.911842] fb_st7789 spi2.0: fbtft_property_value: fps 60 [ 1030.753696] graphics fb0: fb_st7789 frame buffer, 320x240, 150 KiB video memory, 4 KiB buffer memory, fps 62, spi2.0 at 80 MHz ``` Using the screen is straightforward. Simply specify the corresponding `fb` device when creating the `Display` instance. After that, you can use the `SPI` screen in the usual way ([MaixPy Screen Usage](https://wiki.sipeed.com/maixpy/doc/en/vision/display.html)). ```python disp display.Display(device \"/dev/fb0\") ``` ## Notes ### Screen Timing Issues The initialization timing for different screens may vary. For example, the `ST7789` includes different versions such as `ST7789V1` and `ST7789V2`, each with potentially different initialization timings. The drivers in the [LicheeRV Nano Build](https://github.com/sipeed/LicheeRV Nano Build) repository cannot guarantee that they will work properly with every st7789 screen. You can contact the supplier to obtain the specific initialization sequence for your screen and modify the `init_display` function in `LicheeRV Nano Build/linux_5.10/drivers/staging/fbtft/fb_st7789.c`. ### Pin Multiplexing When using `pinmap` to set pin multiplexing, ensure that it matches the pin configuration in the device tree. Generally, the `dc` and `reset` pins for SPI screens are not hardware bound, so you can specify them arbitrarily in the device tree. Simply choose pins that are not in use on the `MaixCAM` board, and then map them to GPIO functions using `pinmap`. ### Drive Other Screens Currently, the `fb` device has tested the `st7789` screen driver. There are other screen drivers available for testing in `linux_5.10/drivers/staging/fbtft`. If you encounter any issues, feel free to submit a `commit` or a `PR` to contribute."},"/maixpy/doc/en/modules/rtc.html":{"title":"Using the RTC Module with MaixCAM MaixPy","content":" title: Using the RTC Module with MaixCAM MaixPy The MaixCAM Pro has an onboard RTC module, which will automatically synchronize the system time upon power on and also sync time from the network. It will automatically re sync when there are changes in network status. Therefore, under normal circumstances, you don’t need to manually operate the RTC; you can directly use the system’s time API to get the current time. If you do need to manually operate the RTC, please refer to [bm8653 RTC Module Usage](./bm8653.html). Before manually operating the RTC, you can disable automatic synchronization by deleting the RTC and NTP related services in the system’s `/etc/init.d` directory. > MaixCAM does not have an onboard RTC."},"/maixpy/doc/en/modules/temp_humi.html":{"title":"Reading Temperature and Humidity Sensors with MaixCAM MaixPy","content":" title: Reading Temperature and Humidity Sensors with MaixCAM MaixPy ## Introduction By attaching a temperature and humidity sensor module to the MaixCAM, you can easily measure the environmental temperature and humidity. Here, we use the `Si7021` sensor as an example, which can be driven using `I2C`. For other sensors, you can request drivers from the manufacturer and use `I2C` or `SPI` to read data from them. ![](../../assets/si7021.png) **Note**: The power supply should be 3.3V. Connecting it to 5V may cause damage. Connect the `SCL` / `SDA` pins of the sensor to the corresponding `SCL` / `SDA` pins on the MaixCAM. For instance, on `I2C5`, this corresponds to `A15(SCL)` / `A27(SDA)`. ## Usage The complete code can be found in the [MaixPy/examples/ext_dev/sensors](https://github.com/sipeed/MaixPy/blob/main/examples/ext_dev/sensors) directory. Look for the `si7021` example. **Note**: The system image version must be `> 2024.6.3_maixpy_v4.2.1`."},"/maixpy/doc/en/modules/ahrs.html":{"title":"MaixCAM MaixPy Read IMU Accelerometer and Gyroscope for Attitude Estimation","content":" title: MaixCAM MaixPy Read IMU Accelerometer and Gyroscope for Attitude Estimation update: date: 2025 07 08 author: neucrack version: 1.0.0 content: Added attitude estimation code support and documentatio ## IMU Introduction IMU (Inertial Measurement Unit) typically consists of several parts: * Accelerometer: Measures linear acceleration, including gravitational acceleration. * Gyroscope: Measures angular velocity (rotation around an axis). * Magnetometer (optional): Measures magnetic field direction, used to assist in yaw calculation. Using data from these sensors, we can calculate the device's attitude, the angles of rotation along its three axes, also known as Euler angles. For example, in the following diagram, the `z` axis is up, the `y` axis is forward, and the `x` axis is right. This is a right handed coordinate system: ``` ^z / y(front) / / . ————————> x(right) ``` The rotation angle around the `x` axis is called `pitch`. The rotation angle around the `y` axis is called `roll`. The rotation angle around the `z` axis is called `yaw`. A system of sensor + reading + attitude estimation + output can be called `AHRS` (Attitude and Heading Reference System). With these three angles, we can determine the current orientation of our device, which can be used in many scenarios, such as: * Drones * Self balancing vehicles * Robots * Motion control * Anti shake * Direction and vibration detection * Motion judgment, gesture recognition, behavior analysis (can be combined with AI models) ## Hardware Support Some devices have a built in IMU, as shown below: Device Name Sensor Model Accelerometer Gyroscope Magnetometer Interface Features MaixCAM Pro QMI8658 ✅ Range: [±2, ±16]g ✅ Range: [±16, ±2048]°/s ❌ IIC4 Address 0x6B Low power<br>High stability<br>High sensitivity MaixCAM None ❌ ❌ ❌ ❌ MaixCAM2 LSM6DSOWTR ✅ Range: [±2, ±16]g ✅ Range: [±125, ±2000]°/s ❌ IIC1 Address 0x6B Low power<br>High stability<br>High sensitivity Besides using the built in IMU, you can also connect an external IMU sensor, such as the classic `MPU6050/MPU9150`. Search for the latest and suitable sensors yourself. In MaixPy, attitude estimation and IMU drivers are separated, so you can use your own driver for external sensors and still call the attitude estimation algorithm. ## MaixPy Read IMU Data Using MaixCAM Pro as an example, use `maix.imu.IMU` to read: ```python from maix.ext_dev import imu # Force calibrate first calibrate_first False # default config: acc + 2g 1KHz, gyro + 256rad/s 8KHz sensor imu.IMU(\"default\", mode imu.Mode.DUAL, acc_scale imu.AccScale.ACC_SCALE_2G, acc_odr imu.AccOdr.ACC_ODR_1000, gyro_scale imu.GyroScale.GYRO_SCALE_256DPS, gyro_odr imu.GyroOdr.GYRO_ODR_8000) # for gyro have bias, we need to calibrate first if calibrate_first or not sensor.calib_gyro_exists(): print(\"\\n\\nNeed calibrate fisrt\") print(\"now calibrate, please !! don't move !! device, wait for 10 seconds\") # calib_gyro will auto calculate and save bias, # the next time use load_calib_gyro load sensor.calib_gyro(10000) else: sensor.load_calib_gyro() while True: data sensor.read_all(calib_gryo True, radian False) msg \"acc: {:10.4f}, {:10.4f}, {:10.4f}, gyro: {:10.4f}, {:10.4f}, {:10.4f}, temp: {:4.1f}\".format( data.acc.x, data.acc.y, data.acc.z, data.gyro.x, data.gyro.y, data.gyro.z, data.temp ) print(msg) ``` Here, an `IMU` object is constructed, then `read_all` is called to read and print the data. Since the onboard IMU has no magnetometer, it's not printed. However, note that calibration is needed, as explained below. ## Data Calibration ### Gyroscope Calibration Why gyroscope calibration is needed: If not calibrated, we assume the gyroscope reads `x, y, z 0` when stationary, but in practice, there's an offset—known as \"zero drift\". As shown in the code, we need to perform `calib_gyro` once, which calibrates the gyroscope data. The principle is simple: collect multiple samples over time and average them. Here, `calib_gyro(10000)` samples for 10s and saves the result in `/maixapp/share/misc`. Next time, use `load_calib_gyro` to load the file, skipping recalibration. When reading data, `read_all(calib_gryo True)` will subtract the bias automatically. You can also set `calib_gyro False` and handle it manually. ### Accelerometer Calibration In theory, the accelerometer should also be calibrated, but its effect is less significant than the gyroscope. Not covered here—please refer to other resources. ### Magnetometer Calibration Similarly, magnetometers need calibration, such as the common ellipsoid calibration. Not covered here—please search and learn. ## Attitude Estimation Once we have sensor values, we can use an attitude estimation algorithm to obtain Euler angles. ### Basic Principles * Theoretically, gyroscopes can tell how much rotation occurred since the last moment: `angular velocity * dt rotation angle`. But due to drift, this is only reliable short term. * Accelerometers measure gravity. Since gravity is constant, when the device is still or moving at constant speed, we can use this to determine absolute orientation relative to the Earth. So we trust the accelerometer long term and gyroscope short term—they complement each other. * However, if the device rotates around an axis perpendicular to gravity, the accelerometer can't detect it. In this case, only the gyroscope helps, but drift may occur over time. * The magnetometer helps here since it points to Earth's magnetic north, making up for the accelerometer's blind spot. ### Attitude Estimation Algorithm There are many algorithms. MaixPy includes the **Mahony complementary filter**—lightweight and fast. Full code in [MaixPy/examples/ext\\_dev/sensors/imu/imu\\_ahrs\\_mahony.py](https://github.com/sipeed/MaixPy/tree/main/examples/ext_dev/sensors/imu) ```python ''' Mahony AHRS get device euler angle demo. Full Application code see: https://github.com/sipeed/MaixPy/tree/main/projects/app_imu_ahrs ''' from maix import ahrs, app, time from maix.ext_dev import imu # P of PI controller, a larger P (proportional gain) leads to faster response, # but it increases the risk of overshoot and oscillation. kp 2 # I of PI controller, a larger I (integral gain) helps to eliminate steady state errors more quickly, # but it can accumulate error over time, potentially causing instability or slow drift. ki 0.01 # Force calibrate first calibrate_first False # default config: acc + 2g 1KHz, gyro + 256rad/s 8KHz sensor imu.IMU(\"\", mode imu.Mode.DUAL, acc_scale imu.AccScale.ACC_SCALE_2G, acc_odr imu.AccOdr.ACC_ODR_1000, gyro_scale imu.GyroScale.GYRO_SCALE_256DPS, gyro_odr imu.GyroOdr.GYRO_ODR_8000) ahrs_filter ahrs.MahonyAHRS(kp, ki) # for gyro have bias, we need to calibrate first if calibrate_first or not sensor.calib_gyro_exists(): print(\"\\n\\nNeed calibrate fisrt\") print(\"now calibrate, please !! don't move !! device, wait for 10 seconds\") # calib_gyro will auto calculate and save bias, # the next time use load_calib_gyro load sensor.calib_gyro(10000) else: sensor.load_calib_gyro() # time.sleep(3) last_time time.ticks_s() while not app.need_exit(): # get imu data data sensor.read_all(calib_gryo True, radian True) # calculate angles t time.ticks_s() dt t last_time last_time t # print(f\"{data.mag.x:8.2f}, {data.mag.y:8.2f}, {data.mag.z:8.2f}, {data.gyro.z:8.2f}\") angle ahrs_filter.get_angle(data.acc, data.gyro, data.mag, dt, radian False) # ^z / y(front) # / # / # . ————————> x(right) # this demo's axis # x axis same with camera # angle.y 90 print(f\"pitch: {angle.x:8.2f}, roll: {angle.y:8.2f}, yaw: {angle.z:8.2f}, dt: {int(dt*1000):3d}ms, temp: {data.temp:.1f}\") # time.sleep_ms(1) ``` As shown, use `get_angle` to input raw sensor data and get Euler angles. If there's no magnetometer, set it to all zero. You may have noticed the use of a `PI` controller to adjust sensitivity. You can experiment with different values (learn about PID tuning): * `kp`: Proportional gain. Higher values respond faster but risk overshoot. * `ki`: Integral gain. Higher values eliminate errors faster but may cause instability or drift. Other parameters, such as the gyroscope's default range of `[ 256degree/s, 256degree/s]`, must be tuned for your scenario. Exceeding the range (e.g., rotating 100° but detecting only 60°) causes errors. Also, different settings affect sensitivity and noise levels. ## API Documentation For details on IMU APIs, see [IMU API Documentation](../../../api/maix/ext_dev/imu.html). For details on AHRS APIs, see [AHRS Documentation](../../../api/maix/ahrs.html)."},"/maixpy/doc/en/modules/bm8653.html":{"title":"MaixPy bm8653 Driver Instructions","content":" title: MaixPy bm8653 Driver Instructions update: date: 2024 08 27 author: iawak9lkm version: 1.0.0 content: Initial document ## Introduction to BM8653 BM8653 is a real time clock (RTC) chip widely used in various electronic devices to provide accurate time and date information. It features low power consumption and high precision, capable of continuing to operate via a backup battery when the device is powered off, ensuring the continuity and accuracy of time. ## Using BM8653 in MaixPy Using BM8653 in MaixPy is straightforward; you only need to know which I2C bus your platform's BM8653 is mounted on. The onboard BM8563 on the MaixCAM Pro is mounted on I2C 4. Example: ```python from maix import ext_dev, pinmap, err, time ### Enable I2C # ret pinmap.set_pin_function(\"PIN_NAME\", \"I2Cx_SCL\") # if ret ! err.Err.ERR_NONE: # print(\"Failed in function pinmap...\") # exit( 1) # ret pinmap.set_pin_function(\"PIN_NAME\", \"I2Cx_SDA\") # if ret ! err.Err.ERR_NONE: # print(\"Failed in function pinmap...\") # exit( 1) BM8653_I2CBUS_NUM 4 rtc ext_dev.bm8563.BM8563(BM8653_I2CBUS_NUM) ### 2020 12 31 23:59:45 t [2020, 12, 31, 23, 59, 45] # Set time rtc.datetime(t) while True: rtc_now rtc.datetime() print(f\"{rtc_now[0]} {rtc_now[1]} {rtc_now[2]} {rtc_now[3]}:{rtc_now[4]}:{rtc_now[5]}\") time.sleep(1) ``` If you are using the onboard BM8653 on the MaixCAM Pro, there is no need to enable I2C 4. The example demonstrates reading from and writing to the BM8653, setting or retrieving the current time. You can also use the following example to set the current time in the BM8653 to the system time, or set the current system time to the time in the BM8653. ```python from maix import ext_dev, pinmap, err, time ### Enable I2C # ret pinmap.set_pin_function(\"PIN_NAME\", \"I2Cx_SCL\") # if ret ! err.Err.ERR_NONE: # print(\"Failed in function pinmap...\") # exit( 1) # ret pinmap.set_pin_function(\"PIN_NAME\", \"I2Cx_SDA\") # if ret ! err.Err.ERR_NONE: # print(\"Failed in function pinmap...\") # exit( 1) BM8653_I2CBUS_NUM 4 rtc ext_dev.bm8563.BM8563(BM8653_I2CBUS_NUM) ### Update RTC time from system rtc.systohc() ### Update system time from RTC # rtc.hctosys() while True: rtc_now rtc.datetime() print(f\"{rtc_now[0]} {rtc_now[1]} {rtc_now[2]} {rtc_now[3]}:{rtc_now[4]}:{rtc_now[5]}\") time.sleep(1) ``` **The underlying implementation of BM8653 is similar to the singleton pattern, ensuring that read and write operations on a single BM8653 are thread safe. This means you can create BM8653 objects freely and read/write to BM8653 from any location without causing data race conditions.** The timetuple passed to the BM8653 object follows the format (year, month, day[, hour[, minute[, second]]]), meaning the first three parameters are mandatory, and any missing subsequent parameters will not modify the corresponding time. BM8653 guarantees that a returned timetuple being empty indicates an error, and if not empty, it will always contain a list of 6 elements: (year, month, day, hour, minute, second). For detailed information on the BM8653 API, please refer to the [BM8653 API Documentation](../../../api/maix/ext_dev/bm8563.html)"},"/maixpy/doc/en/modules/qmi8658.html":{"title":"MaixPy qmi8658 Driver Instructions","content":" title: MaixPy qmi8658 Driver Instructions update: date: 2024 08 27 author: iawak9lkm version: 1.0.0 content: Initial document Please refer to [AHRS Documentation](./ahrs.html)"},"/maixpy/doc/en/modules/acc.html":{"title":"Reading the Accelerometer and Attitude Calculation with MaixCAM MaixPy","content":" title: Reading the Accelerometer and Attitude Calculation with MaixCAM MaixPy Please refer to [AHRS Documentation](./ahrs.html)"},"/maixpy/doc/en/modules/tof.html":{"title":"Using TOF Modules for Distance Measurement and Terrain Detection with MaixCAM and MaixPy","content":" title: Using TOF Modules for Distance Measurement and Terrain Detection with MaixCAM and MaixPy ## Demo Single ToF effect: ![](../../assets/tof003.jpg) ToF and visible light fusion effect: ![](../../assets/tof002.jpg) This APP has been integrated into the MaixCAM Release image. Install the supported module and launch the `ToF Camera` APP to use it. [APP Installation Link](https://maixhub.com/app/88) [Source Code Link](https://github.com/sipeed/MaixCDK/tree/main/projects/app_tof_camera) ## Supported List * PMOD_TOF100 [Purchase Inquiry Link](https://wiki.sipeed.com/en/store.html) ### PMOD_TOF100 ![](../../assets/tof004.jpg) PMOD_TOF100 is a 100x100 TOF module that can be used for distance measurement or terrain detection. Module parameters: Parameter Name Value Resolution 100x100, 50x50, 25x25 Range 0.2~2.5m Field of View 70°H x 60°V Laser Emitter 940nm VCSEL Frame Rate 5~20fps After installing the PMOD_TOF100 module, you can use the API provided by MaixPy to obtain the distance data matrix, pseudo color images, and the minimum, maximum, and center distance values and their corresponding coordinates from the latest frame data. For details, see [Module API Documentation](../../../api/maix/ext_dev/tof100.html). You can also refer to our demo APP to write your application code. Sipeed offers two additional [TOF modules](https://wiki.sipeed.com/hardware/zh/maixsense/index.html) for distance measurement, which can be purchased and used with serial communication."},"/maixpy/doc/en/basic/maixvision.html":{"title":"MaixVision -- MaixCAM MaixPy Programming IDE + Graphical Block Programming","content":" title: MaixVision MaixCAM MaixPy Programming IDE + Graphical Block Programming ## Introduction [MaixVision](https://wiki.sipeed.com/maixvision) is a development tool specifically designed for the Maix ecosystem, supporting MaixPy programming and graphical block programming. It allows for online operation and debugging, real time image preview, and synchronizing images from device displays, which is convenient for debugging and development. It also supports packaging and installing applications on devices, allowing users to easily generate and install applications with one click. In addition, it integrates several handy tools for development, such as file management, threshold editor, QR code generator, and more. ## Download Visit the [MaixVision homepage](https://wiki.sipeed.com/maixvision) to download. ## Using MaixPy Programming and Online Running Follow the steps in [Quick Start](../index.html) to connect your device, and you can easily use MaixPy programming and run it online. ## Real time Image Preview MaixPy provides a `display` module that can show images on the screen. Also, when the `show` method of the `display` module is called, it sends the image to be displayed on MaixVision, for example: ```python from maix import display, camera cam camera.Camera(640, 480) disp display.Display() while 1: disp.show(cam.read()) ``` Here we use the camera to capture an image, then display it on the screen using the `disp.show()` method, and also send it to MaixVision for display. When we click the 'pause' button in the top right corner, it will stop sending images to MaixVision. ## Code Auto Completion Code suggestions depend on local Python packages installed on your computer. To enable code suggestions, you need to install Python on your computer and the required Python packages. > If not installed, red wavy underlines will appear indicating errors. However, the code can still run normally on the device — it's just that the editor won't provide code completion or IntelliSense. * To install Python, visit the [Python official website](https://python.org/). * To install the required packages, for MaixPy, for instance, you need to install the MaixPy package on your computer using `pip install MaixPy`. If `MaixPy` gets updated, you should update it on both your computer and device. On your computer, manually execute `pip install MaixPy U` in the terminal. For device updates, update directly in the `Settings` application. > Users in China can use a local mirror `pip install i https://pypi.tuna.tsinghua.edu.cn/simple MaixPy`. * Restart MaixVision to see the code suggestions. > If suggestions still do not appear, you can manually set the path to the Python executable in settings and restart. >! Note that installing Python packages on your computer is just for code suggestions. The actual code runs on the device (development board), and the device must also have the corresponding packages to run properly. > Additionally, while you have the MaixPy package installed on your computer, due to our limited resources, we cannot guarantee that you can directly use the Maix package in your computer's Python. Please run it on supported devices. In addition, in addition to the MaixPy package, other code hints, such as `numpy/opencv`, also need to be installed on the computer to implement code hints. ## Running a Single File When writing code, there are generally two modes: running a **single file**, or running a **complete project** (which may include multiple `.py` files or other resources like images/models, etc.). For simple scripts, **a single `.py` file is enough**. Just create or open a `.py` file, edit it, and click the **Run** button at the bottom left to execute it on the device. ## Creating a Project (Multi file Project / Modular Code) For slightly more complex programs—when your code grows larger, needs modularization, or requires additional resources such as images or models—you should set up a **project**. * Use your system's file manager to create an **empty folder**, then in MaixVision click **“Open Folder/Project”** to open that folder. (Or click **New Project**, if available in your version.) * Create a `main.py` file as the **main program entry point** (the name **must be** `main.py`). * If `main.py` needs to import other `.py` modules, create them in the same folder. For example, create a file named `a.py`: ```python def say_hello(): print(\"hello from module a\") ``` * Then in `main.py`, import and use it: ```python from a import say_hello say_hello() ``` * To run the whole project, click the **“Run Project”** button at the bottom left. This will automatically **package and send all files in the project folder** to the device and run the code. * If you have a folder/project open but still want to run a **single file**, open that file and click **“Run Current File”**. Only that file will be sent to the device. > ⚠️ Note: This mode **does not** send other `.py` files to the device, so **do not use imports** if you’re running only one file. ## Calculating the Image Histogram In the previous step, we could see the image in real time in MaixVision. By selecting an area with the mouse, we can view the histogram for that area at the bottom of the screen, displaying different color channels. This feature is helpful when finding suitable parameters for some image processing algorithms. ## Distinguishing Between `Device File System` and `Computer File System` This is an important concept to understand: **clearly distinguish between the `device file system` and the `computer file system`**. * **Computer File System**: This runs on your PC. When you open files or projects in MaixVision, you're accessing files from your computer (e.g., from C: or D: drive). Saving also saves to your computer's file system. * **Device File System**: When running a program, the code is sent to the device for execution, so any files accessed in the code are read from the **device's file system**, not your computer. A common mistake: Someone saves an image file on their PC, like `D:\\data\\a.jpg`, and then tries to load it in the device code using: ```python img image.load(\"D:\\data\\a.jpg\") ``` This will **fail**, because the device **does not have access** to `D:\\data\\a.jpg`. Correct Usage: * Use **MaixVision's file manager** to upload the file from your computer to the device, for example into the `/root/` directory. (See the following section for details.) * In your code, load the file from the **device file system**: ```python img image.load(\"/root/a.jpg\") ``` ## Transferring Files to the Device First, connect to the device, then click the button to browse the device file system, as shown below. Then you can upload files to the device or download files to the computer. ![maixvision_browser2](../../assets/maixvision_browser2.jpg) ![maixvision_browser](../../assets/maixvision_browser.jpg) .. details:: Alternatively, other tools can be used, click to expand First, know the device's IP address or name, which MaixVision can find, or see in the device's `Settings >System Information`, such as `maixcam xxxx.local` or `192.168.0.123`. The username and password are `root`, using the `SFTP` protocol for file transfer, and the port number is `22`. There are many useful tools available for different systems: ### Windows Use [WinSCP](https://winscp.net/eng/index.php) or [FileZilla](https://filezilla project.org/) to connect to the device and transfer files, choosing the `SFTP` protocol and entering the device and account information to connect. Specific instructions can be searched online. ### Linux In the terminal, use the `scp` command to transfer files to the device, such as: ```bash scp /path/to/your/file.py root@maixcam xxxx.local:/root ``` ### Mac * **Method 1**: In the terminal, use the `scp` command to transfer files to the device, such as: ```bash scp /path/to/your/file.py root@maixcam xxxx.local:/root ``` * **Method 2**: Use [FileZilla](https://filezilla project.org/) or other tools to connect to the device and transfer files, choosing the `SFTP` protocol and entering the device and account information to connect. ## Packaging Applications Using MaixPy + MaixVison makes it easy to develop, package, and install applications for easy offline deploy: Develop applications with MaixPy in MaixVision, which can be a single file or a project directory. Connect the device. Click the \"Install\" button at the bottom left corner of MaixVision, fill in the basic information of the application in the popup window, where the ID is used to identify the application. A device cannot simultaneously install different applications with the same ID, so the ID should be different from the IDs of applications on MaixHub. The application name can be duplicated. You can also upload an icon. Click \"Package Application\" to package the application into an installer. If you want to upload it to the [MaixHub App Store](https://maixhub./com/app), you can use this packaged file. Click \"Install Application\" to install the packaged application on the device. Disconnect from the device, and you will see your application in the device's app selection interface. Simply click on it to run the application. > If you develop with MaixCDK, you can use `maixcdk release` to package an application. Refer to the MaixCDK documentation for specifics. ## Terminal Usage MaixVision supports direct access to the device terminal. Simply click the **\"Device Terminal\"** button on the right to open it. Of course, you can also use third party shell tools. For example, use your system's built in terminal and connect via the `ssh` command. ## Using Graphical Block Programming Under development, please stay tuned."},"/maixpy/doc/en/basic/python_pkgs.html":{"title":"MaixCAM MaixPy Add extra Python packages.","content":" title: MaixCAM MaixPy Add extra Python packages. ## Introduction MaixPy is based on the Python language and provides a wide range of functionalities and APIs for embedded application development. In addition to this, you can also use other Python packages to extend its functionality. ## Installing Additional Python Packages > Note: Since MaixCAM uses a custom RISC V toolchain, **not all Python packages are supported**. Generally, only **pure Python packages** are supported — C extension packages are **not supported** unless you manually cross compile them on your PC (which is more complex and not covered here). > For **MaixCAM2**, which is **AARCH64** and comes with a built in **GCC**, you can assume that **almost all Python packages can be installed**. ### Method 1: Installing Using Python Code You can install the package you need in MaixVision using Python code, for example: ```python import os os.system(\"pip install package_name\") ``` To update a package, you can use: ```python import os os.system(\"pip install upgrade package_name\") ``` ### Method 2: Installing Using the Terminal and pip Command Follow the terminal usage method introduced in [Linux Basics](./linux_basic.html) and use `pip install package_name` to install the package you need. ## Packages That Cannot Be Installed Directly with pip > This issue mainly affects MaixCAM/MaixCAM Pro. MaixCAM2 generally does not have this problem. pip on the device can install programs written in pure Python. However, for libraries that use other languages like C++ at the lower level, there are generally no precompiled packages available due to the unique nature of MaixCAM RISC V. Solutions: * **Method 1:** Find the source code of the corresponding package, cross compile it into a `.whl` installation package on your computer, and then copy it to the device and use `pip install xxxx.whl` to install it. The toolchain used for compilation is the same as the one used by [MaixCDK](https://github.com/sipeed/MaixCDK/blob/main/platforms/maixcam.yaml). * **Method 2:** According to the [compilation system](../pro/compile_os.html), before compiling, you can execute `make menuconfig` in the `buildroot` directory to check if the Python interpreter's extra packages contain the software you need. After selecting it, you can recompile the system image to include the package. > If you successfully compile and test a package using Method 2 and find it necessary to integrate into the system, feel free to provide feedback through [issues](https://github.com/sipeed/maixpy/issues)."},"/maixpy/doc/en/basic/os.html":{"title":"MaixCAM MaixPy upgrade and burn system","content":" title: MaixCAM MaixPy upgrade and burn system layout: redirect redirect_url: ./upgrade.html "},"/maixpy/doc/en/basic/view_src_code.html":{"title":"MaixCAM MaixPy How to Find the Source Code Corresponding to MaixPy API","content":" title: MaixCAM MaixPy How to Find the Source Code Corresponding to MaixPy API ## Introduction MaixPy is implemented based on Python, with some functions written in Python and most of the underlying code written in C/C++. This ensures efficient performance. If you have questions while using a function, you can consult this document and the API documentation. If your doubts are still unresolved, you can find the underlying implementation source code using the method described in this article. **You are also welcome to contribute to the documentation or code, and become a MaixPy developer!** ## Check the Documentation First Always check the documentation first: [https://wiki.sipeed.com/maixpy/](https://wiki.sipeed.com/maixpy/), then check the API documentation: [https://wiki.sipeed.com/maixpy/api/index.html](https://wiki.sipeed.com/maixpy/api/index.html). The API documentation is only available in English because it is generated from the comments in the code, which are all in English. If you can't understand English, you can use a translation tool. ## How to Find the Source Code Corresponding to the API There are two open source repositories: [MaixPy](https://github.com/sipeed/MaixPy) and [MaixCDK](https://github.com/sipeed/MaixCDK). MaixPy is the project repository containing part of the MaixPy source code, all documents, and examples; MaixCDK contains most of the underlying C/C++ implementations of MaixPy APIs. You can download these two repositories or view them directly on the web. **Don't forget to give them a star so more people can see it!** ### Finding C/C++ Written APIs Assume we want to find the `maix.image.Image.find_blobs` function as an example. First, let's try to find it manually: * Since this is a vision related API, we look in the `components/vision/include` directory of [MaixCDK](https://github.com/sipeed/MaixCDK) and see a `maix_image.hpp` header file, where we might find it. * Searching for `find_blobs` in `maix_image.hpp`, we immediately find the function declaration: ```c++ std::vector<image::Blob> find_blobs(std::vector<std::vector<int>> thresholds std::vector<std::vector<int>>(), bool invert false, std::vector<int> roi std::vector<int>(), int x_stride 2, int y_stride 1, int area_threshold 10, int pixels_threshold 10, bool merge false, int margin 0, int x_hist_bins_max 0, int y_hist_bins_max 0); ``` * We also notice that there are comments before the function declaration, from which the API documentation is automatically generated. If you compare the API documentation with this comment, you will find them identical. Modifying this comment and recompiling will generate updated API documentation. * This is just the function declaration. We find that there is no such function in `components/vision/src/maix_image.cpp`. However, we see `components/vision/src/maix_image_find_blobs.cpp`, indicating that the function is written in a separate `cpp` file. Here, we can see the function's source code. ### Finding APIs Written with Pybind11 If you can't find it in MaixCDK, look in [MaixPy/components](https://github.com/sipeed/MaixPy/tree/main/components). > In the above code, you'll notice that the first parameter we use in `find_blobs` is of type `list`, i.e., `[[...]]`, while the C/C++ definition is `std::vector<std::vector<int>>`. This is because we use `pybind11` to automatically convert the `std::vector` type to `list` type. For some types like `numpy`'s `array`, which are inconvenient to define in MaixCDK, we use the `pybind11` definitions in [MaixPy/components](https://github.com/sipeed/MaixPy/tree/main/components). For example, the `maix.image.image2cv` method uses `pybind11` related code here. ## How to Modify the Code After finding the code, modify it directly and compile the firmware following the [build documentation](../source_code/build.html). ## How to Add Code Copy other APIs, write a function, and add complete comments. Include an extra `@maixpy maix.xxx.xxx` tag in the comments, where `xxx` is the module and API name you want to add. Then compile the firmware. Refer to [MaixCDK/components/basic/includemaix_api_example.hpp](https://github.com/sipeed/MaixCDK/blob/master/components/basic/include/maix_api_example.hpp). API parameters and return values automatically convert from basic `C++` types to Python types, making it very simple. See the [pybind11 automatic type conversion list](https://pybind11.readthedocs.io/en/stable/advanced/cast/overview.html#conversion table) for details. For example, to add `maix.my_module.my_func`, create a header file in the appropriate place in MaixCDK (preferably following the current folder classification) and add the code: ```cpp namespace maix::my_module { /** * My function, add two integers. * @param a arg a, int type * @param b arg b, int type * @return int type, will return a + b * @maixpy maix.my_module.my_func */ int my_func(int a, int b); } ``` Then add a `cpp` file: ```cpp int my_func(int a, int b) { return a + b; } ``` Compile MaixPy to generate the `whl` file and install it on the device to use the `maix.my_module.my_func` function. ## How to Contribute Code If you find any unfinished APIs or bugs in MaixPy, feel free to submit a PR (Pull Request) to the MaixPy repository. For detailed submission methods, see [Contributing Documentation and Code](../source_code/contribute.html)."},"/maixpy/doc/en/basic/python.html":{"title":"Basic Knowledge of Python","content":" title: Basic Knowledge of Python The tutorial documentation of MaixPy does not delve into specific Python syntax tutorials because there are already too many excellent Python tutorials available. Here, we only introduce what needs to be learned, provide guidance on directions and paths. ## Introduction to Python Python is an interpreted, object oriented, dynamically typed high level programming language. * Interpreted: It does not require compilation, runs directly. The advantage is rapid development, while a minor drawback is the slower execution speed due to code interpretation on each run. However, most often, the bottleneck lies in the developer's code rather than the language itself. * Object oriented: It supports object oriented programming, allowing the definition of classes and objects. Compared to procedural languages, it is easier to organize code. For more details, please search independently. * Dynamically typed: Variables do not need to declare types, can be assigned directly, and the type will be automatically determined based on the assignment. This reduces code volume, but can also lead to type errors, requiring the developer's attention. In conclusion, for developers unfamiliar with Python, it is very easy to get started as Python offers plenty of ready to use libraries, a large developer community, short application development cycles, making it highly worthwhile to learn! ## Python Environment Setup You can install Python on your computer according to the Python tutorial you are following for learning. Alternatively, you can connect to a device via MaixVision on MaixVision and then run the program on the development board. ## What Python Basics are Needed to Use MaixPy? * Basic concepts of Python. * Basic concepts of object oriented programming. * Basic syntax of Python, including: * Tab indentation alignment syntax. * Variables, functions, classes, objects, comments, etc. * Control statements such as if, for, while, etc. * Modules and importing modules. * Basic data types such as int, float, str, list, dict, tuple, etc. * Difference between bytes and str, and conversion. * Exception handling, try except. * Common built in functions like print, open, len, range, etc. * Common built in modules like os, sys, time, random, math, etc. Mastering the above foundational knowledge will enable you to smoothly program with MaixPy. With the help of subsequent tutorials and examples, if unsure, you can refer to search engines, official documentation, or ask ChatGPT to successfully complete your development tasks. ## For Developers Experienced in Another Object Oriented Programming Language If you are already proficient in an object oriented language like C++/Java/C#, you simply need to quickly review Python syntax before starting to use it. You can refer to resources like [Runoob Tutorial](https://www.runoob.com/python3/python3 tutorial.html) or the [Python Official Tutorial](https://docs.python.org/3/tutorial/index.html). Alternatively, you can explore individual developers' blogs, such as [Wow! It's Python](https://neucrack.com/p/59). ## For Developers with C Language Experience but No Object Oriented Programming Experience If you only know C and lack understanding of object oriented concepts, you can start by learning about object oriented programming concepts before diving into Python. It's relatively quick and you can search for video tutorials for entry level guidance. After following introductory video tutorials, you can then refer to documentation tutorials such as [Runoob Tutorial](https://www.runoob.com/python3/python3 tutorial.html) or the [Python Official Tutorial](https://docs.python.org/3/tutorial/index.html) to get started! Once you have acquired the basic knowledge, you can start using MaixPy for programming based on the documentation and examples. ## For Programming Beginners If you have never dealt with programming before, you will need to start learning Python from scratch. Python is also quite suitable as an introductory language. You can search for video tutorials for specific guidance. After mastering the basic syntax, you will be able to use MaixPy for programming by following examples provided. ### Using Built in Packages Python comes with many commonly used packages and APIs built in, so if you encounter any issues, you can search for “Python using xxxx” and you might find a solution right away. This applies to various common tasks, such as file handling, multi thread, multi process, networking, system operations, algorithms, and more. For example: For those who are new to Python and have only dabbled in basic microcontroller development, they might wonder why there are no examples in the documentation for reading and writing to SD/TF cards. The reason is that a file system is already running on the SD/TF card by default, so you can use Python’s file handling APIs to read and write files directly on the SD card: ```python with open(\"/root/a.txt\", \"r\") as f: content f.read() print(content) ```"},"/maixpy/doc/en/basic/linux_basic.html":{"title":"Basic Knowledge of Linux","content":" title: Basic Knowledge of Linux ## Introduction For beginners just starting out, you can skip this chapter for now and come back to it after mastering the basics of MaixPy development. The latest MaixPy supports running Linux on the MaixCAM hardware, so the underlying MaixPy development is based on the Linux system. Although Sipeed has done a lot of work for developers with MaixPy, making it possible to enjoy using it without knowledge of the Linux system, there might be situations where some low level operations are necessary or for the convenience of developers unfamiliar with Linux. In this section, we will cover some basic Linux knowledge. ## Why Linux System is Needed Specific reasons can be researched individually. Here are a few examples in simplified terms that may not sound too technical but are easy for beginners to understand: * In microcontrollers, our program is usually a loop, but with Linux, we can run multiple programs simultaneously, each appearing to run independently, where the actual execution is handled by the operating system. * With a large community of Linux based developers, required functionalities and drivers can be easily found without the need to implement them from scratch. * Linux offers a rich set of accompanying software tools for convenient development and debugging. Some Linux common tools not mentioned in this tutorial can theoretically be used as well. ## File System What is a file system? * Similar to a computer's file system, Linux manages hardware disks using a file system, making it easy for us to read and write data to the disk. * For students who have learned about microcontrollers but not familiar with file system development, imagine having a Flash or TF card where data can be read and written through APIs even after power loss. However, Flash has read/write limitations, requiring a program to ensure its longevity. A file system is like a mature program that manages the Flash space and read/write operations. By calling the file system's APIs, we can significantly reduce development work and ensure stability and security with proven programs. ## Transferring Files between Computer and Device (Development Board) Since the device has Linux and a file system, how do we send files to it? For MaixPy, we offer MaixVision for file management in future versions. Before that, you can use the following method: Here we mainly discuss transferring files through the network. Other methods can be explored on your own by searching for \"transferring files to Linux\": * Ensure the device and computer are connected to the same local network, for example: * When the MaixCAM's USB port is connected to the computer, a virtual network card is created which can be seen in the device manager on the computer, and the device's IP can be found in the device's `Settings > Device Information`. * Alternatively, connect to the same local network on the device through `Settings > WiFi`. * Use SCP or SFTP protocols on the computer to transfer files to the device. There are many specific software options and methods, such as: * On Windows, you can use WinSCP, FileZilla, or the scp command. * On Linux, use FileZilla or the scp command. * On Mac, use FileZilla or the scp command. ## Terminal and Command Line The terminal is a tool for communicating with and operating the Linux system, similar to Windows' `cmd` or `PowerShell`. For example, we can enter `ssh root@maixcam xxxx.local` in the Terminal tool on a Windows system with PowerShell or on a Linux system. You can find the specific name in the device's `Settings >Device Information`, which allows us to connect to the device through the terminal (both username and password are `root`). Then, we can operate the device by entering commands. For instance, the `ls` command can list the files in the current directory of the device, while `cd` is used to switch to a different directory (similar to clicking folders in file management on a computer), ```shell cd / # Switch to the root directory ls # Display all files in the current directory (root directory) ``` This will display similar content as below: ```shell bin lib media root tmp boot lib64 mnt run usr dev linuxrc opt sbin var etc lost+found proc sys ``` For more command learning, please search for `Linux command line usage tutorials` on your own. This is just to introduce beginners to basic concepts so that when developers mention them, they can understand what they mean."},"/maixpy/doc/en/basic/maixpy_upgrade.html":{"title":"MaixCAM upgrade MaixPy","content":" title: MaixCAM upgrade MaixPy layout: redirect redirect_url: ./upgrade.html "},"/maixpy/doc/en/basic/upgrade.html":{"title":"MaixCAM MaixPy Upgrade and System Flashing","content":" title: MaixCAM MaixPy Upgrade and System Flashing ## Introduction to the System and MaixPy First, let's distinguish between **`System`** and **`MaixPy`**: * **System**: The foundation for running all software, including the operating system and drivers, serving as the cornerstone for software operation. * **MaixPy**: A software package that relies on system drivers to function. ## Getting the Latest System Find the latest system image files on the [MaixPy Releases page](https://github.com/sipeed/MaixPy/releases), for example: * `maixcam_os_20240401_maixpy_v4.1.0.xz`: MaixCAM system image including MaixPy v4.1.0. * `maixcam pro_os_20240401_maixpy_v4.1.0.xz`: MaixCAM Pro system image including MaixPy v4.1.0. * `maixcam2_os_20250801_maixpy_v4.11.0.xz`: MaixCAM2 system image including MaixPy v4.11.0. Since the MaixCAM system image exceeds 2GB, it will only be available on Sourceforge. Users in China can download it via the shared files in the QQ group. <span style \"color: #e91e63; font weight: 800\">**Make sure to download the system image that corresponds to your device model**</span>. Downloading the wrong image may cause device damage. > Users in China with slow download speeds can use tools like Xunlei for faster downloads. > Alternatively, use proxy sites such as [github.abskoop.workers.dev](https://github.abskoop.workers.dev/) for downloads. Backup mirror: [Sourceforge](https://sourceforge.net/projects/maixpy/files/) (may not be up to date, so prefer the above official sources) ## Backup Your Data **Updating (flashing) the system will erase all data.** If you have important data saved on the device, please back it up to your computer first. Backup methods: * Connect with MaixVision and use the file manager to download important files to your computer, especially files under `/maixapp` and `/root`. * Use the `scp` command to copy files. * Use file transfer tools such as WinSCP or FileZilla. * Remove the storage media and use a card reader to copy files directly. Note: the root filesystem is formatted as `ext4`, which Windows does not support by default (you can use third party software like DiskGenius to read it). ## Flashing the System to Hardware Item MaixCAM / MaixCAM Pro MaixCAM2 Flashing Docs [MaixCAM System Flashing](https://wiki.sipeed.com/hardware/zh/maixcam/os.html) [MaixCAM2 System Flashing](https://wiki.sipeed.com/hardware/zh/maixcam/maixcam2_os.html) System Storage TF Card Built in EMMC (/TF Card) TF Card Required Yes No Flashing Method USB flashing or card reader flashing USB flashing or card reader flashing Recommended Method USB flashing USB flashing Recovery Flashing Card reader flashing USB flashing / card reader flashing ## When to Update the System vs. Updating MaixPy Only To simplify the process and avoid issues, it is **recommended to update the system whenever upgrading MaixPy**. ### You **must** update the system in any of the following scenarios: 1. Using a new TF card, which requires a TF card reader for system flashing. 2. Upgrading MaixPy, and the [MaixPy Release Page](https://github.com/sipeed/MaixPy/releases) indicates that any version between the current and target versions includes a system update. > For example, if your current system is `maixcam_os_20240401_maixpy_v4.1.0` and you wish to upgrade to `4.7.8`, you must update the system if any version between `4.1.0` and `4.7.8` includes a system update. Failure to do so may result in MaixPy not functioning properly. ### It is **strongly recommended** to update the system in these cases: 1. When using the device for the first time, as the factory installed system version may be outdated. Upgrade to the latest version to ensure compatibility with the documentation. ### Avoid updating in the following cases: 1. The current version meets your requirements and is running stably in critical scenarios (e.g., during competitions or product deployment). 2. The update introduces new features, but as a development kit, it may cause minor code incompatibilities or introduce bugs. Only update if you are prepared for development and debugging. ## Upgrading MaixPy Only After carefully considering the points above, if you decide to upgrade MaixPy only, here are three methods: 1. Use the `ssh terminal` feature in MaixVision and run: ``` pip install U MaixPy ``` For faster downloads in China, use: ``` pip install U MaixPy i https://pypi.mirrors.ustc.edu.cn/simple ``` 2. In MaixVision, execute the `examples/tools/install_maixpy.py` script to upgrade. 3. Manually download the file [MaixPy x.x.x py3 none any.whl](https://github.com/sipeed/MaixPy/releases), transfer it to the device, and install it using either of the following methods: Run `pip install xxx.whl` in the `ssh terminal`. Execute: ```python import os os.system(\"xxx.whl\") ``` **Note**: The process may take a while. Please be patient."},"/maixpy/doc/en/basic/app.html":{"title":"MaixCAM MaixPy Application Development and App Store","content":" title: MaixCAM MaixPy Application Development and App Store ## Where to Find Applications After booting, the device will automatically enter the application selection interface. Pre installed apps are published in the [MaixHub App Store](https://maixhub.com/app), where you can find descriptions and usage instructions for each app. ## Where to Find Source Code If available, source code links can be found on each application's page in the app store. All official integrated application source codes are located in the [MaixPy/projects](https://github.com/sipeed/MaixPy/tree/main/projects) or [MaixCDK/projects](https://github.com/sipeed/MaixCDK/tree/main/projects) directories. ## How to Install Applications There are several methods: ### Online QR Code Installation You can first set the language via `Settings > Language` and Wi Fi via `Settings > WiFi`. Once connected to a Wi Fi network with internet access, you can scan a QR code in the [MaixHub App Store](https://maixhub.com/app) to install apps. ### Local Installation Upload the app package to the device and install it using the command: ```shell app_store_cli install <path_to_package> ``` Alternatively, use the script [MaixPy/examples/tools/install\\_app.py](https://github.com/sipeed/MaixPy). Be sure to modify the `pkg_path` variable to your package path. ### Local QR Code Installation for Local Package * You can start a local service on your PC using: ```shell maixtool deploy pkg <path_to_package> ``` Then scan the QR code in the `App Store` app on your device to install the local package. > Ensure `maixtool` is installed on your PC via `pip install maixtool`. * If the app is developed using **MaixPy**, run `maixtool deploy` in the project root directory (containing `app.yaml` and `main.py`) to display a QR code. Keep the device and computer on the same LAN. The device can scan the QR code to install the app. * If the app is developed using **MaixCDK**, run `maixcdk deploy` in the project root directory to generate a QR code for installation in the same manner. ## How to Uninstall Applications On the device, open the `App Store` app and select `Uninstall App`. Alternatively, use the script [MaixPy/examples/tools/uninstall\\_app.py](https://github.com/sipeed/MaixPy) and set the `app_id` to the target application. To list installed apps and get their IDs, run the script [MaixPy/examples/tools/list\\_app.py](https://github.com/sipeed/MaixPy). ## App Ecosystem Overview To enable out of box experience, reduce the entry barrier for users, and provide an efficient way for developers to share and even monetize their apps, we've created a simplified application framework including: * **[App Store](https://maixhub.com/app)**: Developers can upload and share apps. Users can download and use them without coding. Developers can receive cash rewards (from MaixHub or user donations). * **Pre installed Applications**: Official apps include color tracking, AI object tracking, QR code scanning, face recognition, etc. They can be used directly or as serial communication modules. * **MaixPy + MaixCDK SDKs**: Use [MaixPy](https://github.com/sipeed/maixpy) (Python) or [MaixCDK](https://github.com/sipeed/MaixCDK) (C/C++) to quickly build embedded AI vision/audio applications. * **MaixVision PC Tool**: A user friendly development tool for writing, debugging, deploying code, and installing apps. It even supports block based programming for beginners and children. Please explore the App Store, share your apps, and contribute to the community! ## How to Package Applications * **Using MaixVision**: Refer to the [MaixVision documentation](./maixvision.html) for the app packaging section. * **Manual Packaging**: In your project root directory, manually add an `app.yaml` file. Follow the [Maix APP Specification](https://wiki.sipeed.com/maixcdk/doc/zh/convention/app.html), then run `maixtool release` (for MaixPy) or `maixcdk release` (for MaixCDK) to package your app. ## How to Exit an Application If your app doesn't have a UI or exit button, you can press the device's function key (usually labeled USER, FUNC, or OK) or a back button (if available — note that MaixCAM doesn't have a back button by default) to exit. ## Application Development Guidelines * All MaixCAM devices come with a touchscreen, so it's recommended to create a simple UI with basic touch interaction. Refer to existing examples for implementation. * Make UI elements and buttons large enough. MaixCAM has a 2.3\" screen with 552x368 resolution — keep usability in mind for fingers. * Each app should implement basic serial communication based on the [communication protocol](https://github.com/sipeed/MaixCDK/blob/master/docs/doc/convention/protocol.md) ([example](https://github.com/sipeed/MaixPy/tree/main/examples/communication/protocol)). For example, a face detection app can output coordinates via serial when a face is detected. ## Enable App Auto Start on Boot Refer to [Auto Start Application](./auto_start.html) ## System Configuration Some system settings (e.g., language, backlight) can be accessed in the Settings app. Use `maix.app.get_sys_config_kv(item, key)` to fetch these values: ```python from maix import app locale app.get_sys_config_kv(\"language\", \"locale\") print(\"locale:\", locale) backlight app.get_sys_config_kv(\"backlight\", \"value\") print(\"backlight:\", backlight, \", type:\", type(backlight)) ``` **Note**: All setting values are **strings**, so cast them appropriately when used. Configuration is stored in the `/boot/configs` file. You may edit it offline, but ensure correct formatting. The format is `maix_<item>_<key> value`, and must be shell compatible (no spaces around ` `). Example (partial) content of `/boot/configs`: ```ini # All configs user can edit easily # Format: maix_<item>_<key> value # all key characters should be lowercase # Full supported items see documentation of maixpy at: # https://wiki.sipeed.com/maixpy/doc/zh/basic/app.html ### [language] maix_language_locale en ### [wifi] # can be \"ap\" or \"sta\" or \"off\" maix_wifi_mode sta maix_wifi_ssid Sipeed_Guest maix_wifi_passwd qwert123 # encryption, auto detected by default. Can be: # \"NONE\", \"WPA PSK\", \"WPA EAP\", \"SAE\" # maix_wifi_encrypt \"WPA PSK\" ### [comm] Maix comm protocol # can be \"uart\" or \"none\" maix_comm_method uart ## [backlight] Screen backlight, from 0 to 100 maix_backlight_value 50 ### [npu] # for MaixCAM2, enable AI ISP(1) or not(0) # AI ISP improves image quality but occupies half NPU maix_npu_ai_isp 0 ```"},"/maixpy/doc/en/basic/app_usage.html":{"title":"MaixCAM MaixPy Application User Guide","content":" title: MaixCAM MaixPy Application User Guide layout: redirect redirect_url: ./app.html "},"/maixpy/doc/en/README_MaixCAM2.html":{"title":"MaixCAM2 MaixPy Quick Start","content":" title: MaixCAM2 MaixPy Quick Start <style> #head_links table { width: 100%; display: table; } @media screen and (max width: 900px){ #head_links th, #head_links td { /* padding: 8px; */ font size: 0.9em; padding: 0.1em 0.05em; } } </style> ## Getting Started >! MaixCAM2 has built in eMMC storage, so it can operate without requiring a TF card. If you need to upgrade or flash the system, please refer directly to the <a href \"./basic/os\" target \"_blank\">System Upgrade and Flashing</a> guide. ### Power On Use a `Type C` data cable to connect the `MaixCAM` device and power it on. Wait for the device to boot up and enter the function selection interface. ![maixcam_font](../../static/image/maixcam_font.png) If the screen does not display: * Please confirm that you purchased the bundled TF card. If you confirm that you have a TF card and it is inserted into the device, you can try <a href \"./basic/os\" target \"_blank\">updating to the latest system</a>. * If you did not purchase the TF card bundle, you need to follow the instructions in <a href \"./basic/os\" target \"_blank\">updating to the latest system</a> to flash the latest system onto the TF card. * Also, ensure that the screen and camera cables are not loose. The screen cable can easily come off when opening the case, so be careful. ### Connect to the Network For the first run, you need to connect to the network, as you will need it later to activate the device and use the IDE. If you don't have a router, you can use your phone to open a hotspot. Click `Settings` on the device and select `WiFi`. There are two ways to connect to the `WiFi` hotspot: * Scan the WiFi sharing code: * Use your phone to share the `WiFi` hotspot QR code, or go to [maixhub.com/wifi](https://maixhub.com/wifi) to generate a QR code. * Click the `Scan QR code` button, the camera screen will appear, scan the QR code generated previously to connect. * Search for hotspots: * Click the `Scan` button to start scanning the surrounding `WiFi`, you can click multiple times to refresh the list. * Find your WiFi hotspot. * Enter the password and click the `Connect` button to connect. Then wait for the `IP` address to be obtained, which may take `10` to `30` seconds. If the interface does not refresh, you can exit the `WiFi` function and re enter to view it, or you can also see the `IP` information in `Settings` > `Device Information`. ### Update the Runtime Libraries **This step is very important!!!** If this step is not done properly, other applications and functions may not work (e.g., they may crash). * First, ensure that you have completed the previous step of connecting to WiFi and have obtained an IP address to access the internet. * On the device, click `Settings`, and select `Install Runtime Libraries`. * After the installation is complete, you will see that it has been updated to the latest version. Then exit. If it shows `Request failed` or `请求失败` (Request failed), please first check if the network is connected. You need to be able to connect to the internet. If it still doesn't work, please take a photo and contact customer service for assistance. ### Use Built in Applications Many applications are built in, such as Find Blobs, AI Detector, Line Follower, etc. For example, Find Blobs: <video playsinline controls autoplay loop muted preload class \"pl 6 pb 4 self end\" src \"/static/video/self_learn_tracker.mp4\" type \"video/mp4\" style \"width:100%\"> Classifier Result video </video> Please explore other applications on your own. More applications will be updated in the future. For usage documentation and application updates, please see the [MaixHub App Store](https://maixhub.com/app). **Note: The applications only include a part of the functionality that MaixPy can achieve. Using MaixPy, you can create even more features.** ### Logging into the Terminal If you need to log into the terminal, the default username for `MaixCAM2` is `root`, and the password is `sipeed`. ## Use as a Serial Module > If you want to use the device as the main controller (or if you don't understand what a serial module is), you can skip this step. The built in applications can be used directly as serial modules, such as `Find Blobs`, `Find Faces`, `Find QR Codes`, etc. Note that the serial port can only directly connect to other microcontrollers. **If you want to communicate with a computer via a serial port, you must provide a USB to serial module yourself.** Usage: * Hardware connection: You can connect the device to the `Type C one to two mini board`(For MaixCAM Pro is 6Pin interface), which allows you to connect the device via serial to your main controller, such as `Arduino`, `Raspberry Pi`, `STM32`, etc. * Open the application you want to use, such as QR code recognition. When the device scans a QR code, it will send the result to your main controller via serial. > The serial baud rate is `115200`, the data format is `8N1`, and the protocol follows the [Maix Serial Communication Protocol Standard](https://github.com/sipeed/MaixCDK/blob/master/docs/doc/convention/protocol.md). You can find the corresponding application introduction on the [MaixHub APP](https://maixhub.com/app) to view the protocol. > If APP no serial output, you can also do it by yourself, follow function examples and [UART usage doc](./peripheral/uart.html) to add function and serial output. ## Preparing to Connect Computer and Device To enable communication between the computer (PC) and the device (MaixCAM), we need to ensure they are on the same local area network. There are two methods to achieve this: * **Method 1 (Highly Recommended)**: Wireless Connection. Connect the device to the same router or Wi Fi hotspot that the computer is connected to via Wi Fi. Go to the device's `Settings > WiFi Settings` and connect to your Wi Fi. (If you experience **screen lag or high latency** with Wi Fi, you can try Method 2 for a wired connection.) Here is the translation: * **Method Two**: Wired Connection. The device connects to the computer via a USB cable, and the device will emulate as a USB network adapter. This way, the device and the computer will be on the same local network through the USB connection. It is recommended to start with WiFi because although a wired connection offers stable transmission, it may encounter issues such as faulty cables, poor connection, or driver problems. If you encounter any issues, you can refer to the common problems in the [FAQ](./faq.html). .. details::Method Two: Driver Installation on Different Computer Systems: :open: true By default, there are two types of USB virtual network adapter drivers (NCM and RNDIS drivers) to meet the needs of different systems. You can also disable the unused virtual network adapter on the device under `Settings` > `USB Settings`: * **Windows**: All Windows systems will automatically install the RNDIS driver, while only Windows 11 will automatically install the NCM driver. As long as **one of the drivers works**, it is sufficient. * Open Task Manager > Performance, and you should see a virtual Ethernet with an IP address such as `10.131.167.100`, which is the computer's IP address. The device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. If you are using Windows 11, you will see two virtual network adapters; you can use either IP address. * Additionally, you can open `Device Manager` (search for `Device Manager` in the search bar). The RNDIS and NCM drivers should be correctly installed, as shown below: ![RNDIS ok](../../static/image/windows_rndis_ok.png) ![NCM ok](../../static/image/windows_ncm_ok.png) * **Linux**: No additional setup is required. Simply plug in the USB cable. Use `ifconfig` or `ip addr` to see the `usb0` and `usb1` network interfaces, and either IP address can be used. **Note**: The IP address you see, such as `10.131.167.100`, is the computer's IP address, and the device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. * **MacOS**: Check for the `usb` network adapter under `System Settings` > `Network`. **Note**: The IP address you see, such as `10.131.167.100`, is the computer's IP address, and the device's IP address is the same but with the last digit changed to `1`, i.e., `10.131.167.1`. ## Preparing the Development Environment * First, ensure that the computer and the device are on the same local network as per the previous step. * Download and install [MaixVision](https://wiki.sipeed.com/maixvision). * Connect the device and the computer using a Type C cable. Open MaixVision, click the `“Connect”` button in the lower left corner, and it will automatically search for the device. Wait for a moment until the device appears, then click the connection button next to the device to connect. If **no device is detected**, you can also manually enter the device's IP address in the **device**'s `Settings > Device Info`. You can also find solutions in the [FAQ](./faq.html). **After a successful connection, the function selection interface on the device will disappear, and the screen will turn black, releasing all hardware resources. If there is still an image displayed, you can disconnect and reconnect.** Here is a video example of using MaixVision: <video style \"width:100%\" controls muted preload src \"/static/video/maixvision.mp4\"></video> ## Run Examples Click `Example Code` on the left side of MaixVision, select an example, and click the `Run` button in the bottom left to send the code to the device for execution. For example: * `hello_maix.py`: Click the `Run` button, and you will see messages printed from the device in the MaixVision terminal, as well as an image in the upper right corner. * `camera_display.py`: This example will open the camera and display the camera view on the screen. ```python from maix import camera, display, app disp display.Display() # Construct a display object and initialize the screen cam camera.Camera(640, 480) # Construct a camera object, manually set the resolution to 640x480, and initialize the camera while not app.need_exit(): # Keep looping until the program exits (you can exit by pressing the function key on the device or clicking the stop button in MaixVision) img cam.read() # Read the camera view and save it to the variable img, you can print(img) to print the details of img disp.show(img) # Display img on the screen ``` * `yolov5.py` will detect objects in the camera view, draw bounding boxes around them, and display them on the screen. It supports detection of 80 object types. For more details, please see [YOLOv5 Object Detection](./vision/yolov5.html). You can try other examples on your own. > If you encounter image display stuttering when using the camera examples, it may be due to poor network connectivity, or the quality of the USB cable or the host's USB being too poor. You can try changing the connection method or replacing the cable, host USB port, or computer. ## Install Applications on the Device The above examples run code on the device, but the code will stop running when `MaixVision` is disconnected. If you want the code to appear in the boot menu, you can package it as an application and install it on the device. Click the `Install App` button in the bottom left corner of `MaixVision`, fill in the application information, and the application will be installed on the device. Then you will be able to see the application on the device. You can also choose to package the application and share your application to the [MaixHub App Store](https://maixhub.com/app). > The default examples do not explicitly write an exit function, so you can exit the application by pressing the function key on the device. (For MaixCAM, it is the user key.) If you want the program to start automatically on boot, you can set it in `Settings > Boot Startup`. More MaixVision usage refer to [MaixVision documentation](./basic/maixvision.html)。"},"/maixpy/doc/en/vision/image_ops.html":{"title":"MaixCAM MaixPy Basic Image Operations","content":" title: MaixCAM MaixPy Basic Image Operations update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial document ## Introduction Images play a very important role in visual applications. Whether it's a picture or a video, since a video is essentially a series of frames, image processing is the foundation of visual applications. ## API Documentation This document introduces common methods. For more APIs, refer to the documentation of the maix.image module. ## Image Formats MaixPy provides a basic image module `image`, where the most important part is the `image.Image` class, which is used for image creation and various basic image operations, as well as image loading and saving. There are many image formats, and we generally use `image.Format.FMT_RGB888` or `image.Format.FMT_RGBA8888` or `image.Format.FMT_GRAYSCALE` or `image.Format.FMT_BGR888`, etc. We all know that the three colors `RGB` can synthesize any color, so in most cases, we use `image.Format.FMT_RGB888`, which is sufficient. `RGB888` is `RGB packed` in memory, i.e., the arrangement in memory is: `pixel1_red, pixel1_green, pixel1_blue, pixel2_red, pixel2_green, pixel2_blue, ...` arranged in sequence. ## Creating an Image Creating an image is very simple, you only need to specify the width and height of the image, and the image format: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) print(img) print(img.width(), img.height(), img.format()) ``` `320` is the width of the image, `240` is the height of the image, and `image.Format.FMT_RGB888` is the format of the image. The format parameter can be omitted, and the default is `image.Format.FMT_RGB888`. Here, you can get the width, height, and format of the image using `img.width()`, `img.height()`, and `img.format()`. ## Displaying on the Screen MaixPy provides the `maix.display.Display` class, which can conveniently display images: ``` from maix import image, display disp display.Display() img image.Image(320, 240, image.Format.FMT_RGB888) disp.show(img) ``` Note that here, since there is no image data, a black image is displayed. See the following sections for how to modify the image. ## Reading Images from the File System MaixPy provides the `maix.image.load` method, which can read images from the file system: ``` from maix import image img image.load(\"/root/image.jpg\") if img is None: raise Exception(f\"load image failed\") print(img) ``` Note that here, `/root/image.jpg` has been transferred to the board in advance. You can refer to the previous tutorials for the method. It supports `jpg` and `png` image formats. ## Saving Images to the File System MaixPy's `maix.image.Image` provides the `save` method, which can save images to the file system: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) # do something with img img.save(\"/root/image.jpg\") ``` ## Drawing Rectangles `image.Image` provides the `draw_rect` method, which can draw rectangles on the image: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_rect(10, 10, 100, 100, image.Color.from_rgb(255, 0, 0)) ``` Here, the parameters are: `x`, `y`, `w`, `h`, `color`. `x` and `y` are the coordinates of the top left corner of the rectangle, `w` and `h` are the width and height of the rectangle, and `color` is the color of the rectangle, which can be created using the `image.Color.from_rgb` method. You can specify the line width of the rectangle using `thickness`, which defaults to `1`. You can also draw a solid rectangle by passing `thickness 1`: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_rect(10, 10, 100, 100, (255, 0, 0), thickness 1) ``` ## Writing Strings `image.Image` provides the `draw_string` method, which can write text on the image: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_string(10, 10, \"Hello MaixPy\", image.Color.from_rgb(255, 0, 0)) ``` Here, the parameters are: `x`, `y`, `text`, `color`. `x` and `y` are the coordinates of the top left corner of the text, `text` is the text to be written, and `color` is the color of the text, which can be created using the `image.Color.from_rgb` method. You can also enlarge the font by passing the `scale` parameter: ``` img.draw_string(10, 10, \"Hello MaixPy\", image.Color.from_rgb(255, 0, 0), scale 2) ``` Get the width and height of the font: ``` w, h img.string_size(\"Hello MaixPy\", scale 2) print(w, h) ``` **Note** that here, `scale` is the magnification factor, and the default is `1`. It should be consistent with `draw_string`. ## Chinese support and custom fonts The `image` module supports loading `ttf/otf` fonts. The default font only supports English. If you want to display Chinese or custom fonts, you can first download the font file to the device and then load the font. The system also has several built in fonts, under the `/maixapp/share/font` directory, code example: ```python from maix import image, display, app, time image.load_font(\"sourcehansans\", \"/maixapp/share/font/SourceHanSansCN Regular.otf\", size 32) print(\"fonts:\", image.fonts()) image.set_default_font(\"sourcehansans\") disp display.Display() img image.Image(disp.width(), disp.height()) img.draw_string(2, 2, \"Hello! Hello, world!\", image.Color.from_rgba(255, 0, 0)) disp.show(img) while not app.need_exit(): time.sleep(1) ``` Load the font file, then set the default font, or you can set the default font without setting the default font, and set the parameters in the writing function: ```python img.draw_string(2, 2, \"你好！Hello, world!\", image.Color.from_rgba(255, 0, 0), font \"sourcehansans\") ``` Note that the `string_size` method will also use the default font to calculate the size, and you can also use the `font` parameter to set the font to be calculated separately. ## Drawing Lines `image.Image` provides the `draw_line` method, which can draw lines on the image: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_line(10, 10, 100, 100, image.Color.from_rgb(255, 0, 0)) ``` Here, the parameters are: `x1`, `y1`, `x2`, `y2`, `color`. `x1` and `y1` are the coordinates of the starting point of the line, `x2` and `y2` are the coordinates of the end point of the line, and `color` is the color of the line, which can be created using the `image.Color.from_rgb` method. ## Drawing Circles `image.Image` provides the `draw_circle` method, which can draw circles on the image: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_circle(100, 100, 50, image.Color.from_rgb(255, 0, 0)) ``` Here, the parameters are: `x`, `y`, `r`, `color`. `x` and `y` are the coordinates of the center of the circle, `r` is the radius, and `color` is the color of the circle, which can be created using the `image.Color.from_rgb` method. ## Resizing Images `image.Image` provides the `resize` method, which can resize images: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.resize(160, 120) print(img, img_new) ``` Note that here, the `resize` method returns a new image object, and the original image remains unchanged. ## Cropping Images `image.Image` provides the `crop` method, which can crop images: ``` from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.crop(10, 10, 100, 100) print(img, img_new) ``` Note that here, the `crop` method returns a new image object, and the original image remains unchanged. ## Rotating Images `image.Image` provides the `rotate` method, which can rotate images: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.rotate(90) print(img, img_new) ``` Note that here, the `rotate` method returns a new image object, and the original image remains unchanged. ## Copying Images `image.Image` provides the `copy` method, which can copy an independent image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.copy() print(img, img_new) ``` ## Affine Transformations `image.Image` provides the `affine` method, which can perform affine transformations. By providing the coordinates of three or more points in the current image and the corresponding coordinates in the target image, you can automatically perform operations such as rotation, scaling, and translation on the image to transform it into the target image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.affine([(10, 10), (100, 10), (10, 100)], [(10, 10), (100, 20), (20, 100)]) print(img, img_new) ``` For more parameters and usage, please refer to the API documentation. ## Drawing Keypoints `image.Image` provides the `draw_keypoints` method, which can draw keypoints on the image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) keypoints [10, 10, 100, 10, 10, 100] img.draw_keypoints(keypoints, image.Color.from_rgb(255, 0, 0), size 10, thickness 1, fill False) ``` This draws three red keypoints at the coordinates `(10, 10)`, `(100, 10)`, and `(10, 100)`. The size of the keypoints is `10`, the line width is `1`, and they are not filled. ## Drawing Crosses `image.Image` provides the `draw_cross` method, which can draw crosses on the image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_cross(100, 100, image.Color.from_rgb(255, 0, 0), size 5, thickness 1) ``` This draws a red cross at the coordinate `(100, 100)`. The extension size of the cross is `5`, so the length of the line segment is `2 * size + thickness`, and the line width is `1`. ## Drawing Arrows `image.Image` provides the `draw_arrow` method, which can draw arrows on the image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_arrow(10, 10, 100, 100, image.Color.from_rgb(255, 0, 0), thickness 1) ``` This draws a red arrow starting from the coordinate `(10, 10)`, with the end point at `(100, 100)`, and a line width of `1`. ## Drawing Images `image.Image` provides the `draw_image` method, which can draw images on the image: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img2 image.Image(100, 100, image.Format.FMT_RGB888) img2.draw_rect(10, 10, 90, 90, image.Color.from_rgb(255, 0, 0)) img.draw_image(10, 10, img2) ``` ## Converting Formats `image.Image` provides the `to_format` method, which can convert image formats: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) img_new img.to_format(image.Format.FMT_BGR888) print(img, img_new) img_jpg img.to_format(image.Format.FMT_JPEG) print(img, img_new) ``` Note that here, the `to_format` method returns a new image object, and the original image remains unchanged. ## Converting between Numpy/OpenCV and maix.image.Image Formats Refer to [MaixPy use OpenCV documentation](./opencv.html) ## Converting between bytes Data `image.Image` provides the `to_bytes` method, which can convert an image to `bytes` data: ```python from maix import image img image.Image(320, 240, image.Format.FMT_RGB888) data img.to_bytes() print(type(data), len(data), img.data_size()) img_jpeg image.from_bytes(320, 240, image.Format.FMT_RGB888, data) print(img_jpeg) img img_jpeg.to_format(image.Format.FMT_RGB888) print(img) ``` Here, `to_bytes` returns a new `bytes` object, which is independent memory and does not affect the original image. The `image.Image` constructor can directly construct an image object from `bytes` data by passing the `data` parameter. Note that the new image is also independent memory and does not affect `data`. Since memory copying is involved, this method is relatively time consuming and should not be used frequently. > If you want to optimize your program without copying (not recommended for casual use, as poorly written code can easily cause crashes), please refer to the API documentation. ## More Basic API Usage For more API usage, please refer to the documentation of the maix.image module."},"/maixpy/doc/en/vision/self_learn_detector.html":{"title":"MaixCAM MaixPy Self-learning Tracker","content":" title: MaixCAM MaixPy Self learning Tracker update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial version of the document update: date: 2025 09 03 author: neucrack version: 1.1.0 content: Added MixFormerV2 support ## MaixPy Self learning Detection Tracker Similar to the self learning classifier, no training is required; simply select the target object with a bounding box to achieve detection and tracking. It is very useful in simple detection scenarios. Unlike the self learning classifier, since this is a detector, it provides the coordinates and size of the object. There are also other names, such as SOT (Single Object Tracking) and MOT (Multiple Object Tracking). We classify all of them under the self learning detection tracker. Currently, MaixPy supports two SOT algorithms, but of course, you can also port or create more algorithms yourself. Hopefully, this can serve as inspiration. <video playsinline controls autoplay loop muted preload src \"/static/video/self_learn_tracker.mp4\" style \"width: 100%; min height: 20em;\"></video> ## Using the Self learning Detection Tracker in MaixPy Currently, MaixPy provides a single object learning detection tracking algorithm, meaning you start by selecting the target object, and it will continuously track that object. The algorithms used here are [NanoTrack](https://github.com/HonglinChu/SiamTrackers/tree/master/NanoTrack) and [MixFormerV2](https://github.com/MCG NJU/MixFormerV2). Those interested in the principles can study them further. You can flash the latest system image and directly use the built in self learning tracking application to see the results. Different devices support different models, and different algorithms have their own characteristics, as shown in the table below: Model NanoTrack MixFormerV2 Features Lightweight, fast runtime Slightly slower than NanoTrack, supports online update, better recognition performance Sub category SOT SOT MaixCAM / MaixCAM Pro ✅<br> ❌ MaixCAM2 ❌ ✅<br>Model runs at 38fps<br>Full pipeline 640x480 at 33fps ### NanoTrack Use the `maix.nn.NanoTrack` class. After initializing the object, first call the `init` method to specify the target, then call the `track` method to continuously track the target. Simplified code is as follows: ```python from maix import nn model_path \"/root/models/nanotrack.mud\" tracker nn.NanoTrack(model_path) tracker.init(img, x, y, w, h) pos tracker.track(img, threshold 0.9) ``` Here, the target features are learned from a specified `x,y,w,h` region in one image, and then the `track` function is called on a new image to get the target’s position. For detailed code, see [MaixPy/examples/vision/ai\\_vision/nn\\_self\\_learn\\_tracker.py](https://github.com/sipeed/MaixPy/blob/main/examples/vision/ai_vision/nn_self_learn_tracker.py) > Note: This uses a built in model already included under `/root/models` in the system. You can also download models from [MaixHub Model Zoo](https://maixhub.com/model/zoo/437). ### MixFormerV2 Use the `maix.nn.MixFormerV2` class. After initializing the object, first call the `init` method to specify the target, then call the `track` method to continuously track the target. Simplified code is as follows: ```python from maix import nn model_path \"/root/models/nanotrack.mud\" tracker nn.MixFormerV2(model_path, update_interval 200, int lost_find_interval 60) tracker.init(img, x, y, w, h) pos tracker.track(img, threshold 0.5) ``` Similar to `NanoTrack`, the target features are learned from a specified `x,y,w,h` region in one image, and then the `track` function is called on a new image to get the target’s position. The difference here is that two additional parameters are provided: * **update\\_interval**: Updates the target every `update_interval` frames while keeping the initial target. Both the updated and initial targets are input together to produce the result. This ensures adaptation if the target changes, such as angle variation. * **lost\\_find\\_interval**: A simple built in lost target recovery algorithm. If the target is not found for more than `lost_find_interval` frames, it automatically expands the search area to try to find it. For detailed code, see [MaixPy/examples/vision/ai\\_vision/nn\\_self\\_learn\\_tracker.py](https://github.com/sipeed/MaixPy/blob/main/examples/vision/ai_vision/nn_self_learn_tracker.py) ## Lost Target Recovery For both `NanoTrack` and `MixFormerV2`, recognition is based on a search region. For example, in a 1920x1080 image, if the initial target is only 100x50 pixels, the detection is not done across the whole 1920x1080 image but rather within an expanded region cropped around the target. In the video above, you can see the search region box drawn around the target. The problem is that if the target is occluded or disappears for some time and then reappears outside the search region, it cannot be detected. The target is still in the frame, but the algorithm cannot find it. In this case, a recovery algorithm is needed. The official `NanoTrack` and `MixFormerV2` do not provide recovery, but MaixPy’s implementation of `MixFormerV2` includes a simple recovery algorithm. By setting the `lost_find_interval` parameter, it will automatically attempt a global search after losing the target. ## Other Self learning Tracking Algorithms and Optimizations This article serves as inspiration by providing a few algorithms. If you have better algorithms and optimizations, you can refer to the existing NanoTrack / MixFormerV2 implementations to create your own. Contributions and discussions are also welcome."},"/maixpy/doc/en/vision/ai.html":{"title":"MaixCAM MaixPy Basic Knowledge of AI Vision","content":" title: MaixCAM MaixPy Basic Knowledge of AI Vision update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial documentation ## Introduction If you don't have an AI background, you can first read [What is Artificial Intelligence (AI) and Machine Learning](https://wiki.sipeed.com/ai/en/basic/what_is_ai.html) to understand the basic concepts of AI before learning about AI. Then, the visual AI we use is generally based on the `deep neural network learning` method. If you are interested, you can check out [Deep Neural Network (DNN) Basics](https://wiki.sipeed.com/ai/en/basic/dnn_basic.html). ## Using Visual AI in MaixPy Using visual AI in MaixPy is very simple. By default, commonly used AI models are provided, and you can use them directly without having to train the models yourself. You can find the `maixcam` models in the [MaixHub Model Library](https://maixhub.com/model/zoo). Additionally, the underlying APIs have been well encapsulated, and you only need to make simple calls to implement them. If you want to train your own model, you can start with [MaixHub Online Training](https://maixhub.com/model/training/project). On the online platform, you can train models just by clicking, without the need to purchase expensive machines, set up complex development environments, or write code, making it very suitable for beginners and also for experienced users who are too lazy to read code. Generally, once you have obtained the model file, you can transfer it to the device and call the MaixPy API to use it. The specific calling methods are discussed in the following sections."},"/maixpy/doc/en/vision/hand_gesture_classification.html":{"title":"MaixCAM MaixPy Hand Gesture Classification Based on Hand Keypoint Detection","content":" title: MaixCAM MaixPy Hand Gesture Classification Based on Hand Keypoint Detection ## Introduction The `MaixCAM MaixPy Hand Gesture Classification Based on Hand Keypoint Detection` can classify various hand gestures. The following describes how to preprocess features from [AI model estimated hand landmarks](./hand_landmarks.html), which are then classified using LinearSVC (Support Vector Machine). A detailed implementation is available in `MaixPy/projects/app_hand_gesture_classifier/LinearSVC.py`, and the usage example can be found in the app implementation in `MaixPy/projects/app_hand_gesture_classifier/main.py`. **Users can add any distinguishable hand gestures for training.** ## Usage ### Preprocessing Here’s the preprocessing for the raw output `hand_landmarks` from the AI model to derive usable features: ```python def preprocess(hand_landmarks, is_left False, boundary (1,1,1)): hand_landmarks np.array(hand_landmarks).reshape((21, 1)) vector hand_landmarks[:,:2] vector vector[1:] vector[0] vector vector.astype('float64') / boundary[:vector.shape[1]] if not is_left: # mirror vector[:,0] * 1 return vector ``` ### Import Modules Alternatively, you can directly copy the `LinearSVC.py` implementation from the directory `target_dir` ```python # To import LinearSVC target_dir '/maixapp/apps/hand_gesture_classifier/' import sys if target_dir not in sys.path: sys.path.insert(0, target_dir) from LinearSVC import LinearSVC, LinearSVCManager ``` ### Classifier (LinearSVC) Introduction to the LinearSVC classifier's functions and usage. #### Initialization, Loading, and Exporting ```python # Initialize clf LinearSVC(C 1.0, learning_rate 0.01, max_iter 500) # Load clf LinearSVC.load(\"/maixapp/apps/hand_gesture_classifier/clf_dump.npz\") # Export clf.save(\"my_clf_dump.npz\") ``` *Initialization Method Parameters* 1. C 1.0 (Regularization Parameter) Controls the strength of the regularization in the SVM. A larger C value punishes misclassifications more strictly, potentially leading to overfitting. A smaller C value allows some misclassifications, improving generalization but potentially underfitting Default: 1.0, balanced regularization between accuracy and generalization. 2. learning_rate 0.01 (Learning Rate) Controls the step size for weight updates during each gradient descent optimization. Too large a learning rate may cause the optimization process to diverge While too small a learning rate may lead to slow convergence. Default: 0.01, typically a moderate value to ensure gradual approach to the optimal solution. 3. max_iter 500 (Maximum Iterations) Specifies the maximum number of optimization rounds during training. More iterations allow the model more chances to converge, but too many may result in overfitting. A smaller max_iter value may stop optimization prematurely, leading to underfitting. Default: 1000, sufficient iterations to ensure convergence. *Loading and Exporting Method Parameters* 1. filename: str The target file path, both relative and absolute paths are supported. This is a required parameter. Default: None *Training and Prediction (Classification)* After initializing the classifier, it needs to be trained before it can be used for classification. If you load a previously trained classifier, it can directly be used for classification. **Note: Every training session is a full training process, meaning previous training results will be lost. It's recommended to export the classifier backup as needed.** ```python npzfile np.load(\"/maixapp/apps/hand_gesture_classifier/trainSets.npz\") # Preload features and labels (name_classes index) X_train npzfile[\"X\"] # Raw features y_train npzfile[\"y\"] # Labels clf.fit(clf.scaler.fit_transform(X_train), y_train) # Train SVM after feature normalization # Regression y_pred clf.predict(clf.scaler.transform(X_train)) # Predict labels after feature normalization recall_count len(y_train) right_count np.sum(y_pred y_train) print(f\"right/recall {right_count}/{recall_count}, acc: {right_count/recall_count}\") # Prediction X_test X_train[:5] feature_test clf.scaler.transform(X_test) # Feature normalization # y_pred clf.predict(feature_test) # Predict labels y_pred, y_conf clf.predict_with_confidence(feature_test) # Predict labels with confidence print(f\"pred: {y_pred}, conf: {y_conf}\") # Corresponding class names name_classes (\"one\", \"five\", \"fist\", \"ok\", \"heartSingle\", \"yearh\", \"three\", \"four\", \"six\", \"Iloveyou\", \"gun\", \"thumbUp\", \"nine\", \"pink\") ``` Since every training is full, you need to manually maintain the storage of previously trained features and corresponding labels to allow dynamic addition/removal of classes. To simplify usage and reduce extra workload, the `Classifier Manager (LinearSVCManager)` has been encapsulated, as described in the next section. ### Classifier Manager (LinearSVCManager) Introduction to the LinearSVCManager's functions and usage. #### Initialization, Loading, and Exporting Both Initialization or Loading must provide valid X and Y (corresponding features and labels) inputs. And their lengths must be equal and correspond to each other, or an error will occur. ```python # Initialization, Loading def __init__(self, clf: LinearSVC LinearSVC(), X None, Y None, pretrained False) # Initialize with default LinearSVC parameters clfm LinearSVCManager(X X_train, Y y_train) # Initialize with specific LinearSVC parameters clfm LinearSVCManager(LinearSVC(C 1.0, learning_rate 0.01, max_iter 100), X_train, y_train) # Loading requires the loaded LinearSVC and setting pretrained True to avoid unnecessary retraining # Ensure X_train, y_train are the data previously used to train LinearSVC clfm LinearSVCManager(LinearSVC.load(\"/maixapp/apps/hand_gesture_classifier/clf_dump.npz\"), X_train, y_train, pretrained True) # Export parameters using LinearSVC (clfm.clf)'s save clfm.clf.save(\"my_clf_dump.npz\") # Export features and labels used for training np.savez(\"trainSets.npz\", X X_train, y y_train, ) ``` #### Accessing Training Data Used clfm.samples is a Python tuple: 1. clfm.samples[0] is `X` 2. clfm.samples[1] is `Y` **Do not modify directly, for read only access only. To make changes, call `clfm.train()` to retrain the model.** #### Adding or Removing **When adding, ensure X_new and y_new have the same length and match the shape of the previous X_train and y_train.** All are numpy arrays, and you can check their shape with the shape attribute. ```python # Add new data clfm.add(X_new, y_new) # Remove data mask_ge_4 clfm.samples[1] > 4 # Mask for class labels > 4 indices_ge_4 np.where(mask_ge_4)[0] clfm.rm(indices_ge_4) ``` These operations mainly modify clfm.samples, but will trigger a call to clfm.train() to retrain the model. Depending on the size of the training data, wait a few moments before directly applying the updated model. #### Prediction ```python y_pred, y_conf clfm.test(X_test) # Predict labels ``` This is equivalent to: ```python clf clfm.clf feature_test clf.scaler.transform(X_test) # Feature normalization y_pred, y_conf clf.predict_with_confidence(feature_test) # Predict labels with confidence ``` #### Example (Simplified Version of the Resulting Video) Note: Missing preprocess implementation should be copied from the Preprocessing section. Missing LinearSVC module should be copied from the Import Modules section. The classification and prediction part can be run as a single file: ```python from maix import camera, display, image, nn, app import numpy as np # Add below me name_classes (\"one\", \"five\", \"fist\", \"ok\", \"heartSingle\", \"yearh\", \"three\", \"four\", \"six\", \"Iloveyou\", \"gun\", \"thumbUp\", \"nine\", \"pink\") # Easy to understand class names npzfile np.load(\"/maixapp/apps/hand_gesture_classifier/trainSets.npz\") # Preload features and labels (name_classes index) X_train npzfile[\"X\"] y_train npzfile[\"y\"] clfm LinearSVCManager(LinearSVC.load(\"/maixapp/apps/hand_gesture_classifier/clf_dump.npz\"), X_train, y_train, pretrained True) # Initialize LinearSVCManager with preloaded classifier detector nn.HandLandmarks(model \"/root/models/hand_landmarks.mud\") cam camera.Camera(320, 224, detector.input_format()) disp display.Display() # Loading screen img cam.read() img.draw_string(100, 112, \"Loading...\\nwait up to 10s\", color image.COLOR_GREEN) disp.show(img) while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.7, iou_th 0.45, conf_th2 0.8) for obj in objs: hand_landmarks preprocess(obj.points[8:8+21*3], obj.class_id 0, (img.width(), img.height(), 1)) # Preprocessing features np.array([hand_landmarks.flatten()]) class_idx, pred_conf clfm.test(features) # Get predicted class class_idx, pred_conf class_idx[0], pred_conf[0] # Handle multiple inputs and outputs, take the first element msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}\\n{name_classes[class_idx]}({class_idx}) {pred_conf*100:.2f}%' img.draw_string(obj.points[0], obj.points[1], msg, color image.COLOR_RED if obj.class_id 0 else image.COLOR_GREEN, scale 1.4, thickness 2) detector.draw_hand(img, obj.class_id, obj.points, 4, 10, box True) disp.show(img) ``` The current `X_train` is based on the \"14 Class Static Hand Gesture Dataset,\" which consists of 2850 samples, divided into 14 classes. The dataset can be downloaded from the provided [Baidu Netdisk link (Password: 6urr)](https://pan.baidu.com/s/1Sd Ad88Wzp0qjGH6Ngah0g). ![](../../assets/handposex_14class.jpg) ## Demo Video The implementation of this app is located at `MaixPy/projects/app_hand_gesture_classifier/main.py`, with the main logic as follows: 1. Load the `14 class static hand gesture dataset`, which consists of `20` coordinate offsets relative to the wrist after processing by `hand keypoint detection`. 2. Initially train `4` gesture classifications **or directly load pre trained `14` classifier parameters (switchable in the source code)** to support gesture recognition. 3. Load the `hand keypoint detection` model to process the camera input and visualize the results on the screen using the classifier. 4. Click the upper right corner `class14` to add the remaining classification samples and retrain to achieve `14` gesture classifications. 5. Click the lower right corner `class4` to remove the additional classification samples from the previous step and retrain to revert to `4` gesture classifications. 6. Click the small area between the buttons to display the duration of the last classifier training at the top. 7. Click other large areas to display the currently supported classification categories on the left side—green indicates supported, yellow indicates not supported. <video playsinline controls autoplay loop muted preload src \"/static/video/hand_gesture_demo.mp4\" type \"video/mp4\"> Classifier Result video </video> 1. The demo video shows the execution of step `4` **or the bold part of step `2`**, demonstrating the `14 class` mode. It can recognize gestures `1 10` (with default corresponding English meanings), \"OK\", thumbs up, heart shape (requires the back of the hand, difficult to demonstrate in the video but can be verified), and pinky stretch, making a total of `14` gestures. 2. Then, step `5` is executed to revert to the `4 class` mode, where only gestures `1`, `5`, `10` (fist), and \"OK\" can be recognized, while the remaining gestures do not produce correct results. Step `7` is also executed to show the current `4 class` mode, as only the first 4 gestures are displayed in green, and the remaining 10 are shown in yellow. 3. Step `4` is executed again to restore the `14 class` mode, and the gestures that could not be recognized in the `4 class` mode are now correctly identified. 4. Finally, the recognition of gestures with both hands is demonstrated, showing that the system can correctly identify gestures from both hands simultaneously. ## Others **The demo video is captured from the preview window in the upper right corner of MaixVision, and it is consistent with the actual screen display.** **For more detailed usage instructions or secondary development, please refer to the source code analysis mentioned above, which includes comments.** If you still have questions or need assistance, you can post on `maixhub` or send an `e mail` to the company at `support@sipeed.com`. **Please use the subject `[help][MaixPy] guesture classification: xxx`**."},"/maixpy/doc/en/vision/classify.html":{"title":"Using AI Models for Object Classification in MaixCAM MaixPy","content":" title: Using AI Models for Object Classification in MaixCAM MaixPy ## Object Classification Concept For example, if there are two images in front of you, one with an apple and the other with an airplane, the task of object classification is to input these two images into an AI model one by one. The model will then output two results, one for apple and one for airplane. ## Using Object Classification in MaixPy MaixPy provides a pre trained `1000` classification model based on the `imagenet` dataset, which can be used directly: ```python from maix import camera, display, image, nn classifier nn.Classifier(model \"/root/models/mobilenetv2.mud\", dual_buff True) cam camera.Camera(classifier.input_width(), classifier.input_height(), classifier.input_format()) disp display.Display() while 1: img cam.read() res classifier.classify(img) max_idx, max_prob res[0] msg f\"{max_prob:5.2f}: {classifier.labels[max_idx]}\" img.draw_string(10, 10, msg, image.COLOR_RED) disp.show(img) ``` Result video: <video playsinline controls autoplay loop muted preload src \"/static/video/classifier.mp4\" type \"video/mp4\"> Classifier Result video </video> Here, the camera captures an image, which is then passed to the `classifier` for recognition. The result is displayed on the screen. For more API usage, refer to the documentation for the [maix.nn](/api/maix/nn.html) module. ## dual_buff Dual Buffer Acceleration You may have noticed that the model initialization uses `dual_buff` (which defaults to `True`). Enabling the `dual_buff` parameter can improve running efficiency and increase the frame rate. For detailed principles and usage notes, see [dual_buff Introduction](./dual_buff.html). ## Training Your Own Classification Model on MaixHub If you want to train a classification model for specific images, visit [MaixHub](https://maixhub.com) to learn and train the model. When creating a project, select \"Classification Model\", then simply upload your images to train. There's no need to set up a training environment or spend money on expensive GPUs—training can be done quickly with one click. ## Offline Training for Your Own Classification Model For offline training, you need to set up your environment. Search for keywords such as `PyTorch classification model training` or `Mobilenet` for guidance. After training the model, export it in ONNX format, then refer to the [MaixCAM Model Conversion Documentation](../ai_model_converter/maixcam.html) to convert it into a model format supported by MaixCAM. Finally, use the `nn.Classifier` class mentioned above to load the model. The classification model can be Mobilenet or another model like ResNet. During model conversion, it's best to extract the layer just before `softmax` as the final output layer because the `classifier.classify(img, softmax True)` function has `softmax` enabled by default—this means the function will perform a `softmax` calculation on the results. Therefore, the model itself doesn't need a `softmax` layer. However, if the model does include a `softmax` layer, you can specify not to execute it again by using: `classifier.classify(img, softmax False)`."},"/maixpy/doc/en/vision/find_blobs.html":{"title":"MaixCAM MaixPy Find Blobs","content":" title: MaixCAM MaixPy Find Blobs update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial documentation date: 2024 04 03 author: lxowalle version: 1.0.1 content: Added detailed usage for finding blobs Before reading this article, make sure you know how to develop with MaixCAM. For details, please read [Quick Start](../index.html). ## Introduction This article will introduce how to use MaixPy to find color blobs and how to use the default application of MaixCam to find color blobs. In vision applications, finding color blobs is a very common requirement, such as robots finding color blobs, automated production lines finding color blobs, etc., which requires identifying specific color areas in the image and obtaining information such as the position and size of these areas. ## Using MaixPy to Find Blobs The `maix.image.Image` module in MaixPy provides the `find_blobs` method, which can conveniently find color blobs. ### How to Find Blobs A simple example to find color blobs and draw bounding boxes: ```python from maix import image, camera, display cam camera.Camera(320, 240) disp display.Display() # Select the corresponding configuration based on the color of the blob thresholds [[0, 80, 40, 80, 10, 80]] # red # thresholds [[0, 80, 120, 10, 0, 30]] # green # thresholds [[0, 80, 30, 100, 120, 60]] # blue while 1: img cam.read() blobs img.find_blobs(thresholds, pixels_threshold 500) for blob in blobs: img.draw_rect(blob[0], blob[1], blob[2], blob[3], image.COLOR_GREEN) disp.show(img) ``` Steps: 1. Import the image, camera, and display modules ```python from maix import image, camera, display ``` 2. Initialize the camera and display ```python cam camera.Camera(320, 240)\t# Initialize the camera with an output resolution of 320x240 in RGB format disp display.Display() ``` 3. Get the image from the camera and display it ```python while 1: img cam.read() disp.show(img) ``` 4. Call the `find_blobs` method to find color blobs in the camera image and draw them on the screen ```python blobs img.find_blobs(thresholds, pixels_threshold 500) for blob in blobs: img.draw_rect(blob[0], blob[1], blob[2], blob[3], image.COLOR_GREEN) ``` `img` is the camera image obtained through `cam.read()`. When initialized with `cam camera.Camera(320, 240)`, the `img` object is an RGB image with a resolution of 320x240. `img.find_blobs` is used to find color blobs. `thresholds` is a list of color thresholds, where each element is a color threshold. Multiple thresholds can be passed in to find multiple colors simultaneously. Each color threshold is in the format `[L_MIN, L_MAX, A_MIN, A_MAX, B_MIN, B_MAX]`, where `L`, `A`, and `B` are the three channels in the LAB color space. The `L` channel represents brightness, the `A` channel represents the red green component, and the `B` channel represents the blue yellow component. `pixels_threshold` is a pixel count threshold used to filter out unwanted small blobs. `img.draw_rect` is used to draw bounding boxes around the color blobs. `blob[0]`, `blob[1]`, `blob[1]`, and `blob[1]` represent the x coordinate of the top left corner of the blob, the y coordinate of the top left corner of the blob, the width of the blob, and the height of the blob, respectively. ### Common Parameter Explanations Here are explanations of commonly used parameters. If you cannot find parameters that can implement your application, you may need to consider using other algorithms or extending the required functionality based on the current algorithm's results. Parameter Description Example thresholds Thresholds based on the LAB color space, thresholds [[l_min, l_max, a_min, a_max, b_min, b_max]], representing:<br/>Brightness range [l_min, l_max]<br/>Green to red component range [a_min, a_max]<br/>Blue to yellow component range [b_min, b_max]<br/>Multiple thresholds can be set simultaneously Set two thresholds to detect red and green<br/>```img.find_blobs(thresholds [[0, 80, 40, 80, 10, 80], [0, 80, 120, 10, 0, 30]])```<br/>Red threshold is [0, 80, 40, 80, 10, 80]<br/>Green threshold is [0, 80, 120, 10, 0, 30] invert Enable threshold inversion, when enabled, the passed thresholds are inverted. Default is False. Enable threshold inversion<br/>```img.find_blobs(invert True)``` roi Set the rectangular region for the algorithm to compute, roi [x, y, w, h], where x and y represent the coordinates of the top left corner of the rectangle, and w and h represent the width and height of the rectangle, respectively. The default is the entire image. Compute the region at (50, 50) with a width and height of 100<br/>```img.find_blobs(roi [50, 50, 100, 100])``` area_threshold Filter out blobs with a pixel area smaller than area_threshold, in units of pixels. The default is 10. This parameter can be used to filter out some useless small blobs. Filter out blobs with an area smaller than 1000<br/>```img.find_blobs(area_threshold 1000)``` pixels_threshold Filter out blobs with fewer valid pixels than pixels_threshold. The default is 10. This parameter can be used to filter out some useless small blobs. Filter out blobs with fewer than 1000 valid pixels<br/>```img.find_blobs(pixels_threshold 1000)``` This article introduces commonly used methods. For more APIs, please see the [image](../../../api/maix/image.html) section of the API documentation. ## Setting Thresholds Offline To quickly verify the function of find blobs, you can first use the find blobs application provided by MaixCam to experience the effect of finding color blobs. ### Demo Turn on the device, select `Find Blobs` application, then select the colour you want to identify, or customize the colour, then you can identify the corresponding colour, the `setting bar` at the bottom will show the `threshold range`, and the serial port will also output the coordinates and colour information of the identified coordinates. <video src \"/static/video/find_blobs.mp4\" controls \"controls\" width \"100%\" height \"auto\"></video> [source code address](https://github.com/sipeed/MaixCDK/tree/main/projects/app_find_blobs) ### Quick use #### Using the default threshold The find blobs app provides four configurations, `red`, `green`, `blue` and `user`, where `red`, `green` and `blue` are used to find `red`, `green` and `blue` colour blocks, and `user` customized thresholds are saved when the app is exited, and the next time the app is opened the thresholds from the last debugging are loaded. For quick experience, you can switch to the corresponding configuration by `clicking` the `button` at the bottom of the interface, the app interface is referenced below: ![](../../../static/image/find_blobs_app.jpg) #### Quick Debug Thresholds Method of operation: 1. Aim the `camera` at the `object` you need to `find`, `click` on the `target` on the screen, then the `left` side will show the `rectangle` of the corresponding colour of the object, and the LAB value of the object's colour. 2. Click on the rectangular box, the system will `automatically set' the LAB threshold, then the screen will draw the edge of the object. The advantage of this method is that it is easy and quick to set the threshold and find the corresponding colour block. The disadvantage is that it is not precise enough, you can fine tuning it manually in the next step. #### Manually fine tune the threshold Method of operation: 1. `Click` on the `Options icon` in the lower left corner to enter configuration mode 2. Aim the `camera` at the `object` you need to `find`, `click` on the `target object` on the screen, at this time the `left` side will show the `rectangular box` of the corresponding colour of the object, and display the `LAB value` of the object's colour. 3. Click on the lower option `L Min, L Max, A Min, A Max, B Min, B Max`, and a slider will appear on the right to set the value of this option. These values correspond to the minimum and maximum values of the L, A and B channels of the LAB colour format. 4. Referring to the `LAB value` of the object colour calculated in step 2, adjust `L Min, L Max, A Min, A Max, B Min, B Max` to the appropriate value to identify the corresponding colour block. For example, `LAB (20, 50, 80)`, since `L 20`, in order to fit a certain range, let `L Min 10`, `L Max 30`; similarly, since `A 50`, let `A Min 40`, `A Max 60`; since `B 80`, let `B Min 70`, `B Max 90`. This method can be more precise to find the right threshold, with the `Quick Debug Threshold` method, it is easy to find the desired threshold. #### Get recognition results via serial protocol The find blobs app supports reporting information about detected color blobs via the serial port (default baud rate is 115200). Since only one report message is sent, we can illustrate the content of the report message with an example. For instance, if the report message is: ``` shellCopy code AA CA AC BB 14 00 00 00 E1 08 EE 00 37 00 15 01 F7 FF 4E 01 19 00 27 01 5A 00 A7 20 ``` `AA CA AC BB`: Protocol header, content is fixed `14 00 00 00`: Data length, the total length excluding the protocol header and data length `E1`: Flag, used to identify the serial message flag `08`: Command type, for the find blobs app application, this value is fixed at 0x08 `EE 00 37 00 15 01 F7 FF 4E 01 19 00 27 01 5A 00`: Coordinates of the four vertices of the found color blob, with each value represented by 2 bytes in little endian format. `EE 00` and `37 00` represent the first vertex coordinate as (238, 55), `15 01` and `F7 FF` represent the second vertex coordinate as (277, 9), `4E 01` and `19 00` represent the third vertex coordinate as (334, 25), `27 01` and `5A 00` represent the fourth vertex coordinate as (295, 90). `A7 20`: CRC checksum value, used to verify if the frame data has errors during transmission. ## About the LAB Color Space The LAB color space, like the RGB color space, is a way to represent colors. LAB can represent all colors visible to the human eye. If you need to learn more about LAB, you can search for relevant articles online, which will provide more details. However, for you, it should be sufficient to understand why LAB is advantageous for MaixPy. Advantages of LAB for MaixPy: 1. The color gamut of the LAB color space is larger than that of RGB, so it can completely replace RGB. 2. In the LAB color space, since the L channel is the brightness channel, we often set it to a relatively large range (commonly [0, 80]), and when coding, we mainly focus on the A and B channels. This can save a lot of time spent struggling with how to select color thresholds. 3. The color perception in the LAB color space is more uniform and easier to debug with code. For example, if you only need to find red color blobs, you can fix the values of the L and B channels and only adjust the value of the A channel (in cases where high color accuracy is not required). For RGB channels, you generally need to adjust all three R, G, and B channels simultaneously to find suitable thresholds."},"/maixpy/doc/en/vision/depth_anything.html":{"title":"MaixCAM2 MaixPy Using Depth-Anything for Monocular Depth Estimation","content":" title: MaixCAM2 MaixPy Using Depth Anything for Monocular Depth Estimation update: date: 2025 06 09 version: v1.0 author: neucrack content: Added support for Depth Anything V2 code and documentation ## Introduction [Depth Anything V2](https://github.com/DepthAnything/Depth Anything V2) can estimate depth maps from regular images. Compared to Depth Anything V1, it has improvements in detail and optimizations in certain materials such as glass. For more information, refer to the [official open source repository](https://github.com/DepthAnything/Depth Anything V2) or the [web homepage](https://depth anything v2.github.io/). <video playsinline controls autoplay loop muted preload src \"../../assets/depth_anything_v2.mp4\" type \"video/mp4\" style \"min height:0\"> depth_anything_v2.mp4 </video> ## Supported Devices Device Supported MaixCAM2 ✅ MaixCAM ❌ ## Using Depth Anything on MaixCAM2 MaixPy MaixPy supports Depth Anything V2. Note that the first generation MaixCAM hardware is not supported. The default system already includes the model, just run the code directly: ```python from maix import camera, display, image, nn, app cmap image.CMap.TURBO model nn.DepthAnything(model \"/root/models/depth_anything_v2_vits.mud\", dual_buff True) cam camera.Camera(model.input_width(), model.input_height(), model.input_format()) disp display.Display() while not app.need_exit(): img cam.read() res model.get_depth_image(img, image.Fit.FIT_CONTAIN, cmap) if res: disp.show(res) ``` Here, the `get_depth_image` function is used to directly obtain a pseudo color image. The `cmap` can be set to a color map. All supported pseudo color maps are documented in the `maix.image.CMap` API. Of course, if you want the model's raw output, you can use the `get_depth` method to obtain the original `float32` data. The returned data can be converted to NumPy format using `maix.tensor.tensor_to_numpy_float32`. Real world performance on MaixCAM2: ![](../../assets/depth_anything_v2_maixcam2.jpg) ## Notes Note that Depth Anything V2 performs depth estimation on a single image, and the raw output is a relative value. The value range varies based on the image content. For example, when depth differences are small, the range might be 0.0~2.0; when there is greater variation, it might be 0.0~8.0. The final image is a normalized value between 0~255. Therefore, for videos or continuous image frames, the absolute depth value may fluctuate. That means when you perform continuous depth estimation using a camera feed, you may observe slight flickering in the output images. ## More Input Resolutions Due to the high computational demand of the model, the default resolution used is 448x336. If you wish to use other resolutions, you can download available models from the [MaixHub model library](https://maixhub.com/model/zoo?platform maixcam2). If MaixHub does not have the resolution you need, you can convert the model yourself. For MaixCAM2, refer to the [model quantization documentation](../ai_model_converter/maixcam2.html) as well as [huggingface.co/AXERA TECH/Depth Anything V2](https://huggingface.co/AXERA TECH/Depth Anything V2/tree/main) and [github.com/AXERA TECH/DepthAnythingV2.axera](https://github.com/AXERA TECH/DepthAnythingV2.axera) for model conversion. (Note: The input in `config.json` in that project is BGR, it is recommended to change it to RGB which is the default in MaixPy.)"},"/maixpy/doc/en/vision/hand_landmarks.html":{"title":"3D Coordinate Detection of 21 Hand Keypoints with MaixPy MaixCAM","content":" title: 3D Coordinate Detection of 21 Hand Keypoints with MaixPy MaixCAM update: date: 2024 12 31 version: v1.0 author: neucrack content: Added source code, models, examples, and documentation ## Introduction In certain applications requiring hand position or gesture detection, this algorithm can be utilized. It provides: * Hand position with coordinates for four vertices. * 3D coordinates of 21 hand keypoints, including depth estimation relative to the palm. Example applications: * Touch reading devices * Gesture control * Finger based games * Sign language translation * Magic casting simulation Sample image: <img src \"../../assets/hands_landmarks.jpg\" style \"max height:24rem\"> Sample video: <video playsinline controls autoplay loop muted preload src \"/static/video/hands_landmarks.mp4\" type \"video/mp4\"> Classifier Result video </video> The 21 keypoints include: ![](../../assets/hand_landmarks_doc.jpg) ## Using Hand Keypoint Detection in MaixPy MaixCAM The **MaixPy** platform integrates this algorithm (ported from MediaPipe for ease of use, firmware version **> 4.9.3** is required). The example can also be found in the [MaixPy/examples](https://github.com/sipeed/maixpy) directory: ```python from maix import camera, display, image, nn, app detector nn.HandLandmarks(model \"/root/models/hand_landmarks.mud\") # detector nn.HandLandmarks(model \"/root/models/hand_landmarks_bf16.mud\") landmarks_rel False cam camera.Camera(320, 224, detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.7, iou_th 0.45, conf_th2 0.8, landmarks_rel landmarks_rel) for obj in objs: # img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.points[0], obj.points[1], msg, color image.COLOR_RED if obj.class_id 0 else image.COLOR_GREEN, scale 1.4, thickness 2) detector.draw_hand(img, obj.class_id, obj.points, 4, 10, box True) if landmarks_rel: img.draw_rect(0, 0, detector.input_width(detect False), detector.input_height(detect False), color image.COLOR_YELLOW) for i in range(21): x obj.points[8 + 21*3 + i * 2] y obj.points[8 + 21** + i * 2 + 1] img.draw_circle(x, y, 3, color image.COLOR_YELLOW) disp.show(img) ``` Detection results are visualized using the `draw_hand` function. Keypoint data can be accessed via `obj.points`, providing `4 + 21` points: * The first 4 points are the bounding box corners in clockwise order: `topleft_x, topleft_y, topright_x, topright_y, bottomright_x, bottomright_y, bottomleft_x, bottomleft_y`. Values may be negative. * The remaining 21 points are keypoints in the format: `x0, y0, z0, x1, y1, z1, ..., x20, y20, z20`, where `z` represents depth relative to the palm and may also be negative. Additionally, `obj.x, y, w, h, angle` attributes provide the bounding box and rotation details. **Precision Optimization**: The `nn.HandLandmarks` class uses an `int8` quantized model by default for faster detection. For higher precision, switch to the `hand_landmarks_bf16.mud` model. **Relative Landmark Coordinates**: By setting the `landmarks_rel` parameter to `True`, the function will output the 21 keypoints as relative coordinates to the top left corner of the hand's bounding box. In this case, the last `21x2` values in `obj.points` are arranged as `x0, y0, x1, y1, ..., x20, y20`. ## Advanced: Gesture Recognition Based on Keypoint Detection ### Example: Rock Paper Scissors Detection Two approaches: 1. **Traditional Method**: Use code to classify gestures based on keypoint analysis. 2. **AI Model Based Method**: Train a classification model. **Approach 2**: This involves using the 21 keypoints as input for a classification model. Without image background interference, fewer data samples are needed for effective training. Steps: 1. Define gesture categories (e.g., rock, paper, scissors). 2. Record keypoint data upon user input. 3. Normalize keypoint coordinates to relative values (0 to object width `obj.w`) using `landmarks_rel` parameter as described above. 4. Collect data for each category. 5. Train a classification model (e.g., using MobileNetV2 in PyTorch). 6. Convert the trained model to MaixCAM supported format. This approach requires knowledge of training and quantizing classification models. ## Simplified Model Training Alternative For users unfamiliar with PyTorch: 1. Generate an image from the 21 keypoints (customize visualization). 2. Upload the images to [MaixHub.com](https://maixhub.com) for model training. 3. Use the trained model in MaixPy for classification. ## Complex Action Recognition For actions requiring time series analysis (e.g., circular motions): * Store keypoint history in a queue for temporal analysis. * Input historical sequences into a classification model for time series gesture recognition. * Alternatively, generate a single image from historical data and classify it. These methods allow advanced gesture and action recognition leveraging MaixPy's integrated tools. This version includes all details, including the explanation for `landmarks_rel`."},"/maixpy/doc/en/vision/ocr.html":{"title":"OCR Image Text Recognition with MaixCAM MaixPy","content":" title: OCR Image Text Recognition with MaixCAM MaixPy ## Introduction to OCR OCR (Optical Character Recognition) refers to the visual recognition of text in images. It can be applied in various scenarios, such as: * Recognizing text/numbers on cards * Extracting text from cards, such as ID cards * Digitizing paper documents * Reading digital displays, useful for meter reading and digitizing old instrument data * License plate recognition ## Using OCR in MaixPy MaixPy has integrated [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), an open source OCR algorithm developed by Baidu. For understanding the principles, you can refer to this open source project. ![OCR](../../assets/ocr.jpg) **First, ensure that your MaixPy version is > 4.6.** Then, execute the code: (The complete, latest code can be found in the [MaixPy repository](https://github.com/sipeed/MaixPy/blob/main/examples/vision/ai_vision/nn_pp_ocr.py); please refer to the source code.) ```python from maix import camera, display, image, nn, app model \"/root/models/pp_ocr.mud\" ocr nn.PP_OCR(model) cam camera.Camera(ocr.input_width(), ocr.input_height(), ocr.input_format()) disp display.Display() image.load_font(\"ppocr\", \"/maixapp/share/font/ppocr_keys_v1.ttf\", size 20) image.set_default_font(\"ppocr\") while not app.need_exit(): img cam.read() objs ocr.detect(img) for obj in objs: points obj.box.to_list() img.draw_keypoints(points, image.COLOR_RED, 4, 1, 1) img.draw_string(obj.box.x4, obj.box.y4, obj.char_str(), image.COLOR_RED) disp.show(img) ``` You can see that `ocr nn.PP_OCR(model)` loads the model, and then `ocr.detect(img)` detects and recognizes the text, displaying the results on the screen. ## More Model Options You can download more complete models with different input resolutions, languages, and versions from the [MaixHub Model Download](https://maixhub.com/model/zoo/449) (MaixPy currently defaults to the pp_ocr.mud model, which uses PPOCRv3 for detection and v4 for recognition). ## Recognizing Without Detection If you already have a processed image with known coordinates for the four corners of the text, you can skip calling the `detect` function and simply call the `recognize` function. This way, it will only recognize the text in the image without detection. ## Custom Models The default model provides detection and recognition for Chinese and English text. If you have specific requirements, such as another language or only want to detect certain shapes without recognizing all types of text, you can download the corresponding model from the [PaddleOCR Official Model Library](https://paddlepaddle.github.io/PaddleOCR/ppocr/model_list.html) and convert it to a format supported by MaixCAM. The most complex part here is converting the model into a format usable by MaixCAM, which is a **relatively complex** process that requires basic Linux skills and adaptability. * First, either train your model using PaddleOCR source code or download the official models. Choose PP OCRv3 for detection because it is efficient and faster than v4, and download the v4 model for recognition; tests show that v3 does not perform well when quantized on MaixCAM. * Then, convert the model to ONNX: ```shell model_path ./models/ch_PP OCRv3_rec_infer paddle2onnx model_dir ${model_path} model_filename inference.pdmodel params_filename inference.pdiparams save_file ${model_path}/inference.onnx opset_version 14 enable_onnx_checker True ``` * Next, set up the environment according to the [ONNX to MUD format model documentation](../ai_model_converter/maixcam.html) and convert the model. Sample conversion scripts are provided in the appendix. * Finally, load and run it using MaixPy. ## Appendix: Model Conversion Scripts Detection: ```shell #!/bin/bash set e net_name ch_PP_OCRv3_det input_w 320 input_h 224 output_name sigmoid_0.tmp_0 # scale 1/255.0 # \"mean\": [0.485, 0.456, 0.406], # \"std\": [0.229, 0.224, 0.225], # mean: mean * 255 # scale: 1/(std*255) # mean: 123.675, 116.28, 103.53 # scale: 0.01712475, 0.017507, 0.01742919 mkdir p workspace cd workspace # convert to mlir model_transform.py \\ model_name ${net_name} \\ model_def ../${net_name}.onnx \\ input_shapes [[1,3,${input_h},${input_w}]] \\ mean \"123.675,116.28,103.53\" \\ scale \"0.01712475,0.017507,0.01742919\" \\ keep_aspect_ratio \\ pixel_format bgr \\ channel_format nchw \\ output_names \"${output_name}\" \\ test_input ../test_images/test3.jpg \\ test_result ${net_name}_top_outputs.npz \\ tolerance 0.99,0.99 \\ mlir ${net_name}.mlir # export bf16 model # not use quant_input, use float32 for easy coding model_deploy.py \\ mlir ${net_name}.mlir \\ quantize BF16 \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ model ${net_name}_bf16.cvimodel echo \"calibrate for int8 model\" # export int8 model run_calibration.py ${net_name}.mlir \\ dataset ../images \\ input_num 200 \\ o ${net_name}_cali_table echo \"convert to int8 model\" # export int8 model # add quant_input, use int8 for faster processing in maix.nn.NN.forward_image model_deploy.py \\ mlir ${net_name}.mlir \\ quantize INT8 \\ quant_input \\ calibration_table ${net_name}_cali_table \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ tolerance 0.9,0.5 \\ model ${net_name}_int8.cvimodel ``` Recognition: ```shell #!/bin/bash set e # net_name ch_PP_OCRv4_rec # output_name softmax_11.tmp_0 net_name ch_PP_OCRv3_rec_infer_sophgo output_name softmax_5.tmp_0 input_w 320 input_h 48 cali_images ../images_crop_320 # scale 1/255.0 # \"mean\": [0.5, 0.5, 0.5], # \"std\": [0.5, 0.5, 0.5], # mean: mean * 255 # scale: 1/(std*255) # mean: 127.5,127.5,127.5 # scale: 0.00784313725490196,0.00784313725490196,0.00784313725490196 mkdir p workspace cd workspace # convert to mlir model_transform.py \\ model_name ${net_name} \\ model_def ../${net_name}.onnx \\ input_shapes [[1,3,${input_h},${input_w}]] \\ mean \"127.5,127.5,127.5\" \\ scale \"0.00784313725490196,0.00784313725490196,0.00784313725490196\" \\ keep_aspect_ratio \\ pixel_format bgr \\ channel_format nchw \\ output_names \"${output_name}\" \\ test_input ../test_images/test3.jpg \\ test_result ${net_name}_top_outputs.npz \\ tolerance 0.99,0.99 \\ mlir ${net_name}.mlir # export bf16 model # not use quant_input, use float32 for easy coding model_deploy.py \\ mlir ${net_name}.mlir \\ quantize BF16 \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ model ${net_name}_bf16.cvimodel echo \"calibrate for int8 model\" # export int8 model run_calibration.py ${net_name}.mlir \\ dataset $cali_images \\ input_num 200 \\ o ${net_name}_cali_table echo \"convert to int8 model\" # export int8 model # add quant_input, use int8 for faster processing in maix.nn.NN.forward_image model_deploy.py \\ mlir ${net_name}.mlir \\ quantize INT8 \\ quant_input \\ calibration_table ${net _name}_cali_table \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ tolerance 0.9,0.5 \\ model ${net_name}_int8.cvimodel ```"},"/maixpy/doc/en/vision/display.html":{"title":"MaixCAM MaixPy Screen Usage","content":" title: MaixCAM MaixPy Screen Usage update: date: 2024 03 31 author: neucrack version: 1.0.0 content: Initial document ## Introduction MaixPy provides the `display` module, which can display images on the screen, and can also send images to MaixVision for display, facilitating debugging and development. ## API Documentation This document introduces commonly used methods. For more APIs, please refer to the [display](/api/maix/display.html) section of the API documentation. ## Using the Screen * Import the `display` module: ```python from maix import display ``` * Create a `Display` object: ```python disp display.Display() ``` * Display an image: ```python disp.show(img) ``` Here, the `img` object is a `maix.image.Image` object, which can be obtained through the `read` method of the `camera` module, or loaded from an image file in the file system using the `load` method of the `image` module, or created as a blank image using the `Image` class of the `image` module. For example: ```python from maix import image, display disp display.Display() img image.load(\"/root/dog.jpg\") disp.show(img) ``` Here, you need to transfer the `dog.jpg` file to the `/root` directory on the device first. Display text: ```python from maix import image, display disp display.Display() img image.Image(320, 240) img.draw_rect(0, 0, disp.width(), disp.height(), color image.Color.from_rgb(255, 0, 0), thickness 1) img.draw_rect(10, 10, 100, 100, color image.Color.from_rgb(255, 0, 0)) img.draw_string(10, 10, \"Hello MaixPy!\", color image.Color.from_rgb(255, 255, 255)) disp.show(img) ``` Read an image from the camera and display it: ```python from maix import camera, display, app disp display.Display() cam camera.Camera(320, 240) while not app.need_exit(): img cam.read() disp.show(img) ``` > Here, `while not app.need_exit():` is used to facilitate exiting the loop when the `app.set_exit_flag()` method is called elsewhere. ## Adjusting Backlight Brightness You can manually adjust the backlight brightness in the system's \"Settings\" app. If you want to adjust the backlight brightness programmatically, you can use the `set_backlight` method, with the parameter being the brightness percentage, ranging from 0 to 100: ```python disp.set_backlight(50) ``` Note that when the program exits and returns to the app selection interface, the backlight brightness will automatically revert to the system setting. If the brightness is set to `100%` and still feels dim, you can try modifying the `disp_max_backlight 50` option in the `/boot/board` file to a larger value. When `disp_max_backlight 100` and `disp.set_backlight(100)` are set, the hardware backlight control pin outputs a 100% duty cycle, which is a high level. The final duty cycle output to the hardware is calculated as: `set_backlight value` * `disp_max_backlight`. **Note**: Increasing the maximum brightness limit will lead to higher power consumption and heat generation. Adjust it reasonably based on your actual needs and avoid blindly maxing out the brightness. ## Displaying on MaixVision When running code in MaixVision, images can be displayed on MaixVision for easier debugging and development. When calling the `show` method, the image will be automatically compressed and sent to MaixVision for display. Of course, if you don't have a screen, or to save memory by not initializing the screen, you can also directly call the `send_to_maixvision` method of the `maix.display` object to send the image to MaixVision for display. ```python from maix import image,display from maix import image,display img image.Image(320, 240) disp display.Display() img.draw_rect(0, 0, img.width(), img.height(), color image.Color.from_rgb(255, 0, 0), thickness 1) img.draw_rect(10, 10, 100, 100, color image.Color.from_rgb(255, 0, 0)) img.draw_string(10, 10, \"Hello MaixPy!\", color image.Color.from_rgb(255, 255, 255)) display.send_to_maixvision(img) ``` ## Replacing with Other Screen Models If you wish to switch to a screen of a different size, you can consult and purchase from the [store](https://wiki.sipeed.com/store). For **MaixCAM**, currently, there are 4 types of screens and 1 type of MIPI to HDMI module supported: * 2.3 inch 552x368 resolution capacitive touch screen: The default screen that comes with MaixCAM. * 2.4 inch 640x480 resolution capacitive touch screen: The default screen that comes with MaixCAM Pro. * 5 inch 854x480 resolution non touch screen: Note that this is a non touch screen, similar in size to a mobile phone screen. * 7 inch 1280x800 resolution capacitive touch screen: A large 7 inch screen, suitable for scenarios requiring a fixed screen display. * LT9611 (MIPI to HDMI module) supports various resolutions including 1280x720, suitable for driving various HDMI screens. The image refresh time difference between different screens is about 1 5 milliseconds, which is not significant; the main difference lies in the image resolution, which affects image processing time. When replacing the screen, you must also **modify the configuration file**; otherwise, mismatched refresh timing could **cause screen burn in** (leaving a ghost image on the screen). It’s important to follow the steps strictly as outlined below. If screen burn in occurs, don’t panic; powering off and leaving it overnight usually resolves the issue. * Follow the system burning documentation to burn the system. Once completed, a USB drive will appear. * Open the USB drive, and you will see a `board` file. * Edit the `board` file, modifying the `pannel` key value as follows: * 2.3 inch (MaixCAM default screen): `st7701_hd228001c31`. * 2.4 inch (MaixCAM Pro default screen): `st7701_lct024bsi20`. * 5 inch: `st7701_dxq5d0019_V0`, with the earlier (2023) test screen being `st7701_dxq5d0019b480854`. * 7 inch: `mtd700920b`, with the earlier (2023) test screen being `zct2133v1`. * LT9611 (MIPI to HDMI module): * Wiring: * LT9611 I2C < > MaixCAM I2C5 * LT9611 MIPI IN < > MaixCAM MIPI OUT * Supported configurations: * `lt9611_1280x720_60hz`: 1280x720 60Hz * `lt9611_1024x768_60hz`: 1024x768 60Hz * `lt9611_640x480_60hz`: 640x480 60Hz * `lt9611_552x368_60hz`: 552x368 60Hz * Save the `board` file, and **click to eject the USB drive**—do not just disconnect the power, or the file may be lost. * Press the board's `reset` button, or power cycle to restart. The above method is the safest, ensuring the screen model is set correctly before powering on. If you have already burned the system, you can also modify the system’s `/boot/board` file and then reboot. > If you use earlier system and binary program(< 2024.11.25), you may have to change `uEnv.txt` too. For the **MaixCAM2**, only the 2.4 inch 640x480 resolution capacitive touchscreen is currently supported."},"/maixpy/doc/en/vision/object_track.html":{"title":"MaixCAM MaixPy Object Tracking and Counting (e.g., Pedestrian Counting)","content":" title: MaixCAM MaixPy Object Tracking and Counting (e.g., Pedestrian Counting) ## Introduction to Object Tracking Previously, we used YOLOv5, YOLOv8, or even `find_blobs` to detect objects. However, when there are multiple objects in the frame and we need to distinguish between each object, object tracking becomes necessary. For instance, if there are five people moving in the frame, we need to assign each person a number and track their movement. Applications: * Pedestrian counting, such as counting the number of people passing through a certain area. * Counting workpieces, such as counting products on a production line. * Recording and recognizing the movement trajectories of objects. ## MaixCAM/MaixPy Object Tracking and Pedestrian Counting Results As shown in the video below, the system can track each person and count those who cross the yellow area from top to bottom (displayed in the lower left corner): <video playsinline controls autoplay loop muted preload src \"/static/video/tracker.mp4\" style \"width: 100%; min height: 20em;\"></video> ## Using MaixCAM/MaixPy for Object Tracking and Pedestrian Counting You can directly install the [application](https://maixhub.com/app/61) to experience it. You can also check the [examples in the `examples/vision/tracker` directory](https://github.com/sipeed/MaixPy/tree/main/examples/vision/tracker). The `tracker_bytetrack.py` example is a basic object tracking example and involves several steps: * Use YOLOv5 or YOLOv8 to detect objects. This allows you to replace the model to detect different objects according to your needs. * Use the `maix.tracker.ByteTracker` algorithm for object tracking. Simply calling the `update` function will give you the results (the trajectory of each object in the frame), which is very straightforward. Several parameters need to be adjusted according to your specific scenario. Refer to the example code and API documentation for detailed parameter descriptions: ```python # configs conf_threshold 0.3 # detection threshold iou_threshold 0.45 # detection IOU threshold max_lost_buff_time 120 # the number of frames to keep lost tracks track_thresh 0.4 # tracking confidence threshold high_thresh 0.6 # threshold to add a new track match_thresh 0.8 # matching threshold for tracking; if IOU < match_thresh between an object in two frames, they are considered the same object max_history_num 5 # maximum length of a track's position history show_detect False # show detection valid_class_id [0] # classes used in the detection model ``` The `tracker_bytetrack_count.py` example adds pedestrian counting. To keep it simple, the example only implements counting for people walking from top to bottom. If a person is below the yellow area and their trajectory crosses into the yellow area, they are counted as crossing from top to bottom. You can write custom logic based on your specific application scenario."},"/maixpy/doc/en/vision/line_tracking.html":{"title":"MaixCAM MaixPy Line Tracking","content":" title: MaixCAM MaixPy Line Tracking update: date: 2024 05 09 author: lxowalle version: 1.0.0 content: Initial document Before reading this article, make sure you already know how to develop MaixCAM. For details, please read [Quick Start](../index.html). ## Introduction In vision applications, the function of tracking line is often required in applications such as line following robot. In this article, we will describe: How to use MaixPy to tracking line. How to tracking line using MaixCam's default application ## How to use MaixPy to tracking line The `maix.image.Image` module in MaixPy provides the `get_regression` method, which can conveniently tracking line. ### Code example A simple example of finding and drawing a line. ```python from maix import camera, display, image cam camera.Camera(320, 240) disp display.Display() # thresholds [[0, 80, 40, 80, 10, 80]] # red thresholds [[0, 80, 120, 10, 0, 30]] # green # thresholds [[0, 80, 30, 100, 120, 60]] # blue while 1: img cam.read() lines img.get_regression(thresholds, area_threshold 100) for a in lines: img.draw_line(a.x1(), a.y1(), a.x2(), a.y2(), image.COLOR_GREEN, 2) theta a.theta() rho a.rho() if theta > 90: theta 270 theta else: theta 90 theta img.draw_string(0, 0, \"theta: \" + str(theta) + \", rho: \" + str(rho), image.COLOR_BLUE) disp.show(img) ``` Steps: 1. import image, camera, display modules ```python from maix import image, camera, display ``` 2. Initialize camera and display ```python cam camera.Camera(320, 240) # Initialise camera, output resolution 320x240 in RGB format. disp display.Display() ``` 3. Get the image from the camera and display it ```python while 1: img cam.read() disp.show(img) ``` 4. Call the `get_regression` method to find the straight line in the camera image and draw it to the screen ```python lines img.get_regression(thresholds, area_threshold 100) for a in lines: img.draw_line(a.x1(), a.y1(), a.x2(), a.y2(), image.COLOR_GREEN, 2) theta a.theta() rho a.rho() if theta > 90: theta 270 theta else: theta 90 theta img.draw_string(0, 0, \"theta: \" + str(theta) + \", rho: \" + str(rho), image.COLOR_BLUE) ``` `img` is the camera image read via `cam.read()`, when initialised as `cam camera.Camera(320, 240)`, the `img` object is an RGB image with a resolution of 320x240. `img.get_regression` is used to find straight lines, `thresholds` is a list of colour thresholds, each element is a colour threshold, multiple thresholds are passed in if multiple thresholds are found at the same time, and each colour threshold has the format `[L_MIN, L_MAX, A_MIN, A_MAX, B_MIN, B_MAX]`, where ` L`, `A`, `B` are the three channels of `LAB` colour space, `L` channel is the luminance, `A` channel is the red green channel, `B` channel is the blue yellow channel. `pixels_threshold` is a pixel area threshold used to filter some unwanted straight lines. `for a in lines` is used to iterate through the returned `Line` objects, where `a` is the current `Line` object. Normally the `get_regression` function will only return one `Line` object, but if you need to find more than one line, try the `find_line` method. Use `img.draw_line` to draw the found line, `a.x1(), a.y1(), a.x2(), a.y2()` represent the coordinates of the ends of the line. Use `img.draw_string` to show the angle between the line and the x axis in the upper left corner, and `a.theta()` is the angle between the line and the y axis, which is converted to `theta` for easier understanding, `a.rho()` is the length of the vertical line from the origin to the line. 5. Run the code through the maixvision, you can find the line, look at the effect! ![image 20240509110204007](../../../static/image/line_tracking_demo.jpg) ### Common Parameter Explanations Here are explanations of commonly used parameters. If you cannot find parameters that can implement your application, you may need to consider using other algorithms or extending the required functionality based on the current algorithm's results. Parameter Description Example thresholds Thresholds based on the LAB color space, thresholds [[l_min, l_max, a_min, a_max, b_min, b_max]], representing:<br/>Brightness range [l_min, l_max]<br/>Green to red component range [a_min, a_max]<br/>Blue to yellow component range [b_min, b_max]<br/>Multiple thresholds can be set simultaneously Set two thresholds to detect red and green<br/>```img.find_blobs(thresholds [[0, 80, 40, 80, 10, 80], [0, 80, 120, 10, 0, 30]])```<br/>Red threshold is [0, 80, 40, 80, 10, 80]<br/>Green threshold is [0, 80, 120, 10, 0, 30] invert Enable threshold inversion, when enabled, the passed thresholds are inverted. Default is False. Enable threshold inversion<br/>```img.find_blobs(invert True)``` roi Set the rectangular region for the algorithm to compute, roi [x, y, w, h], where x and y represent the coordinates of the top left corner of the rectangle, and w and h represent the width and height of the rectangle, respectively. The default is the entire image. Compute the region at (50, 50) with a width and height of 100<br/>```img.find_blobs(roi [50, 50, 100, 100])``` area_threshold Filter out blobs with a pixel area smaller than area_threshold, in units of pixels. The default is 10. This parameter can be used to filter out some useless small blobs. Filter out blobs with an area smaller than 1000<br/>```img.find_blobs(area_threshold 1000)``` pixels_threshold Filter out blobs with fewer valid pixels than pixels_threshold. The default is 10. This parameter can be used to filter out some useless small blobs. Filter out blobs with fewer than 1000 valid pixels<br/>```img.find_blobs(pixels_threshold 1000)``` This article introduces commonly used methods. For more APIs, please see the [image](../../../api/maix/image.html) section of the API documentation. ### Increasing the speed of line tracking Here are a few ways to increase the speed of line tracking 1. Choose a suitable resolution The larger the resolution, the slower the calculation speed, you can choose a more suitable resolution according to the recognition distance and accuracy requirements. 2. Use gray scale image When using gray scale recognition, the algorithm will only process one channel, there is a faster recognition speed, in the environment of a single color will be very useful. Note that only `l_min` and `l_max` are valid when passing `thresholds` to `get_regression` when using gray scale image recognition. Methods for get gray scale image: ```python # Example 1 cam camera.Camera(320, 240， image.Format.FMT_GRAYSCALE) \t# Support after MaixPy v4.2.1 gray_img cam.read()\t\t\t\t\t\t\t\t\t\t\t# get gray scale image # Example 2 cam camera.Camera(320, 240) img cam.read() gray_img img.to_format(image.Format.FMT_GRAYSCALE)\t\t\t# get gray scale image ``` ## How to tracking line using MaixCam's default application To quickly verify the line tracking functionality, you can use the `line_tracking` application provided by MaixCam to experience the line finding effect. ### How to use it 1. Select and open the `Line tracking` application. 2. Click on the line in the screen that needs to be identified and the colour of the line will be displayed on the left hand side 3. Click on the colour to be detected on the left (the colour below L A B in the screen) 4. The line will be identified and the coordinates and angle of the line will be output from the serial port. ### Demo <video src \"/static/video/line_tracking_app.mp4\" controls \"controls\" width \"100%\" height \"auto\"></video> ### Advanced operations #### Manual adjustment of LAB threshold to tracking line The application provides manual setting of LAB threshold to tracking line accurately. Steps: 1. `Click` the `options icon` in the bottom left corner to enter configuration mode. 2. Point the `camera` at the `object` you need to `find`, `click` on the `target object` on the screen, and the `left side` will display a `rectangular frame` of the object's color and show the `LAB values` of that color. 3. Click on the bottom options `L Min`, `L Max`, `A Min`, `A Max`, `B Min`, `B Max`. After clicking, a slider will appear on the right side to set the value for that option. These values correspond to the minimum and maximum values of the L, A, and B channels in the LAB color format, respectively. 4. Referring to the `LAB values` of the object color calculated in step 2, adjust `L Min`, `L Max`, `A Min`, `A Max`, `B Min`, `B Max` to appropriate values to identify the corresponding color blobs. For example, if `LAB (20, 50, 80)`, since `L 20`, to accommodate a certain range, set `L Min 10` and `L Max 30`. Similarly, since `A 50`, set `A Min 40` and `A Max 60`. Since `B 80`, set `B Min 70` and `B Max 90`. #### Getting Detection Data via Serial Protocol The line tracking application supports reporting detected straight line information via the serial port (default baud rate is 115200). Since only one report message is sent, we can illustrate the content of the report message with an example. For instance, if the report message is: ```shell AA CA AC BB 0E 00 00 00 00 E1 09 FC 01 01 00 E9 01 6F 01 57 00 C1 C6 ``` `AA CA AC BB`: Protocol header, fixed content `0E 00 00 00`: Data length, the total length excluding the protocol header and data length, here means the length is 14. `E1`: Flag bit, used to identify the serial message flag `09`: Command type, for the line tracking application, this value is fixed at 0x09. `FC 01 01 00 E9 01 6F 01 57 00`: The coordinates and angle information for both ends of line, with each value represented as a 2 byte value in little end format. `FC 01` and `01 00` indicate that the coordinates of the first endpoint are (508, 1), `E9 01` and `6F 01` indicate that the coordinates of the second endpoint are (489, 367), and `57 00` indicates that the angle of the line to the x axis is 87 degrees `C1 C6`: CRC checksum value, used to verify if the frame data has errors during transmission."},"/maixpy/doc/en/vision/segmentation.html":{"title":"MaixCAM MaixPy Image Semantic Segmentation","content":" title: MaixCAM MaixPy Image Semantic Segmentation ## Introduction Image semantic segmentation refers to identifying specific objects in an image and recognizing the pixels that represent the parts of those objects. For example, in the image below, the human body and the dog are identified, and their body parts are segmented. This can be used for collision detection, autonomous vehicle navigation, area measurement, and more. ![](../../assets/yolov8_seg.jpg) ## Image Semantic Segmentation with MaixPy MaixPy includes `YOLOv8 seg` and `YOLO11 seg` for object detection and image segmentation. MaixPy provides a model for 80 object categories from the COCO dataset by default. > To use YOLOv8, MaixPy version must be > 4.4.0 > To use YOLO11, MaixPy version must be > 4.7.0 The following code demonstrates the usage, and you can also find it in [MaixPy examples](https://github.com/sipeed/maixpy/tree/main/examples/). ```python from maix import camera, display, image, nn, app, time detector nn.YOLOv8(model \"/root/models/yolov8n_seg.mud\", dual_buff True) # detector nn.YOLO11(model \"/root/models/yolo11n_seg.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: # img.draw_image(obj.x, obj.y, obj.seg_mask) detector.draw_seg_mask(img, obj.x, obj.y, obj.seg_mask, threshold 127) img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` > To switch between YOLOv8 and YOLO11, just modify the commented part of the above code. ## Models with More Resolutions The default model resolution is 320x224. For models with different resolutions, download them from the MaixHub model library: * YOLOv8 seg: [[MaixHub Model Library](https://maixhub.com/model/zoo/413)](https://maixhub.com/model/zoo/413) * YOLO11 seg: [[MaixHub Model Library](https://maixhub.com/model/zoo/455)](https://maixhub.com/model/zoo/455) ## dual_buff for Double Buffering Acceleration You may notice that `dual_buff` is used for model initialization (default value is `True`). Enabling the `dual_buff` parameter can improve efficiency and increase the frame rate. For more details and considerations, refer to the [dual_buff Introduction](./dual_buff.html). ## Customizing Your Own Object Segmentation Model The provided models are based on the 80 categories from the COCO dataset. If this does not meet your needs, you can train your own specific object detection and segmentation model. Follow the instructions in [Offline Training YOLOv8/YOLO11](./customize_model_yolov8.html) to use the official YOLOv8/YOLO11 model training method, and then convert it to a model format supported by MaixCAM."},"/maixpy/doc/en/vision/face_detection.html":{"title":"MaixCAM MaixPy Face Detection and Keypoint Detection","content":" title: MaixCAM MaixPy Face Detection and Keypoint Detection ## Introduction Face detection can be applied in many scenarios, such as providing the face detection step for face recognition, or for face tracking applications, etc. The face detection provided here can not only detect faces but also detect 5 key points, including two eyes, one nose, and two corners of the mouth. ![face detection](../../assets/face_detection.jpg) ## Using Face Detection in MaixPy MaixPy officially provides three face detection models from the open source projects [Face Detector 1MB with landmark](https://github.com/biubug6/Face Detector 1MB with landmark), [Retinaface](https://github.com/biubug6/Pytorch_Retinaface), and [YOLOv8 face](https://github.com/derronqi/yolov8 face). All three models can be used. `YOLOv8 face` performs better but is slightly slower, so you can choose based on your testing. Using `YOLOv8 face` (requires MaixPy version > 4.3.8): ```python from maix import camera, display, image, nn, app detector nn.YOLOv8(model \"/root/models/yolov8n_face.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45, keypoint_th 0.5) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) detector.draw_pose(img, obj.points, 2, image.COLOR_RED) disp.show(img) ``` For the other two models: Here, a line of commented out code is used to load the `Retinaface` model. Choose which line of code to use based on the model you download. ```python from maix import camera, display, image, nn, app import math detector nn.Retinaface(model \"/root/models/retinaface.mud\") # detector nn.FaceDetector(model \"/root/models/face_detector.mud\") cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.4, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) radius math.ceil(obj.w / 10) img.draw_keypoints(obj.points, image.COLOR_RED, size radius if radius < 5 else 4) disp.show(img) ``` ## Model Downloads and Other Resolution Models Download the models; the compressed package contains multiple resolutions to choose from. Higher resolution models are more accurate but take longer to process: * [Face Detector 1MB with landmark](https://maixhub.com/model/zoo/377) * [Retinaface](https://maixhub.com/model/zoo/378) * [YOLOv8 face](https://maixhub.com/model/zoo/407) ## dual_buff Dual Buffer Acceleration You may have noticed that the model initialization uses `dual_buff` (which defaults to `True`). Enabling the `dual_buff` parameter can improve running efficiency and increase the frame rate. For detailed principles and usage notes, see [dual_buff Introduction](./dual_buff.html)."},"/maixpy/doc/en/vision/dual_buff.html":{"title":"Introduction to Running Models in Dual Buffer Mode with MaixPy MaixCAM","content":" title: Introduction to Running Models in Dual Buffer Mode with MaixPy MaixCAM ## Introduction You may have noticed that there is a parameter `dual_buff True` when initializing the code for model running. For example, in `YOLOv5`: ```python from maix import camera, display, image, nn, app detector nn.YOLOv5(model \"/root/models/yolov5s.mud\", dual_buff True) # detector nn.YOLOv8(model \"/root/models/yolov8n.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` Generally, this parameter defaults to `True`, unless you manually set `dual_buff False` to disable the dual buffer function. Enabling this feature improves running efficiency, thereby increasing the frame rate (assuming the camera's frame rate is not limited, the above code will halve the loop time on MaixCAM, effectively doubling the frame rate). However, there are drawbacks. The `detect` function returns the result of the previous call to the `detect` function, meaning there is a one frame delay between the result and the input. If you want the detection result to match the input `img` rather than the previous frame, disable this feature. Additionally, due to the preparation of dual buffers, memory usage will increase. If you encounter insufficient memory issues, you will also need to disable this feature. ## Principle Model object detection involves several steps: * Capturing the image * Image preprocessing * Model execution * Post processing the results Only the model execution step runs on the hardware NPU, while other steps run on the CPU. If `dual_buff` is set to `False`, during `detect`, the CPU preprocesses (while the NPU is idle), then the NPU performs the computation (while the CPU is idle waiting for the NPU to finish), and then the CPU post processes (while the NPU is idle). This process is linear and relatively simple. However, a problem arises because either the CPU or the NPU is always idle. When `dual_buff True` is enabled, the CPU preprocesses and hands off to the NPU for computation. At this point, the CPU does not wait for the NPU to produce results but instead exits the `detect` function and proceeds to the next camera read and preprocess. Once the NPU finishes its computation, the CPU has already prepared the next data, immediately passing it to the NPU to continue computing without giving the NPU any idle time. This maximizes the efficient simultaneous operation of both the CPU and NPU. However, note that if the camera frame rate is not high enough, it will still limit the overall frame rate."},"/maixpy/doc/en/vision/customize_model_yolov5.html":{"title":"Offline Training of YOLOv5 Model for Custom Object Detection with MaixCAM MaixPy","content":" title: Offline Training of YOLOv5 Model for Custom Object Detection with MaixCAM MaixPy update: date: 2024 6 20 version: v1.0 author: neucrack content: Documentation written date: 2025 7 01 version: v2.0 author: neucrack content: Add MaixCAM2 Support ## Introduction The default official model provides detection for 80 types of objects. If this does not meet your needs, you can train your own detection objects using two methods: * Use [MaixHub Online Training](./maixhub_train.html), which is convenient and fast, without needing to buy a server or set up an environment, just a few clicks of the mouse. * Set up a training environment on your own computer or server. The former is simple and quick, while the latter uses your own computer and the number of training images is not limited, but the latter is much more difficult. **Note:** This article explains how to customize training, but some basic knowledge is assumed. If you do not have this knowledge, please learn it yourself: * This article will not explain how to install the training environment. Please search and install it yourself (Pytorch environment installation) and test it. * This article will not explain the basic concepts of machine learning or basic Linux usage knowledge. If you think there is something in this article that needs improvement, feel free to click `Edit this article` in the upper right corner to contribute and submit a documentation PR. ## Process and Goals of this Article To use our model on MaixPy (MaixCAM), the following process is required: * Set up the training environment (this is not covered in this article, please search for Pytorch training environment setup). * Pull the [yolov5](https://github.com/ultralytics/yolov5) source code to your local machine. * Prepare the dataset and format it as required by the yolov5 project. * Train the model to get an `onnx` model file, which is the final output file of this article. * Convert the `onnx` model into a MaixPy supported `MUD` file. This process is detailed in the model conversion articles: * [MaixCAM Model Conversion](../ai_model_converter/maixcam.html) * [MaixCAM2 Model Conversion](../ai_model_converter/maixcam2.html) * Use MaixPy to load and run the model. ## Reference Articles Since this is a relatively common operational process, this article only provides an overview. For specific details, you can refer to the **[YOLOv5 official code and documentation](https://github.com/ultralytics/yolov5)** (**recommended**), and search for training tutorials to ultimately export the onnx file. Here are some articles from the MaixHub community: * [Deploy yolov5s custom model on maixcam](https://maixhub.com/share/23) * [【Process Sharing】YOLOv5 training custom dataset and deploying on Maixcam](https://maixhub.com/share/32) * [YOLOv5 cat and dog recognition model—free cloud training (reproducible by beginners)](https://maixhub.com/share/25) If you find any good articles, feel free to modify this article and submit a PR. ## Exporting YOLOv5 ONNX Model File YOLOv5 provides an export option. Execute the following command in the `yolov5` directory: ```shell python export.py weights ../yolov5s.pt include onnx img 224 320 ``` This command loads the `pt` parameter file and converts it to `onnx`, while also specifying the resolution. Note that the height comes first, followed by the width. The model was trained with `640x640`, but we re specified the resolution to improve the running speed. The resolution `320x224` is used because it is closer to the MaixCAM screen ratio for better display. You can set it according to your needs. ## MaixCAM MUD File When converting an ONNX model to the `mud` format model file, refer to [MaixCAM Model Conversion](../ai_model_converter/maixcam.html) and [MaixCAM2 Model Conversion](../ai_model_converter/maixcam2.html). In the end, you will get a `mud` file and a `cvimodel` file. The content of the `mud` file is as follows: MaixCAM/MaixCAM Pro: ```ini [basic] type cvimodel model yolov5s_320x224_int8.cvimodel [extra] model_type yolov5 input_type rgb mean 0, 0, 0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 anchors 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush ``` MaixCAM2: ```ini [basic] type axmodel model_npu yolov5s_640x480_npu.axmodel model_vnpu yolov5s_640x480_vnpu.axmodel [extra] model_type yolov5 type detector input_type rgb input_cache true output_cache true input_cache_flush false output_cache_inval true anchors 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush mean 0,0,0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 ``` Replace the parameters according to the content of your training. For example, if you train to detect digits `0 9`, then just replace `labels 0,1,2,3,4,5,6,7,8,9`, and then place the two files in the same directory and load the `mud` file when running the model. ## Upload share on MaixHub Share your model on [MaixHub model zoo](https://maixhub.com/model/zoo?platform maixcam) 上传并分享你的模型，可以多提供几个分辨率供大家选择。"},"/maixpy/doc/en/vision/detect_obb.html":{"title":"","content":" tite: Target Detection with Rotation Angles (OBB, Oriented Bounding Box) update: date: 2024 12 20 version: v1.0 author: neucrack content: Support YOLO11/YOLOv8 OBB model and add documentation ## Introduction In standard object detection, the output is typically a rectangular bounding box. However, in certain scenarios, objects may have a rotated shape, requiring a bounding box with a rotation angle. This type of bounding box is referred to as an OBB (Oriented Bounding Box). ![](../../assets/ships detection using obb.jpeg) **Standard detection results:** `x, y, w, h` represent the top left or center coordinates of the rectangle, along with its width and height. **OBB detection results:** `x, y, w, h, angle` includes an additional parameter for the rotation angle. ## Using Rotated Bounding Boxes (OBB) in MaixPy MaixCAM `MaixPy` supports the `YOLO11/YOLOv8` OBB model, enabling quick and convenient implementation. Below is an example based on the `nn_yolo11_obb.py` script from [MaixPy/examples](https://github.com/sipeed/maixpy): ```python from maix import camera, display, image, nn, app detector nn.YOLO11(model \"/root/models/yolo11n_obb.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: points obj.get_obb_points() msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}, {obj.angle * 180:.1f}' img.draw_string(points[0], points[1] 4, msg, color image.COLOR_RED) detector.draw_pose(img, points, 8 if detector.input_width() > 480 else 4, image.COLOR_RED, close True) disp.show(img) ``` In this example, the `YOLO11` model is used to load an OBB model. After detecting an object, the rotation angle can be accessed via `obj.angle`. The `x, y, w, h` attributes of `obj` represent the unrotated rectangle. The `get_obb_points` method retrieves the four vertices of the rotated rectangle, and `draw_pose` is used to draw the rotated bounding box. The `close` parameter ensures the vertices are connected. The default model is the official YOLO11 15 class model with the following labels: ```python plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, swimming pool ``` The model file can be found at `/root/models/yolo11n_obb.mud`. ## More Input Resolutions The default input image resolution is `320x224`. For higher resolutions, download from the [MaixHub Model Zoo](https://maixhub.com/model/zoo/869) or customize your own model as described below. ## Customizing Your Own OBB Model for MaixPy MaixCAM ### Using the Model on a Computer For an introduction to the official `YOLO11` OBB models, refer to [YOLO11 OBB](https://docs.ultralytics.com/tasks/obb/). This documentation explains how to use OBB models on a computer and export ONNX model files. ### Exporting the Model for MaixCAM Follow the [YOLO11/YOLOv8 Custom Models](./customize_model_yolov8.html) guide to convert an ONNX model into a MUD model compatible with MaixCAM. **Note:** Ensure the output names of the model's layers are correctly configured during conversion. ### Training Your Own OBB Model Refer to the [YOLO11 Official Training Documentation](https://docs.ultralytics.com/datasets/obb/dota v2/) to prepare your dataset and train your own OBB model."},"/maixpy/doc/en/vision/yolov5.html":{"title":"MaixPy MaixCAM Using YOLOv5 / YOLOv8 / YOLO11 for Object Detection","content":" title: MaixPy MaixCAM Using YOLOv5 / YOLOv8 / YOLO11 for Object Detection ## Object Detection Concept Object detection refers to detecting the position and category of objects in images or videos, such as identifying apples or airplanes in a picture and marking their locations. Unlike classification, object detection includes positional information. Therefore, the result of object detection is generally a rectangular box that marks the location of the object. ## Object Detection in MaixPy MaixPy provides `YOLOv5`, `YOLOv8`, and `YOLO11` models by default, which can be used directly: > YOLOv8 requires MaixPy > 4.3.0. > YOLO11 requires MaixPy > 4.7.0. ```python from maix import camera, display, image, nn, app detector nn.YOLOv5(model \"/root/models/yolov5s.mud\", dual_buff True) # detector nn.YOLOv8(model \"/root/models/yolov8n.mud\", dual_buff True) # detector nn.YOLO11(model \"/root/models/yolo11n.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` Example video: <div> <video playsinline controls autoplay loop muted preload src \"/static/video/detector.mp4\" type \"video/mp4\"> </div> Here, the camera captures an image, passes it to the `detector` for detection, and then displays the results (classification name and location) on the screen. You can switch between `YOLO11`, `YOLOv5`, and `YOLOv8` simply by replacing the corresponding line and modifying the model file path. For the list of 80 objects supported by the model, see the appendix of this document. For more API usage, refer to the documentation for the [maix.nn](/api/maix/nn.html) module. ## dual_buff for Double Buffering Acceleration You may notice that the model initialization uses `dual_buff` (default value is `True`). Enabling the `dual_buff` parameter can improve efficiency and increase the frame rate. For more details and usage considerations, see the [dual_buff Introduction](./dual_buff.html). ## More Input Resolutions The default model input resolution is `320x224`, which closely matches the aspect ratio of the default screen. You can also download other model resolutions: YOLOv5: [https://maixhub.com/model/zoo/365](https://maixhub.com/model/zoo/365) YOLOv8: [https://maixhub.com/model/zoo/400](https://maixhub.com/model/zoo/400) YOLO11: [https://maixhub.com/model/zoo/453](https://maixhub.com/model/zoo/453) Higher resolutions provide more accuracy, but take longer to process. Choose the appropriate resolution based on your application. ## Which Model to Use: YOLOv5, YOLOv8, or YOLO11? We provide three models: `YOLOv5s`, `YOLOv8n`, and `YOLO11n`. The `YOLOv5s` model is larger, while `YOLOv8n` and `YOLO11n` are slightly faster. According to official data, the accuracy is `YOLO11n > YOLOv8n > YOLOv5s`. You can test them to decide which works best for your situation. Additionally, you may try `YOLOv8s` or `YOLO11s`, which will have a lower frame rate (e.g., `yolov8s_320x224` is 10ms slower than `yolov8n_320x224`), but offer higher accuracy. You can download these models from the model library mentioned above or export them yourself from the official `YOLO` repository. ## Different Resolutions for Camera and Model If the resolution of `img` is different from the model's resolution when using the `detector.detect(img)` function, the function will automatically call `img.resize` to adjust the image to the model's input resolution. The default `resize` method is `image.Fit.FIT_CONTAIN`, which scales while maintaining the aspect ratio and fills the surrounding areas with black. The detected coordinates will also be automatically mapped back to the original `img`. ## Training Your Own Object Detection Model on MaixHub If you need to detect specific objects beyond the 80 categories provided, visit [MaixHub](https://maixhub.com) to learn and train an object detection model. Select \"Object Detection Model\" when creating a project. Refer to the [MaixHub Online Training Documentation](./maixhub_train.html). Alternatively, you can find models shared by community members at the [MaixHub Model Library](https://maixhub.com/model/zoo?platform maixcam). ## Training Your Own Object Detection Model Offline We strongly recommend starting with MaixHub for online training, as the offline method is much more difficult and is not suitable for beginners. Some knowledge may not be explicitly covered here, so be prepared to do further research. Refer to [Training a Custom YOLOv5 Model](./customize_model_yolov5.html) or [Training a Custom YOLOv8/YOLO11 Model Offline](./customize_model_yolov8.html). ## Appendix: 80 Classes The 80 objects in the COCO dataset are: ```txt person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair dryer toothbrush ```"},"/maixpy/doc/en/vision/find_barcodes.html":{"title":"MaixCAM MaixPy Barcode Recognition","content":" title: MaixCAM MaixPy Barcode Recognition update: date: 2024 12 16 author: lxowalle version: 1.0.0 content: Initial documentation Before reading this article, make sure you know how to develop with MaixCAM. For details, please read [Quick Start](../index.html). ## Introduction This article explains how to use MaixPy for Barcode recognition. ## Using MaixPy to Recognize Barodes MaixPy's `maix.image.Image` includes the `find_barcodes` method for Barcode recognition. ### How to Recognize Barcodes A simple example that recognizes Barcodes and draws a bounding box: ```python from maix import image, camera, display cam camera.Camera(480, 320) disp display.Display() while 1: img cam.read() barcodes img.find_barcodes() for b in barcodes: rect b.rect() img.draw_rect(rect[0], rect[1], rect[2], rect[3], image.COLOR_BLUE, 2) img.draw_string(0, 0, \"payload: \" + b.payload(), image.COLOR_GREEN) disp.show(img) ``` Steps: 1. Import the image, camera, and display modules: ```python from maix import image, camera, display ``` 2. Initialize the camera and display: ```python cam camera.Camera(480, 320) # Initialize the camera with a resolution of 480x320 in RGB format disp display.Display() ``` 3. Capture and display images from the camera: ```python while 1: img cam.read() disp.show(img) ``` 4. Use the `find_barcodes` method to detect barcodes in the camera image: ```python barcodes img.find_barcodes() ``` `img` is the camera image captured by `cam.read()`. When initialized as `cam camera.Camera(480, 320)`, the `img` object is a 480x320 resolution RGB image. `img.find_barcodes` searches for barcodes and saves the results in `barcodes` for further processing. Note: The spacing of barcodes is small, and the width of barcodes is generally much larger than the height, so when adjusting the recognition rate and speed, you can try to make the width of the target image larger and the height smaller. 5. Process and display the results of barcode recognition on the screen: ```python for b in barcodes: rect b.rect() img.draw_rect(rect[0], rect[1], rect[2], rect[3], image.COLOR_BLUE, 2) img.draw_string(0, 0, \"payload: \" + b.payload(), image.COLOR_GREEN) ``` `barcodes` is the result of querying barcodes by `img.find_barcodes()`, if no barcode is found then `barcodes` is empty. `b.rect()` is used to get the position and size of the scanned barcode, `img.draw_rect()` uses the position information to draw the shape of the barcode. `img.draw_string` is used to display the content and position of the barcode, `b.payload()` is used to get the content of the barcode. ### 常用参数说明 List common parameters and their explanations. If you cannot find parameters that fit your application, consider whether to use a different algorithm or extend the functionality based on the current algorithm's results. Parameter Description Example roi Sets the rectangular area for the algorithm to compute, where roi [x, y, w, h], x and y denote the top left coordinates of the rectangle, and w and h denote the width and height of the rectangle, defaulting to the entire image. Compute the area with coordinates (50,50) and width and height of 100:<br />`img.find_barcodes(roi [50, 50, 100, 100])` This article introduces common methods. For more API details, refer to the [image](../../../api/maix/image.html) section of the API documentation."},"/maixpy/doc/en/vision/opencv.html":{"title":"MaixCAM MaixPy Use OpenCV","content":" title: MaixCAM MaixPy Use OpenCV ## Introduction For MaixCAM, since it uses Linux and the performance can support using the Python version of OpenCV, you can use the `cv2` module directly in addition to the `maix` module. The examples in this article and more can be found in [MaixPy/examples/vision/opencv](https://github.com/sipeed/MaixPy/tree/main/examples/vision/opencv). **Note that OpenCV functions are basically CPU calculated. If you can use maix modules, try not to use OpenCV, because many maix functions are hardware accelerated.** ## Converting between Numpy/OpenCV and maix.image.Image Formats You can convert `maix.image.Image` object to a `numpy` array, which can then be used by libraries such as `numpy` and `opencv`: ```python from maix import image, time, display, app disp display.Display() while not app.need_exit(): img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_rect(0, 0, 100, 100, image.COLOR_RED, thickness 1) t time.ticks_ms() img_bgr image.image2cv(img, ensure_bgr True, copy True) img2 image.cv2image(img_bgr, bgr True, copy True) print(\"time:\", time.ticks_ms() t) print(type(img_bgr), img_bgr.shape) print(type(img2), img2) print(\"\") disp.show(img2) ``` The previous program is slower because each conversion involves a memory copy. Below is an optimized version for better performance. However, it is not recommended to use this unless you are aiming for extreme speed, as it is prone to errors: ```python from maix import image, time, display, app disp display.Display() while not app.need_exit(): img image.Image(320, 240, image.Format.FMT_RGB888) img.draw_rect(0, 0, 100, 100, image.COLOR_RED, thickness 1) t time.ticks_ms() img_rgb image.image2cv(img, ensure_bgr False, copy False) img2 image.cv2image(img_rgb, bgr False, copy False) print(\"time:\", time.ticks_ms() t) print(type(img_rgb), img_rgb.shape) print(type(img2), img2) disp.show(img2) ``` * In `img_rgb image.image2cv(img, ensure_bgr False, copy False)`, `img_rgb` directly uses the data from `img` without creating a memory copy. Note that the obtained `img_rgb` is an `RGB` image. Since OpenCV APIs assume the image is `BGR`, you need to be careful when using OpenCV APIs to process the image. If you are not sure, set `ensure_bgr` to `True`. * In `img2 image.cv2image(img_rgb, bgr False, copy False)`, setting `copy` to `False` means `img2` directly uses the memory of `img_rgb` without creating a new memory copy, resulting in faster performance. However, be cautious because `img_rgb` must not be destroyed before `img2` finishes using it; otherwise, the program will crash. * Note that since memory is borrowed, modifying the converted image will also affect the original image. ## Load an Image ```python import cv2 file_path \"/maixapp/share/icon/detector.png\" img cv2.imread(file_path) print(img) ``` Since the `cv2` module is quite large, `import cv2` may take some time. ## Display Image on Screen To display an image on the screen, convert it to a `maix.image.Image` object and then use `display` to show it: ```python from maix import display, image, time import cv2 disp display.Display() file_path \"/maixapp/share/icon/detector.png\" img cv2.imread(file_path) img_show image.cv2image(img) disp.show(img_show) while not app.need_exit(): time.sleep(1) ``` ## Use OpenCV Functions For example, edge detection: Based on the code above, use the `cv2.Canny` function: ```python from maix import image, display, app, time import cv2 file_path \"/maixapp/share/icon/detector.png\" img0 cv2.imread(file_path) disp display.Display() while not app.need_exit(): img img0.copy() # canny method t time.ticks_ms() edged cv2.Canny(img, 180, 60) t2 time.ticks_ms() t # show by maix.display t time.ticks_ms() img_show image.cv2image(edged) print(f\"edge time: {t2}ms, convert time: {time.ticks_ms() t}ms\") disp.show(img_show) ``` ## Use Camera On a PC, we use OpenCV's `VideoCapture` class to read from the camera. For MaixCAM, OpenCV does not support this directly, so we use the `maix.camera` module to read from the camera and then use it with OpenCV. Convert a `maix.image.Image` object to a `numpy.ndarray` object using the `image.image2cv` function: ```python from maix import image, display, app, time, camera import cv2 disp display.Display() cam camera.Camera(320, 240, image.Format.FMT_BGR888) while not app.need_exit(): img cam.read() # convert maix.image.Image object to numpy.ndarray object t time.ticks_ms() img image.image2cv(img, ensure_bgr False, copy False) print(\"time: \", time.ticks_ms() t) # canny method edged cv2.Canny(img, 180, 60) # show by maix.display img_show image.cv2image(edged, bgr True, copy False) disp.show(img_show) ``` ## Read USB camera First, in the development board settings, select `USB Mode` under `USB Settings` and set it to `HOST` mode. If there is no screen available, you can use the `examples/tools/maixcam_switch_usb_mode.py` script to set it. ```python from maix import image, display, app import cv2 import sys cap cv2.VideoCapture(0) cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640) cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # cap.set(cv2.CAP_PROP_CONVERT_RGB, 0) disp display.Display() if not cap.isOpened(): print(\"无法打开摄像头\") sys.exit(1) print(\"开始读取\") while not app.need_exit(): ret, frame cap.read() if not ret: print(\"无法读取帧\") break img image.cv2image(frame, bgr True, copy False) disp.show(img) ```"},"/maixpy/doc/en/vision/custmize_model.html":{"title":"","content":"Please refer to [MaixCAM Model Conversion](../ai_model_converter/maixcam.html), and find the model documentation you need to convert in the left directory, such as [Custom YOLOv5 Model](./customize_model_yolov5.html)."},"/maixpy/doc/en/vision/face_landmarks.html":{"title":"MaixCAM MaixPy Face 478 Keypoints Detection","content":" title: MaixCAM MaixPy Face 478 Keypoints Detection update: date: 2025 01 08 version: v1.0 author: neucrack content: Add face 478 landmarks detection source code, documentation and demo. ## Introduction In the previous article [Face Detection](./face_detection.html), we introduced how to detect faces and a few keypoints (e.g., 5 keypoints). This article explains how to detect more keypoints. More keypoints have a wide range of applications, such as expression detection, facial feature recognition, face swapping, and more. ![face_landmarks](../../assets/face_landmarks.jpg) <a href \"../../assets/maixcam_face_landmarks_full.jpg\" target \"_blank\">full size 478 landmarks image</a> ## Using Face Keypoints Detection in MaixPy MaixPy integrates MediaPipe's **478 face keypoints** detection, with the results as shown below: ![maixcam_face_landmarks](../../assets/maixcam_face_landmarks_1.jpg) Video demonstration: <video playsinline controls autoplay loop muted preload src \"/static/video/maixcam_face_landmarks.mp4\" type \"video/mp4\"> Classifier Result video </video> ### Code Usage To use this feature, MaixPy version must be > 4.10.0. Refer to the latest code in [MaixPy/examples](https://github.com/sipeed/MaixPy): ```python from maix import camera, display, image, nn, app detect_conf_th 0.5 detect_iou_th 0.45 landmarks_conf_th 0.5 landmarks_abs True landmarks_rel False max_face_num 2 detector nn.YOLOv8(model \"/root/models/yolov8n_face.mud\", dual_buff False) landmarks_detector nn.FaceLandmarks(model \"/root/models/face_landmarks.mud\") cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() results [] objs detector.detect(img, conf_th detect_conf_th, iou_th detect_iou_th, sort 1) count 0 for obj in objs: img_std landmarks_detector.crop_image(img, obj.x, obj.y, obj.w, obj.h, obj.points) if img_std: res landmarks_detector.detect(img_std, landmarks_conf_th, landmarks_abs, landmarks_rel) if res and res.valid: results.append(res) count + 1 if count > max_face_num: break for res in results: landmarks_detector.draw_face(img, res.points, landmarks_detector.landmarks_num, res.points_z) disp.show(img) ``` ### Explanation of Key Points `max_face_num`: Limits the maximum number of faces detected to prevent lag due to too many faces in the frame. `landmarks_abs`: Specifies the coordinates of face keypoints in the original `img`. The `points` variable contains 478 keypoints in the order `x0, y0, x1, y1, ..., x477, y477`. `landmarks_rel`: Outputs coordinates in `img_std` and appends the results to the `points` variable. `points_z`: Represents depth estimation of the keypoints relative to the face's center of gravity. The closer to the camera, the larger the value. If behind the face's center, the value is negative. The values are proportional to the face's width. ## Extracting Partial Keypoints The 478 keypoints may be excessive for some applications. If you only need specific ones, you can select them based on the <a href \"../../assets/maixcam_face_landmarks_full.jpg\" target \"_blank\">high resolution image</a> index. Common subsets include: **Note: These are for reference only. Please rely on the actual model output for accuracy.** * **146 Keypoints:** ```python sub_146_idxes [0, 1, 4, 5, 6, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58, 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105, 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 168, 172, 173, 176, 178, 181, 185, 191, 195, 197, 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291, 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324, 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409, 415, 454, 466, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477] ``` * **68 Keypoints:** ```python sub_68_idxes [162, 234, 93, 58, 172, 136, 149, 148, 152, 377, 378, 365, 397, 288, 323, 454, 389, 71, 63, 105, 66, 107, 336, 296, 334, 293, 301, 168, 197, 5, 4, 75, 97, 2, 326, 305, 33, 160, 158, 133, 153, 144, 362, 385, 387, 263, 373, 380, 61, 39, 37, 0, 267, 269, 291, 405, 314, 17, 84, 181, 78, 82, 13, 312, 308, 317, 14, 87] ``` * **5 Keypoints:** ```python sub_5_idxes [468, 473, 4, 61, 291] ``` With these indices, you can use the following code to extract and display specific subsets of keypoints: ```python def get_sub_landmarks(points, points_z, idxes): new_points [] new_points_z [] for i in idxes: new_points.append(points[i * 2]) new_points.append(points[i * 2 + 1]) new_points_z.append(points_z[i]) return new_points, new_points_z sub_xy, sub_z get_sub_landmarks(res.points, res.points_z, sub_146_idxes) landmarks_detector.draw_face(img, sub_xy, len(sub_z), sub_z) ```"},"/maixpy/doc/en/vision/yolo_world.html":{"title":"Using YOLO World Model on MaixPy MaixCAM2 for Detection of Any Target Without Training","content":" title: Using YOLO World Model on MaixPy MaixCAM2 for Detection of Any Target Without Training ## YOLO World Hardware Platform Support Hardware Platform Supported MaixCAM No MaixCAM2 Yes ## Overview of YOLO Object Detection YOLO is a well known object detection model suitable for edge device deployment, capable of quickly detecting pre trained targets. For example, if we want to detect the position of `apple` in an image, we need to collect image data of `apple`, train a model, and then export the model to perform detection in MaixPy. In other words, for each new target to be detected, we need to retrain a model. This process is cumbersome, time consuming, and most importantly, training cannot be done on the edge device itself; it must be done on a computer or server with a GPU. ## YOLO World Concept YOLO World, however, eliminates the need for additional training. You simply need to tell the model what target you want to detect, and YOLO World will be able to detect it. Sounds magical, doesn't it? YOLO World is a new concept, enabling detection of any target. YOLO World achieves this functionality through the use of `Prompt`. The model has incorporated language model capabilities, allowing you to specify the target you want to detect through text descriptions. For example, if we want to detect `apple`, we simply input `apple` along with the image to be detected, and the model can detect the coordinates of `apple` in the image. I won’t go into more technical details here, but for those interested, please refer to the [YOLO World official repository](https://github.com/AILab CVC/YOLO World). ## Using YOLO World in MaixPy ### YOLO World Implementation in MaixPy MaixPy has ported the official YOLO World model (YOLOv8/YOLO11) from [ultralytics](https://github.com/ultralytics/ultralytics/blob/main/docs/en/models/yolo world.md) using the project [ONNX YOLO World Open Vocabulary Object Detection](https://github.com/AXERA TECH/ONNX YOLO World Open Vocabulary Object Detection). If you want to try it on a PC (**Note: this is not running on MaixCAM**), you can follow this documentation. For instance, you can: First, specify the target categories you want to detect and save a model that can only detect those specific targets: ```python from ultralytics import YOLO model YOLO(\"yolov8s world.pt\") # or select yolov8m/l world.pt model.set_classes([\"person\", \"bus\"]) model.save(\"custom_yolov8s.pt\") ``` Then use the new model to perform detection: ```python from ultralytics import YOLO model YOLO(\"custom_yolov8s.pt\") results model.predict(\"path/to/image.jpg\") results[0].show() ``` ### Principle Overview As seen above, we first specify the target categories for detection and obtain a new model, which is then used for detection. The principle is as follows: Due to the performance and power consumption limitations of edge computing hardware, the YOLO World model (the previously mentioned `yolov8s world.pt`) is divided into two parts: * **Language Model (text_feature model)**: This model encodes the language input (`Prompt`) into a text feature vector (which can be simply understood as an array) and stores it in a `.bin` file. This model is large and runs slowly. * **Detection Model (yolo model)**: This model takes both the image and the feature vector (`.bin`) as input for object detection and outputs the coordinates and class names of the detected objects. This model is smaller and runs faster. The reason for splitting into two parts is that during actual detection, we only need to run the detection model, avoiding the need to run the language model every time, thus saving runtime and power consumption. For example, if we want to detect `apple`, we only need to run the language model once (which occupies more resources) to encode `apple` into a feature vector (a very small file). Then, for each subsequent detection, we only need to run the detection model, combining the image and the feature vector for detection. When we need to detect a new target, we can run the language model again to generate a new feature vector file. Therefore, in the previous example, the model file `yolov8s world.pt` used on the computer contains both the language model and the detection model. To make it more convenient for edge devices, we split it into two separate model files: * `yolo world_4_class.mud`: Detection model, fast and small. * `yolo world_text_feature_4_class.mud`: Language model, slow and large. ### Running the Language Model to Specify the Target for Detection Run the language model once, specify the target to be detected, and generate the feature vector file: ```python import os labels [\"apple\", \"banana\", \"orange\", \"grape\"] out_dir \"/root/models\" name \"yolo world_4_class_my_feature\" feature_file os.path.join(out_dir, f\"{name}.bin\") labels_file os.path.join(out_dir, f\"{name}.txt\") with open(labels_file, \"w\") as f: for label in labels: f.write(f\"{label}\\n\") cmd f\"python u m yolo_world_utils gen_text_feature labels_path {labels_file} out_feature_path {feature_file}\" print(f\"Now run\\n\\t`{cmd}`\\nto generate text feature of\\n{labels}\") print(\"\\nplease wait a moment, it may take a few seconds ...\\n\") ret os.system(cmd) if ret ! 0: print(\"[ERROR] execute have error, please see log\") else: print(f\"saved\\n\\tlabels to:\\n\\t{labels_file}\\n and text feature to:\\n\\t{feature_file}\") print(f\"please use yolo world_{len(labels)}_class.mud model to run detect\") ``` Here, we specify four categories to detect by setting `labels`. Two necessary files (`*.bin` and `*.txt`) will be generated. **Note**: The `labels` format requirements: * Each category name must be in **English**. It cannot be in Chinese or any other language because the language model only supports English. * Multiple words are allowed, but the length should not be too long. After BPE encoding, the length should not exceed 75 tokens. If it’s too long, an error will occur, so try it and you'll know. ### Running the Detection Model We have generated the feature vector, and now we have three files: 1. Text feature vector `yolo world_4_class_my_feature.bin`. 2. Label text file `yolo world_4_class_my_feature.txt`. 3. One category detection model `yolo world_4_class.mud`, located in the `/root/models` directory. > Note that this detection model is for `4_class`, meaning it can only detect four categories. The `.txt` file must contain exactly four categories, and they must correspond one by one. For instance, for a one category model, you would use `yolo world_1_class.mud`, and the `.txt` would contain only one category. The system has built in detection models for 1/4 categories in the `/root/models` directory, namely `yolo world_1_class.mud` and `yolo world_4_class.mud`. If you need to detect a different number of categories, download the corresponding model files as outlined in the section **Download More Detection Models with Different Categories**. Now we can directly use the YOLO World model for real time object detection. The code is similar to YOLOv8 and YOLO11: ```python detector nn.YOLOWorld(\"/root/models/yolo world_4_class.mud\", \"/root/models/yolo world_4_class_my_feature.bin\", \"/root/models/yolo world_4_class_my_feature.txt\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` ## Download More Detection Models with Different Categories There are two ways: ### Download from MaixHub model zoo Download from [MaixHub model zoo](), the supported class num please refer to MaixHub model zoo's description documentation. ### Generate by yourself If you need models for other categories, you need to generate them yourself. > If you don't have the environment or ability to do this, you can request help from other capable group members in QQ group 86234035 or [telegram](https://t.me/maixpy) for paid assistance. A Docker image is provided, and you can pull the image to run locally to generate models for any number of categories. * First, ensure you can successfully pull the image from Docker Hub, preferably with a proxy. Refer to [docker proxy setup](https://neucrack.com/p/286). * You can test with `docker pull hello world`. * `docker pull sipeed/yolo world generator maixcam2` > The `sipeed/sipeed/yolo world generator maixcam2` image relies on the `sipeed/pulsar2` image, which will be automatically downloaded. Alternatively, you can manually download the image from [Pulsar2 official site](https://pulsar2 docs.readthedocs.io/zh cn/latest/user_guides_quick/quick_start_prepare.html), then load and rename it: > * `docker load i *.tar.gz` > * `docker tag pulsar2:3.3 sipeed/pulsar2:latest`, change `3.3` based on the actual version. * `docker run it rm v ${PWD}/out:/root/out sipeed/yolo world generator maixcam2 /bin/bash` Here ` v ${PWD}/out:/root/out` maps the current directory's `out` folder to the container, so files generated inside the container can be viewed in the current directory (use multiple ` v src:dst` arguments for additional directories). Now you can enter the Docker container and run the conversion command: ```shell cd /root ./gen_model.sh 1 640 480 ``` Here, the three parameters are: * class_num: The number of categories. * width: Input resolution width. * height: Input resolution height. * After completion, the model file will be available in the `out` directory, compressed. Simply extract and use it. Model generation takes time, so be patient."},"/maixpy/doc/en/vision/customize_model_yolov8.html":{"title":"Offline Training for YOLO11/YOLOv8 Models on MaixCAM MaixPy to Customize Object and Keypoint Detection","content":" title: Offline Training for YOLO11/YOLOv8 Models on MaixCAM MaixPy to Customize Object and Keypoint Detection update: date: 2024 06 21 version: v1.0 author: neucrack content: Document creation date: 2024 10 10 version: v2.0 author: neucrack content: Added YOLO11 support date: 2025 07 01 version: v3.0 author: neucrack content: Add MaixCAM2 support ## Introduction The default official model provides detection for 80 different objects. If this doesn't meet your needs, you can train your own model to detect custom objects, which can be done on your own computer or server by setting up a training environment. YOLOv8 / YOLO11 not only supports object detection but also supports keypoint detection with YOLOv8 pose / YOLO11 pose. Apart from the official human keypoints, you can also create your own keypoint dataset to train models for detecting specific objects and keypoints. Since YOLOv8 and YOLO11 mainly modify the internal network while the preprocessing and post processing remain the same, the training and conversion steps for YOLOv8 and YOLO11 are identical, except for the output node names. **Note:** This article explains how to train a custom model but assumes some basic knowledge. If you do not have this background, please learn it independently: * This article will not cover how to set up the training environment; please search for how to install and test a PyTorch environment. * This article will not cover basic machine learning concepts or Linux related knowledge. If you think there are parts of this article that need improvement, please click on `Edit this article` at the top right and submit a PR to contribute to the documentation. ## Process and Article Goal To ensure our model can be used on MaixPy (MaixCAM), it must go through the following steps: * Set up the training environment (not covered in this article, please search for how to set up a PyTorch training environment). * Clone the [YOLO11/YOLOv8](https://github.com/ultralytics/ultralytics) source code locally. * Prepare the dataset and format it according to the YOLO11 / YOLOv8 project requirements. * Train the model to obtain an `onnx` model file, which is the final output of this article. * Convert the `onnx` model into a `MUD` file supported by MaixPy, as described in the [MaixCAM Model Conversion](../ai_model_converter/maixcam.html) article. * Use MaixPy to load and run the model. ## Where to Find Datasets for Training Please refer to [Where to find datasets](../pro/datasets.html) ## Reference Articles Since this process is quite general, this article only provides an overview. For specific details, please refer to the **[YOLO11 / YOLOv8 official code and documentation](https://github.com/ultralytics/ultralytics)** (**recommended**) and search for training tutorials to eventually export an ONNX file. If you come across good articles, feel free to edit this one and submit a PR. ## Exporting YOLO11 / YOLOv8 ONNX Models Create an `export_onnx.py` file in the `ultralytics` directory: ```python from ultralytics import YOLO import sys print(sys.path) net_name sys.argv[1] # yolov8n.pt yolov8n pose.pt # https://docs.ultralytics.com/models/yolov8/#supported tasks and modes input_width int(sys.argv[2]) input_height int(sys.argv[3]) # Load a model model YOLO(net_name) # load an official model # model YOLO(\"path/to/best.pt\") # load a custom model # Predict with the model results model(\"https://ultralytics.com/images/bus.jpg\") # predict on an image path model.export(format \"onnx\", imgsz [input_height, input_width]) # export the model to ONNX format print(path) ``` Then run `python export_onnx.py yolov8n.pt 320 224` to export the `onnx` model. Here, the input resolution is redefined; the model was originally trained with `640x640`, but we specify a different resolution to improve runtime speed. The reason for using `320x224` is that it closely matches the MaixCAM screen aspect ratio, making display easier. For MaixCAM2, you can use `640x480` or `320x240` — feel free to set it according to your specific needs. ## Convert to MaixCAM Supported Models and mud Files MaixPy/MaixCDK currently supports YOLOv8 / YOLO11 detection, YOLOv8 pose / YOLO11 pose keypoint detection, and YOLOv8 seg / YOLO11 seg segmentation models (2024.10.10). Convert the models according to [MaixCAM Model Conversion](../ai_model_converter/maixcam.html) and [MaixCAM2 Model Conversion](../ai_model_converter/maixcam2.html). ### Select output nodes Note the selection of model output nodes (**Note that your model values may not be exactly the same, just find the corresponding nodes according to the pictures below**): For YOLO11 / YOLOv8, MaixPy support two types node select method, choose proper method according to your device: Model & Feature Method A Method B Supported Devices **MaixCAM2**(Recommend)<br>MaixCAM(running speed lowe than method B) **MaixCAM**(Recommend) Feature More computation on CPU (safer quantization, slightly slower than Solution 2) More computation on NPU (included in quantization) Attention None Quantization failed in actual tests on MaixCAM2. Detect YOLOv8 `/model.22/Concat_1_output_0`<br>`/model.22/Concat_2_output_0`<br>`/model.22/Concat_3_output_0` `/model.22/dfl/conv/Conv_output_0`<br>`/model.22/Sigmoid_output_0` Detect YOLO11 `/model.23/Concat_output_0`<br>`/model.23/Concat_1_output_0`<br>`/model.23/Concat_2_output_0` `/model.23/dfl/conv/Conv_output_0`<br>`/model.23/Sigmoid_output_0` Keypoint YOLOv8 pose `/model.22/Concat_1_output_0`<br>`/model.22/Concat_2_output_0`<br>`/model.22/Concat_3_output_0`<br>`/model.22/Concat_output_0` `/model.22/dfl/conv/Conv_output_0`<br>`/model.22/Sigmoid_output_0`<br>`/model.22/Concat_output_0` Keypoint YOLO11 pose `/model.23/Concat_1_output_0`<br>`/model.23/Concat_2_output_0`<br>`/model.23/Concat_3_output_0`<br>`/model.23/Concat_output_0` `/model.23/dfl/conv/Conv_output_0`<br>`/model.23/Sigmoid_output_0`<br>`/model.23/Concat_output_0` Segment YOLOv8 seg`/model.22/Concat_1_output_0`<br>`/model.22/Concat_2_output_0`<br>`/model.22/Concat_3_output_0`<br>`/model.22/Concat_output_0`<br>`output1` `/model.22/dfl/conv/Conv_output_0`<br>`/model.22/Sigmoid_output_0`<br>`/model.22/Concat_output_0`<br>`output1` Segment YOLO11 seg `/model.23/Concat_1_output_0`<br>`/model.23/Concat_2_output_0`<br>`/model.23/Concat_3_output_0`<br>`/model.23/Concat_output_0`<br>`output1``/model.23/dfl/conv/Conv_output_0`<br>`/model.23/Sigmoid_output_0`<br>`/model.23/Concat_output_0`<br>`output1` OBB YOLOv8 obb `/model.22/Concat_1_output_0`<br>`/model.22/Concat_2_output_0`<br>`/model.22/Concat_3_output_0`<br>`/model.22/Concat_output_0``/model.22/dfl/conv/Conv_output_0`<br>`/model.22/Sigmoid_1_output_0`<br>`/model.22/Sigmoid_output_0` OBB YOLO11 obb `/model.23/Concat_1_output_0`<br>`/model.23/Concat_2_output_0`<br>`/model.23/Concat_3_output_0`<br>`/model.23/Concat_output_0``/model.23/dfl/conv/Conv_output_0`<br>`/model.23/Sigmoid_1_output_0`<br>`/model.23/Sigmoid_output_0` YOLOv8/YOLO11 Detect output nodes ![](../../assets/yolo11_detect_nodes.png) ![](../../assets/yolov8_out.jpg) YOLOv8/YOLO11 pose extra nodes ![](../../assets/yolo11_pose_node.png) pose branch in the figure above YOLOv8/YOLO11 seg extra nodes ![](../../assets/yolo11_seg_node.png) ![](../../assets/yolo11_seg_node.png) YOLOv8/YOLO11 OBB extra nodes ![](../../assets/yolo11_obb_node.png) ![](../../assets/yolo11_out_obb.jpg) ### Edit mud file For object detection, the MUD file would be as follows (replace `yolo11` for YOLO11): MaixCAM/MaixCAM Pro: ```ini [basic] type cvimodel model yolov8n.cvimodel [extra] model_type yolov8 input_type rgb mean 0, 0, 0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush ``` MaixCAM2: ```ini [basic] type axmodel model_npu yolo11n_640x480_npu.axmodel model_vnpu yolo11n_640x480_vnpu.axmodel [extra] model_type yolo11 type detector input_type rgb labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush input_cache true output_cache true input_cache_flush false output_cache_inval true mean 0,0,0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 ``` Replace the `labels` according to your trained objects. For keypoint detection (yolov8 pose), modify `type pose`. For keypoint detection (yolov8 seg), modify `type seg`. For keypoint detection (yolov8 obb), modify `type obb`. ## Upload and Share on MaixHub Visit the [MaixHub Model Library](https://maixhub.com/model/zoo?platform maixcam) to upload and share your model. Consider providing multiple resolutions for others to choose from."},"/maixpy/doc/en/vision/apriltag.html":{"title":"MaixCAM MaixPy Apriltag Recognition","content":" title: MaixCAM MaixPy Apriltag Recognition update: date: 2024 04 03 author: lxowalle version: 1.0.0 content: Initial documentation Before reading this article, make sure you are familiar with how to develop with MaixCAM. For more details, please read [Quick Start](../index.html). ## Introduction This article introduces how to use MaixPy to recognize Apriltag labels. ## Using MaixPy to Recognize Apriltag Labels MaixPy's `maix.image.Image` provides the `find_apriltags` method, which can be used to recognize Apriltag labels. ### How to Recognize Apriltag Labels A simple example of recognizing Apriltag labels and drawing bounding boxes: ```python from maix import image, camera, display cam camera.Camera() disp display.Display() families image.ApriltagFamilies.TAG36H11 x_scale cam.width() / 160 y_scale cam.height() / 120 while 1: img cam.read() new_img img.resize(160, 120) apriltags new_img.find_apriltags(families families) for a in apriltags: corners a.corners() for i in range(4): corners[i][0] int(corners[i][0] * x_scale) corners[i][1] int(corners[i][1] * y_scale) x int(a.x() * x_scale) y int(a.y() * y_scale) w int(a.w() * x_scale) h int(a.h() * y_scale) for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_RED) img.draw_string(x + w, y, \"id: \" + str(a.id()), image.COLOR_RED) img.draw_string(x + w, y + 15, \"family: \" + str(a.family()), image.COLOR_RED) disp.show(img) ``` Steps: 1. Import the image, camera, and display modules ```python from maix import image, camera, display ``` 2. Initialize the camera and display ```python cam camera.Camera() disp display.Display() ``` 3. Get the image from the camera and display it ```python while 1: img cam.read() disp.show(img) ``` 4. Call the `find_apriltags` method to recognize Apriltag labels in the camera image ```python new_img img.resize(160, 120) apriltags new_img.find_apriltags(families families) ``` `img` is the camera image obtained through `cam.read()` `img.resize(160, 120)` is used to scale down the image to a smaller size, allowing the algorithm to compute faster with a smaller image `new_img.find_apriltags(families families)` is used to find Apriltag labels, and the query results are saved in `apriltags` for further processing. The `families` parameter is used to select the Apriltag family, defaulting to `image.ApriltagFamilies.TAG36H11` 5. Process the recognized label results and display them on the screen ```python for a in apriltags: # Get position information (and map coordinates to the original image) x int(a.x() * x_scale) y int(a.y() * y_scale) w int(a.w() * x_scale) corners a.corners() for i in range(4): corners[i][0] int(corners[i][0] * x_scale) corners[i][1] int(corners[i][1] * y_scale) # Display for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_RED) img.draw_string(x + w, y, \"id: \" + str(a.id()), image.COLOR_RED) img.draw_string(x + w, y + 15, \"family: \" + str(a.family()), image.COLOR_RED) img.draw_string(x + w, y + 30, \"rotation : \" + str(180 * a.rotation() // 3.1415), image.COLOR_RED) ``` Iterate through the members of `apriltags`, which is the result of scanning Apriltag labels through `img.find_apriltags()`. If no labels are found, the members of `apriltags` will be empty. `x_scale` and `y_scale` are used to map coordinates. Since `new_img` is a scaled down image, the coordinates of the Apriltag need to be mapped to be drawn correctly on the original image `img`. `a.corners()` is used to get the coordinates of the four vertices of the detected label, and `img.draw_line()` uses these four vertex coordinates to draw the shape of the label. `img.draw_string` is used to display the label content, where `a.x()` and `a.y()` are used to get the x and y coordinates of the top left corner of the label, `a.id()` is used to get the label ID, `a.family()` is used to get the label family type, and `a.rotation()` is used to get the rotation angle of the label. ### Common Parameter Explanations Here are explanations for common parameters. If you can't find parameters to implement your application, you may need to consider using other algorithms or extending the required functionality based on the current algorithm's results. Parameter Description Example roi Set the rectangular region for the algorithm to compute. roi [x, y, w, h], where x and y represent the coordinates of the top left corner of the rectangle, and w and h represent the width and height of the rectangle. The default is the entire image. Compute the region with coordinates (50,50) and a width and height of 100:<br />```img.find_apriltags(roi [50, 50, 100, 100])``` families Apriltag label family type Scan for labels from the TAG36H11 family:<br />```img.find_apriltags(families image.ApriltagFamilies.TAG36H11)``` This article introduces common methods. For more API information, please refer to the [image](../../../api/maix/image.html) section of the API documentation. ### Distance Measurement 1: The vertical distance between the object and the camera. This section describes a method to estimate the distance using the formula `distance k / width`, where: `distance`: Distance between the camera and the object in millimeters (mm). `k`: A constant. `width`: Width of the object in the image, measured in pixels. **Advantages**: Simple and easy to understand, convenient for measurement. **Disadvantages**: It can only measure the vertical distance between the tag and the camera. When the tag is tilted, the error increases. The process consists of two steps: 1. Measure the constant coefficient `k`. 2. Calculate the distance between the camera and the object using the constant and the object's `width`. #### Preparation 1. AprilTag paper for calibration. 2. A ruler (or other measuring tool). #### Measuring the Constant Coefficient `k` Fix the AprilTag in place and set the camera (maixcam) at a distance of 20 cm from the tag. Use `maixcam` to detect the `AprilTag` and calculate the tag's `width`. Refer to the following code: ```python from maix import camera, display import math ''' x1, y1, x2, y2: Coordinates of two points defining the tag's width, typically obtained using the corners() method. Returns the width of the tag in pixels. ''' def caculate_width(x1, y1, x2, y2): return math.sqrt((x2 x1)**2 + (y2 y1)**2) cam camera.Camera(160, 120) disp display.Display() while 1: img cam.read() apriltags img.find_apriltags() for a in apriltags: corners a.corners() # Calculate width using two horizontal corner points width caculate_width(corners[0][0], corners[0][1], corners[1][0], corners[1][1]) # Print the detected width of the AprilTag print(f'apriltag width:{width}') disp.show(img) ``` Calculate the constant `k`: ```python ''' width: Width of the AprilTag detected at a known distance. distance: The actual distance to the AprilTag during detection, in mm. Returns the constant coefficient. ''' def caculate_k(width, distance): return width * distance # Example: At a distance of 200 mm, the tag width is detected as 43 pixels k caculate_k(43, 200) ``` #### Calculate Distance Between Camera and Object Using `k` ```python ''' width: Width of the AprilTag in pixels. k: Constant coefficient. Returns the distance between the camera and the object in mm. ''' def caculate_distance(width, k): return k / width distance caculate_distance(55, 8600) ``` #### 完整的代码参考: ```python from maix import camera, display, image import math ''' x1, y1, x2, y2: Coordinates of two points defining the tag's width, typically obtained using the corners() method. Returns the width of the tag in pixels. ''' def caculate_width(x1, y1, x2, y2): return math.sqrt((x2 x1)**2 + (y2 y1)**2) ''' width: Width of the AprilTag detected at a known distance. distance: The actual distance to the AprilTag during detection, in mm. Returns the constant coefficient. ''' def caculate_k(width, distance): return width * distance ''' width: Width of the AprilTag in pixels. k: Constant coefficient. Returns the distance between the camera and the object in mm. ''' def caculate_distance(width, k): return k / width cam camera.Camera(192, 108) disp display.Display() # Example: At a distance of 200 mm, the tag width is detected as 43 pixels k caculate_k(43, 200) while 1: img cam.read() apriltags img.find_apriltags() for a in apriltags: corners a.corners() for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_GREEN) # Calculate width using two horizontal corner points width caculate_width(corners[0][0], corners[0][1], corners[1][0], corners[1][1]) # Calculate distance distance caculate_distance(width, k) print(f'apriltag width:{width} distance:{distance} mm') disp.show(img) ``` This method uses the width of the `AprilTag` to calculate the distance. It can also be extended to use height for distance calculation. However, note that this approach provides an estimate of the distance, and slight inaccuracies may occur due to real world factors. ### Distance Measurement 2: Measuring the distance from the object to the camera using the AprilTag By using the `apriltag` for distance measurement, we can accurately determine the position of the tag in space. In this case, we also use the parameters returned by the `find_apriltag()` method to calculate the position of any `AprilTag` relative to the camera. Of course, the prerequisite is that you must detect the `apriltag`. The process consists of two steps: 1. Calculate the constant coefficient `k`. 2. Use the position information returned by `find_apriltag()` to calculate the distance from the tag to the camera. **Advantages**: This method allows you to measure the distance between the `apriltag` and the camera even if the tag is rotated or offset relative to the camera. #### Preparations: 1. `apriltag` paper tag. 2. Ruler (or other distance measuring tools). #### Measuring the Constant Coefficient `k`: Fix the `apriltag` paper tag and place the `MaixCAM` at a distance of `20cm` from the `apriltag`. Use `MaixCAM` to detect the `apriltag` and calculate the constant coefficient using the `z_translation`. Reference code: ```python from maix import camera, display ''' z_trans: The value of z_translation() when the distance is 'distance' (actual distance in mm). distance: The actual distance to the AprilTag when detected, in mm. Returns the constant coefficient 'k'. ''' def caculate_k(z_trans, distance): return distance / z_trans cam camera.Camera(160, 120) disp display.Display() while 1: img cam.read() apriltags img.find_apriltags() for a in apriltags: k caculate_k(a.z_translation(), 200) print(f\"k:{k}\") disp.show(img) ``` #### Measuring the Distance from the Tag to the Object: Calculate the distance from the `apriltag` to the camera using `x_translation`, `y_translation`, and `z_translation` returned by the `apriltag`: ```python ''' x_trans: The value of x_translation() returned by detecting the AprilTag. y_trans: The value of y_translation() returned by detecting the AprilTag. z_trans: The value of z_translation() returned by detecting the AprilTag. k: Constant coefficient. Returns the distance in mm. ''' def calculate_distance(x_trans, y_trans, z_trans, k): return k * math.sqrt(x_trans * x_trans + y_trans * y_trans + z_trans * z_trans) ``` #### Complete Code Example: ```python from maix import camera, display, image import math ''' z_trans: The value of z_translation() when the distance is 'distance' (actual distance in mm). distance: The actual distance to the AprilTag when detected, in mm. Returns the constant coefficient 'k'. ''' def caculate_k(z_trans, distance): return distance / z_trans ''' x_trans: The value of x_translation() returned by detecting the AprilTag. y_trans: The value of y_translation() returned by detecting the AprilTag. z_trans: The value of z_translation() returned by detecting the AprilTag. k: Constant coefficient. Returns the distance in mm. ''' def calculate_distance(x_trans, y_trans, z_trans, k): return abs(k * math.sqrt(x_trans * x_trans + y_trans * y_trans + z_trans * z_trans)) cam camera.Camera(160, 120) disp display.Display() # When the distance is 200mm, the detected z_translation() of the AprilTag is 9.7 k caculate_k( 9.7, 200) while 1: img cam.read() apriltags img.find_apriltags() for a in apriltags: corners a.corners() for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_GREEN) # Calculate the distance x_trans a.x_translation() y_trans a.y_translation() z_trans a.z_translation() distance calculate_distance(x_trans, y_trans, z_trans, k) print(f'apriltag k:{k} distance:{distance} mm') disp.show(img) ``` This code uses the `MaixCAM` to continuously read the camera feed, detect the `apriltag`, and calculate the distance from the tag to the camera using the constant coefficient and the tag's translations in all three axes. The calculated distance is printed out in millimeters. Note that this approach provides an estimate of the distance, and slight inaccuracies may occur due to real world factors."},"/maixpy/doc/en/vision/qrcode.html":{"title":"MaixCAM MaixPy QR Code Recognition","content":" title: MaixCAM MaixPy QR Code Recognition update: date: 2024 04 03 author: lxowalle version: 1.0.0 content: Initial document Before reading this article, make sure you are familiar with how to develop with MaixCAM. For details, please read [Quick Start](../index.html). ## Introduction This article explains how to use MaixPy for QR code recognition. ## Using MaixPy to Recognize QR Codes MaixPy's `maix.image.Image` includes the `find_qrcodes` method for QR code recognition. ### How to Recognize QR Codes A simple example that recognizes QR codes and draws a bounding box: ```python from maix import image, camera, display cam camera.Camera(320, 240) disp display.Display() while True: img cam.read() qrcodes img.find_qrcodes() for qr in qrcodes: corners qr.corners() for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_RED) img.draw_string(qr.x(), qr.y() 15, qr.payload(), image.COLOR_RED) disp.show(img) ``` Steps: 1. Import the image, camera, and display modules: ```python from maix import image, camera, display ``` 2. Initialize the camera and display: ```python cam camera.Camera(320, 240) # Initialize the camera with a resolution of 320x240 in RGB format disp display.Display() ``` 3. Capture and display images from the camera: ```python while True: img cam.read() disp.show(img) ``` 4. Use the `find_qrcodes` method to detect QR codes in the camera image: ```python qrcodes img.find_qrcodes() ``` `img` is the camera image captured by `cam.read()`. When initialized as `cam camera.Camera(320, 240)`, the `img` object is a 320x240 resolution RGB image. `img.find_qrcodes` searches for QR codes and saves the results in `qrcodes` for further processing. 5. Process and display the results of QR code recognition on the screen: ```python for qr in qrcodes: corners qr.corners() for i in range(4): img.draw_line(corners[i][0], corners[i][1], corners[(i + 1) % 4][0], corners[(i + 1) % 4][1], image.COLOR_RED) img.draw_string(qr.x(), qr.y() 15, qr.payload(), image.COLOR_RED) ``` `qrcodes` contains the results from `img.find_qrcodes()`. If no QR codes are found, `qrcodes` will be empty. `qr.corners()` retrieves the coordinates of the four corners of the detected QR code. `img.draw_line()` uses these coordinates to draw the QR code outline. `img.draw_string` displays information about the QR code content and position. `qr.x()` and `qr.y()` retrieve the x and y coordinates of the QR code's top left corner, and `qr.payload()` retrieves the content of the QR code. ### Common Parameter Explanation List common parameters and their explanations. If you cannot find parameters that fit your application, consider whether to use a different algorithm or extend the functionality based on the current algorithm's results. Parameter Description Example roi Sets the rectangular area for the algorithm to compute, where roi [x, y, w, h], x and y denote the top left coordinates of the rectangle, and w and h denote the width and height of the rectangle, defaulting to the entire image. Compute the area with coordinates (50,50) and width and height of 100:<br />`img.find_qrcodes(roi [50, 50, 100, 100])` qrcoder_type Set the QR code library decoder type; you can choose either `image.QRCodeDecoderType.QRCODE_DECODER_TYPE_ZBAR` or `image::QRCodeDecoderType::QRCODE_DECODER_TYPE_QUIRC`. `QRCODE_DECODER_TYPE_ZBAR` offers faster recognition speed and higher accuracy at lower resolutions. `QRCODE_DECODER_TYPE_QUIRC` is relatively faster at higher resolutions but with slightly lower accuracy. By default, `QRCODE_DECODER_TYPE_ZBAR` is used.<br />Effective in version 4.7.7 and later. img.find_qrcodes(decoder_type image.QRCodeDecoderType.QRCODE_DECODER_TYPE_ZBAR) This article introduces common methods. For more API details, refer to the [image](../../../api/maix/image.html) section of the API documentation. ## Using Hardware Acceleration for QR Code Detection MaixPy includes a built in `image.QRCodeDetector` object that can use hardware acceleration for QR code detection. At a resolution of 320x224, the maximum frame rate for a single frame algorithm can reach 60+ fps. > Note: This feature is supported in MaixPy v4.7.9 and later versions ### Usage ```python from maix import camera, display, app, image cam camera.Camera(320, 224) disp display.Display() detector image.QRCodeDetector() while not app.need_exit(): img cam.read() qrcodes detector.detect(img) for q in qrcodes: img.draw_string(0, 0, \"payload: \" + q.payload(), image.COLOR_BLUE) disp.show(img) ``` Steps: 1. Import the `image`, `camera`, and `display` modules: ```python from maix import camera, display, app, image ``` 2. Capture and display the image: ```python cam camera.Camera(320, 224) disp display.Display() while not app.need_exit(): img cam.read() disp.show(img) ``` Create `Camera` and `Display` objects. Use the `cam.read()` method to capture the image and the `disp.show()` method to display it. 3. Create a `QRCodeDetector` object for QR code detection: ```python detector image.QRCodeDetector() ``` 4. Use the `detect` method to detect QR codes, saving the results in the `qrcodes` variable: ```python qrcodes detector.detect(img) for q in qrcodes: img.draw_string(0, 0, \"payload: \" + q.payload(), image.COLOR_BLUE) ``` Note: The detection process will utilize NPU resources. If other models are using the NPU at the same time, it may cause unexpected results. The structure of the detection result is the same as the data returned by `find_qrcodes`. Refer to the `QRCode` object's methods to access the detection results. For example, calling `q.payload()` will retrieve the QR code's content string."},"/maixpy/doc/en/vision/face_emotion.html":{"title":"MaixCAM MaixPy Facial Expression Recognition, Gender, Mask, Age, and More","content":" title: MaixCAM MaixPy Facial Expression Recognition, Gender, Mask, Age, and More update: date: 2025 01 10 version: v1.0 author: neucrack content: Added source code, documentation, and examples for facial emotion recognition. ## Introduction In the previous articles, [Facial Detection and Keypoint Detection](./face_detection.html) and [Facial Multi Keypoint Detection], we introduced how to detect faces, keypoints, and facial recognition. This article focuses on recognizing facial emotions (expressions). It also explores how to identify other characteristics, such as gender, mask wearing, and age. ![](../../assets/face_emotion_happy.jpg) ![](../../assets/face_emotion_neutral.jpg) Demonstration video on MaixCAM: <video playsinline controls autoplay loop muted preload src \"/static/video/maixcam_face_emotion.mp4\" type \"video/mp4\"> Classifier Result video </video> > Video source: [oarriaga/face_classification](https://github.com/oarriaga/face_classification) ## Using Facial Emotion Recognition in MaixCAM MaixPy MaixPy provides a default emotion recognition model with seven categories: * angry * disgust * fear * happy * sad * surprise * neutral The process for emotion recognition involves several steps: 1. Detect the face. 2. Crop the face into a standard format, as shown in the small image in the top left corner above. 3. Classify the cropped face image using a simple model. In MaixPy, the `yolov8 face` model is used for detecting facial and eye positions, followed by classification. Below is the code, which is also available in the [MaixPy](https://github.com/sipeed/maixpy) `examples` directory: ```python from maix import camera, display, image, nn, app detect_conf_th 0.5 detect_iou_th 0.45 emotion_conf_th 0.5 max_face_num 1 crop_scale 0.9 # detect face model detector nn.YOLOv8(model \"/root/models/yolov8n_face.mud\", dual_buff False) # landmarks detector for cropping images landmarks_detector nn.FaceLandmarks(model \"\") # emotion classify model classifier nn.Classifier(model \"/root/models/face_emotion.mud\", dual_buff False) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() # for drawing result info max_labels_length 0 for label in classifier.labels: size image.string_size(label) if size.width() > max_labels_length: max_labels_length size.width() max_score_length cam.width() / 4 while not app.need_exit(): img cam.read() results [] objs detector.detect(img, conf_th detect_conf_th, iou_th detect_iou_th, sort 1) count 0 idxes [] img_std_first: image.Image None for i, obj in enumerate(objs): img_std landmarks_detector.crop_image(img, obj.x, obj.y, obj.w, obj.h, obj.points, classifier.input_width(), classifier.input_height(), crop_scale) if img_std: img_std_gray img_std.to_format(image.Format.FMT_GRAYSCALE) res classifier.classify(img_std_gray, softmax True) results.append(res) idxes.append(i) if i 0: img_std_first img_std count + 1 if max_face_num > 0 and count > max_face_num: break for i, res in enumerate(results): if i 0: img.draw_image(0, 0, img_std_first) for j in range(len(classifier.labels)): idx res[j][0] score res[j][1] img.draw_string(0, img_std_first.height() + idx * 16, classifier.labels[idx], image.COLOR_WHITE) img.draw_rect(max_labels_length, int(img_std_first.height() + idx * 16), int(score * max_score_length), 8, image.COLOR_GREEN if score > emotion_conf_th else image.COLOR_RED, 1) img.draw_string(int(max_labels_length + score * max_score_length + 2), int(img_std_first.height() + idx * 16), f\"{score:.1f}\", image.COLOR_RED) color image.COLOR_GREEN if res[0][1] > emotion_conf_th else image.COLOR_RED obj objs[idxes[i]] img.draw_rect(obj.x, obj.y, obj.w, obj.h, color, 1) img.draw_string(obj.x, obj.y, f\"{classifier.labels[res[0][0]]}: {res[0][1]:.1f}\", color) disp.show(img) ``` ### Key Code The core code steps are as follows: ```python objs detector.detect(img, conf_th detect_conf_th, iou_th detect_iou_th, sort 1) img_std landmarks_detector.crop_image(...) img_std_gray img_std.to_format(image.Format.FMT_GRAYSCALE) res classifier.classify(img_std_gray, softmax True) ``` These correspond to: 1. Detect the face. 2. Crop the face. 3. Classify the face image using a model (convert to grayscale before input as required). ## Improving Recognition Accuracy The default MaixPy model offers basic classification but can be optimized by: * **Using keypoints as model input:** Instead of cropped images, facial keypoints can be used for input, removing background interference and improving training accuracy. * **Enhancing datasets:** Increase the number and variety of samples. * **Improving cropping techniques:** Use advanced transformations for precise cropping, such as affine transformations commonly used in facial recognition. ## Training a Custom Classification Model ### Overview 1. **Define categories:** E.g., 7 emotions, gender, mask detection, etc. 2. **Choose a model:** Lightweight classification models like MobileNetV2 work well. 3. **Select a training platform:** * Use [MaixHub](https://maixhub.com) for online training (**recommended**). * Alternatively, train locally using PyTorch or TensorFlow. 4. **Collect data:** Modify the code to save captured images, e.g., `img.save(\"/root/image0.jpg\")`. 5. **Clean data:** Organize samples into labeled folders. 6. **Train:** * On MaixHub for an easy to deploy model. * Locally, then convert the model to ONNX and to MUD format for MaixPy. ## Recognizing Other Facial Features (Gender, Mask, Age, etc.) The same principles apply to features like gender or mask detection. For numerical outputs like age, consider using regression models. Research online for more advanced techniques."},"/maixpy/doc/en/vision/camera.html":{"title":"MaixCAM MaixPy Camera Usage","content":" title: MaixCAM MaixPy Camera Usage update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial version of the document date: 2024 08 21 author: YWJ version: 1.0.1 content: Fixed some bugs in the document, added some content date: 2024 10 24 author: neucrack version: 1.1.0 content: Added USB camera support instructions date: 2025 07 28 author: neucrack & lxo version: 1.2.0 content: Add AWB and lens usage doc. ## Introduction ## API Documentation This document introduces commonly used methods. For more API usage, please refer to the documentation of the [maix.camera](/api/maix/camera.html) module. ## Supported Cameras <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM</span></div> MaixCAM comes with a default GC4653 camera or optional OS04A10 camera, global shutter camera, or even HDMI to MIPI module, all of which can be used directly through simple API calls. CameraDescription **GC4653**M12 universal lens, 1/3\" sensor, clear image quality, 4M pixels. Suitable for common scenarios such as AI recognition and image processing. **OS04A10**M12 universal lens, 1/1.8\" large sensor, ultra high image quality, 4M pixels. Suitable for scenarios that require high image quality, such as photography and video recording. Note that it generates more heat. **OV2685**Lens is not replaceable, 1/5\" sensor, 2M pixels, lowest image quality and cost. Generally not recommended. **SC035HGS**Monochrome global shutter camera, 0.3M monochrome pixels, suitable for capturing fast moving objects. <br> <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM2</span></div> CameraDescription **OS04D10**M12 universal lens with a 1/3\" sensor. Provides clear image quality at 4MP. Well suited for common applications such as AI recognition and image processing. **OS04A10**M12 universal lens with a large 1/1.79\" sensor. Delivers ultra clear 4MP image quality. Ideal for scenarios demanding high visual fidelity. The AI ISP enhances low light performance for applications like photography and videography. Note: Heat generation is comparatively higher. **SC850SL**M12 universal lens with a large 1/1.8\" sensor. Produces ultra clear 8MP resolution. Designed for high quality imaging scenarios. Combined with the AI ISP, it offers excellent night vision capability, making it perfect for photography and video recording. The system will switch automatically; just replace the hardware to use. ## Lens Cap The lens cap is for dust protection. **Please remove the lens cap!!** before use. ## Adjusting Camera Focus MaixCAM is equipped with a **manual focus lens** by default. You can physically rotate the lens to adjust focus. If you find the **image is blurry**, try rotating the lens (clockwise and counterclockwise) to focus until the image is clear. ## Getting Image from the Camera Use MaixPy to easily acquire images: ```python from maix import camera cam camera.Camera(640, 480) while 1: img cam.read() print(img) ``` Here we import the `camera` module from `maix`, create a `Camera` object with specified width and height, and then continuously read images in a loop. The default image format is `RGB`. For `BGR` or other formats, see the API documentation. You can also get grayscale images: ```python from maix import camera, image cam camera.Camera(640, 480, image.Format.FMT_GRAYSCALE)\t# Set output to grayscale image ``` Or get NV21 images: ```python from maix import camera, image cam camera.Camera(640, 480, image.Format.FMT_YVU420SP)\t# Set output to NV21 image ``` ## Set Camera Resolution Specify width and height directly in the code: ```python from maix import camera cam camera.Camera(width 640, height 480) ``` or ```python from maix import camera cam camera.Camera() cam.set_resolution(width 640, height 480) ``` ### Choosing Resolution Size Different boards and camera modules support different resolutions. Make sure to use **even number values**. It’s important to understand that **higher resolution is not always better**—choose the right resolution based on your scenario: * Photography/Video/Monitoring: Higher resolution can give clearer images. OS04D10, GC4653 and OS04A10 support up to `2560x1440` resolution (i.e. `2K/4M pixels`). But high resolution demands more programming skill and memory. You may consider smaller resolutions such as `1920x1080`, `1280x720`, `640x480`, etc. Note: When running code in **MaixVision**, if you set high resolution (e.g., `2560x1440`), disable the image preview feature in MaixVision to avoid errors due to insufficient memory. * AI Recognition / Image Processing: For faster performance, reduce resolution as much as possible while still ensuring recognizability. * `640x480`: VGA resolution; large enough for most AI recognition and clear image processing. Demands more from MaixCAM; easier for MaixCAM2. * `320x320`: Square resolution; suitable for some AI models, but will have black borders on rectangle screens. * `320x240`: QVGA resolution; easy for algorithms, and still meets clarity needs. * `320x224`: Both width and height are multiples of 32; suitable for small resolution AI input and displays well on `552x368` screen. * `224x224`: Square, both dimensions are multiples of 32; fits small resolution AI models like `MobileNetV2`, `MobileNetV3`. ### Aspect Ratio of Resolution Aspect ratio affects field of view. For example, sensor max is `2560x1440` (16:9). Using `640x480` changes to 4:3, reducing field of view. To maximize view, choose resolution matching sensor's aspect ratio, e.g., `1280x720`, `2560x1440`. Different ratios will result in center cropping. ## Set Camera Frame Rate The camera operates at specific frame rates. MaixPy supports frame rate settings. Supported frame rates vary by module: <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM</span></div> GC4653 OS04A10 OV2685 SC035HGS 2560x1440\\@30fps<br>1280x720\\@60fps<br>1280x720\\@80fps 2560x1440\\@30fps<br>1280x720\\@80fps 1920x1080\\@30fps 640x480\\@180fps <br> <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM2</span></div> GC4653 SC850SL 2560x1440@30fps<br>1280x720@60fps 3840x2160@30fps<br>1920x1080@60fps Frame rate is set via `width`, `height`, and `fps` when creating `Camera`. ### Set Frame Rate to 30fps ```python from maix import camera cam camera.Camera(640, 480, fps 30) # or cam camera.Camera(1920, 1280) # Above 1280x720 will auto use 30fps ``` ### Set Frame Rate to 60fps ```python from maix import camera cam camera.Camera(640, 480, fps 60) # or cam camera.Camera(640, 480) # ≤1280x720 defaults to 80fps ``` ### Set Frame Rate to 80fps ```python from maix import camera cam camera.Camera(640, 480, fps 80) ``` Notes: 1. If size > `1280x720`, e.g., `camera.Camera(1920, 1080, fps 60)`, then `fps` param is ignored and stays at 30fps. 2. 60/80fps images may shift by a few pixels compared to 30fps—consider correcting this if precise alignment is needed. 3. 60/80fps and 30fps share ISP config—image quality may vary slightly. 4. Some camera setups can't handle 80fps—may show visual artifacts. Switch back to 60fps if needed. ## Image Correction For fisheye or distortion, use `lens_corr` under `Image` to correct. Adjust the `strength` value for best results. ```python from maix import camera, display, app, time cam camera.Camera(320, 240) disp display.Display() while not app.need_exit(): t time.ticks_ms() img cam.read() img img.lens_corr(strength 1.5) # Adjust strength until distortion is gone disp.show(img) ``` Note: This is software based correction and consumes time. You may also use non distorted lenses (ask vendor). ## Skip Initial Frames During initialization, camera may output unstable images. Skip first few frames using `skip_frames`: ```python cam camera.Camera(640, 480) cam.skip_frames(30) # Skip first 30 frames ``` ## Display Camera Image MaixPy provides the `display` module for easy image display: ```python from maix import camera, display cam camera.Camera(640, 480) disp display.Display() while 1: img cam.read() disp.show(img) ``` ## Set Camera Parameters ### Set Exposure Time Note: Setting exposure time switches to manual exposure. To revert to auto mode, run `cam.exp_mode(camera.AeMode.Auto)` ```python from maix import camera cam camera.Camera() cam.exposure(1000) ``` ### Set Gain Note: Setting gain switches to manual exposure. Custom gain only works in manual mode. ```python from maix import camera cam camera.Camera() cam.gain(100) ``` ### Set White Balance Usually auto white balance suffices. For color sensitive scenarios, manually set it: Set `awb_mode` to `Manual`, then set gain for `R`, `Gr`, `Gb`, `B` channels. Range: `[0.0, 1.0]`. Default gain values: * `MaixCAM`: `[0.134, 0.0625, 0.0625, 0.1239]` * `MaixCAM2`: `[0.0682, 0, 0, 0.04897]` Usually only `R` and `B` need adjustment. ```python from maix import camera cam camera.Camera() cam.awb_mode(camera.AwbMode.Manual) cam.set_wb_gain([0.134, 0.0625, 0.0625, 0.1239]) ``` ### Setting Lower Capture Latency You can reduce image capture latency by setting the buff_num parameter. However, note that changing this parameter affects the image buffer size, and setting it too low may lead to image loss. For MaixCam, due to limitations in the internal software framework, even if buff_num is set to 1, there will still be at least a double buffering mechanism in place. In testing, the minimum achievable capture latency is around 30+ ms. ```python from maix import camera cam camera.Camera(buff_num 1) # Use only 1 buffer ``` ### Set Brightness, Contrast, Saturation ```python from maix import camera cam camera.Camera() cam.luma(50)\t\t # Brightness [0, 100] cam.constrast(50)\t\t# Contrast [0, 100] cam.saturation(50)\t\t# Saturation [0, 100] ``` ### Read Raw Image To read raw `bayer` image data for processing or debugging: ```python from maix import camera cam camera.Camera(raw true) raw_img cam.read_raw() print(raw_img) ``` For viewing on PC, use script like [bayer\\_to\\_tiff](https://github.com/sipeed/MaixPy/blob/dev/examples/tools/bayer_to_tiff.py). ## Change Lens MaixCAM uses M12 lens by default and supports lens replacement. Notes: 1. Ensure new lens is M12. 2. Different lenses have different flange back distances. Default lens mount has fixed height—check compatibility. 3. Avoid scratching sensor surface. Blow dust gently, clean with lens paper only if necessary. 4. Want a zoom lens? Buy M12 zoom lens. 5. Default is manual focus. Auto focus lenses require driver support; MaixCAM lacks AF circuit—you may need to write control program. ## Use USB Camera (UVC) Besides built in MIPI camera, you can use USB camera. Steps: * Set `USB Mode` to `HOST` in system settings. Without screen, use script `examples/tools/maixcam_switch_usb_mode.py`. * As of 2024.10.24, `maix.camera` module **does not** support USB camera. Refer to [Using USB Camera with OpenCV](./opencv.html)."},"/maixpy/doc/en/vision/self_learn_classifier.html":{"title":"MaixCAM MaixPy Self-Learning Classifier","content":" title: MaixCAM MaixPy Self Learning Classifier ## Introduction to MaixPy Self Learning Classifier Usually, to recognize new categories, we need to collect a dataset on a computer and retrain the model, which is a cumbersome and difficult process. Here, we provide a method that allows for instant learning of new objects directly on the device without the need for computer side training, suitable for less complex scenarios. For example, if there is a bottle and a phone in front of you, you can use the device to take a picture of each as the basis for two classifications. Then, you collect a few more pictures of them from different angles, extract their features and save them. During recognition, the feature values of the image are compared with the saved feature values, and the classification that is more similar to the saved features is considered the corresponding classification. ## Using the Self Learning Classifier in MaixPy The default image comes with the [Self Learning Classification APP](https://maixhub.com/app/30), which you can use directly to get familiar with the process. ![](../../assets/self_learn_classifier.jpg) Steps: * Click the `+ Class` button to collect n classification (class) images. The object needs to be within the white frame on the screen while collecting the images. * Click the `+ Sample` button to collect m sample images. Collect some images for each classification. The order does not matter, and the number is flexible. It's best to take pictures from different angles, but not too different. * Click the `Learn` button to start learning. The device will automatically classify and learn based on the collected classification and sample images, obtaining the characteristics of the classifications. * Align the object with the center of the screen, recognize the image, and output the result. The screen will show the classification it belongs to and the similarity distance to this classification. The closer the similarity distance, the more similar it is. * The feature values ​​learned by this APP will be saved to `/root/my_classes.bin`, so the last one will be automatically loaded after exiting the application or restarting it. Simplified version of the code, for the complete version, please refer to the [examples](https://github.com/sipeed/maixpy/tree/main/examples/vision/ai_vision) for the full code. ```python from maix import nn, image classifier nn.SelfLearnClassifier(model \"/root/models/mobilenet_v2_no_top.mud\", dual_buff True) img1 image.load(\"/root/1.jpg\") img2 image.load(\"/root/2.jpg\") img3 image.load(\"/root/3.jpg\") sample_1 image.load(\"/root/sample_1.jpg\") sample_2 image.load(\"/root/sample_2.jpg\") sample_3 image.load(\"/root/sample_3.jpg\") sample_4 image.load(\"/root/sample_4.jpg\") sample_5 image.load(\"/root/sample_5.jpg\") sample_6 image.load(\"/root/sample_6.jpg\") classifier.add_class(img1) classifier.add_class(img2) classifier.add_class(img3) classifier.add_sample(sample_1) classifier.add_sample(sample_2) classifier.add_sample(sample_3) classifier.add_sample(sample_4) classifier.add_sample(sample_5) classifier.add_sample(sample_6) classifier.learn() img image.load(\"/root/test.jpg\") max_idx, max_score classifier.classify(img) print(max_idx, max_score) ``` ## Storing and Loading Learned Feature Values Use the `save` function to store the learned feature values. This will generate a binary file containing the feature values of the objects. When you need to use it again, simply use the `load` function to load the feature values. ```python classifier.save(\"/root/my_classes.bin\") classifier.load(\"/root/my_classes.bin\") ``` If you have named each classification and stored them in the `labels` variable, you can also use: ```python classifier.save(\"/root/my_classes.bin\", labels labels) labels classifier.load(\"/root/my_classes.bin\") ``` ## dual_buff Dual Buffer Acceleration You may have noticed that the model initialization uses `dual_buff` (which defaults to `True`). Enabling the `dual_buff` parameter can improve running efficiency and increase the frame rate. For detailed principles and usage notes, see [dual_buff Introduction](./dual_buff.html)."},"/maixpy/doc/en/vision/touchscreen.html":{"title":"MaixPy / MaixCAM Touchscreen Usage Guide","content":" title: MaixPy / MaixCAM Touchscreen Usage Guide ## Introduction MaixCAM comes equipped with a touchscreen, which, when used in conjunction with applications, can facilitate numerous engaging functionalities. We can utilize APIs to detect touch interactions on the touchscreen. ## Reading Touch Input with MaixPy MaixPy offers a straightforward `maix.touchscreen.TouchScreen` class for reading touch inputs. Here's an example: ```python from maix import touchscreen, app, time ts touchscreen.TouchScreen() pressed_already False last_x 0 last_y 0 last_pressed False while not app.need_exit(): x, y, pressed ts.read() if x ! last_x or y ! last_y or pressed ! last_pressed: print(x, y, pressed) last_x x last_y y last_pressed pressed if pressed: pressed_already True else: if pressed_already: print(f\"clicked, x: {x}, y: {y}\") pressed_already False time.sleep_ms(1) # sleep some time to free some CPU usage ``` ## Interactivity with the Screen Integrating the screen can enable various interactive user experiences. More examples can be found in the [MaixPy/examples/vision/touchscreen](https://github.com/sipeed/MaixPy) directory. As previously described, to display content on the screen, typically, a `maix.image.Image` object is created and displayed using `disp.show(img)`. Implementing a button is as simple as drawing one on the image and then detecting touches within its area, ensuring that the image's dimensions match those of the screen: ```python from maix import touchscreen, app, time, display, image ts touchscreen.TouchScreen() disp display.Display() img image.Image(disp.width(), disp.height()) # draw exit button exit_label \"< Exit\" size image.string_size(exit_label) exit_btn_pos [0, 0, 8*2 + size.width(), 12 * 2 + size.height()] img.draw_string(8, 12, exit_label, image.COLOR_WHITE) img.draw_rect(exit_btn_pos[0], exit_btn_pos[1], exit_btn_pos[2], exit_btn_pos[3], image.COLOR_WHITE, 2) def is_in_button(x, y, btn_pos): return x > btn_pos[0] and x < btn_pos[0] + btn_pos[2] and y > btn_pos[1] and y < btn_pos[1] + btn_pos[3] while not app.need_exit(): x, y, pressed ts.read() if is_in_button(x, y, exit_btn_pos): app.set_exit_flag(True) img.draw_circle(x, y, 1, image.Color.from_rgb(255, 255, 255), 2) disp.show(img) ``` ## Handling Different Screen and Image Sizes In the example above, the `img` matches the screen size. If your `img` and screen sizes differ (e.g., using `img image.Image(240, 240)` on a `640x480` screen), the default behavior of `disp.show(img)` is `image.Fit.FIT_CONTAIN`, which scales the image to `480x480` and fills the sides with black. If a button is drawn on the `240x240` image, such as at coordinates `(0, 0, 60, 40)`, the button will also be scaled up. Thus, the coordinates for touch detection should be adjusted to `((640 480) / 2, 0, 480/240*60, 480/240*40)`, which translates to `(80, 0, 120, 80)`. For convenience in scaling images and quickly calculating the positions and sizes of points or rectangles in the scaled image, the `image.resize_map_pos` function is provided: ```python from maix import touchscreen, app, time, display, image ts touchscreen.TouchScreen() disp display.Display() img image.Image(240, 240) img.draw_rect(0, 0, img.width(), img.height(), image.COLOR_WHITE) # draw exit button exit_label \"< Exit\" size image.string_size(exit_label) exit_btn_pos [0, 0, 8*2 + size.width(), 12 * 2 + size.height()] img.draw_string(8, 12, exit_label, image.COLOR_WHITE) img.draw_rect(exit_btn_pos[0], exit_btn_pos[1], exit_btn_pos[2], exit_btn_pos[3], image.COLOR_WHITE, 2) # 图像按键坐标映射到屏幕上的坐标 exit_btn_disp_pos image.resize_map_pos(img.width(), img.height(), disp.width(), disp.height(), image.Fit.FIT_CONTAIN, exit_btn_pos[0], exit_btn_pos[1], exit_btn_pos[2], exit_btn_pos[3]) def is_in_button(x, y, btn_pos): return x > btn_pos[0] and x < btn_pos[0] + btn_pos[2] and y > btn_pos[1] and y < btn_pos[1] + btn_pos[3] while not app.need_exit(): x, y, pressed ts.read() if is_in_button(x, y, exit_btn_disp_pos): app.set_exit_flag(True) # 屏幕的坐标映射回图像上对应的坐标，然后在图像上画点 x, y image.resize_map_pos_reverse(img.width(), img.height(), disp.width(), disp.height(), image.Fit.FIT_CONTAIN, x, y) img.draw_circle(x, y, 1, image.Color.from_rgb(255, 255, 255), 2) disp.show(img, fit image.Fit.FIT_CONTAIN) ```"},"/maixpy/doc/en/vision/body_key_points.html":{"title":"MaixCAM MaixPy Human Pose Keypoint Detection","content":" title: MaixCAM MaixPy Human Pose Keypoint Detection ## Introduction Using MaixPy, you can easily detect the coordinates of keypoints on human joints, which can be used for posture detection, such as monitoring sitting posture or providing input for motion based games. MaixPy implements human pose detection based on [YOLOv8 Pose / YOLO11 Pose](https://github.com/ultralytics/ultralytics), capable of detecting `17` keypoints on the human body. ![](../../assets/body_keypoints.jpg) ## Usage You can easily implement this using the `maix.nn.YOLOv8` or `maix.nn.YOLO11` classes in MaixPy: ```python from maix import camera, display, image, nn, app detector nn.YOLO11(model \"/root/models/yolo11n_pose.mud\", dual_buff True) # detector nn.YOLOv8(model \"/root/models/yolov8n_pose.mud\", dual_buff True) cam camera.Camera(detector.input_width(), detector.input_height(), detector.input_format()) disp display.Display() while not app.need_exit(): img cam.read() objs detector.detect(img, conf_th 0.5, iou_th 0.45, keypoint_th 0.5) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) detector.draw_pose(img, obj.points, 8 if detector.input_width() > 480 else 4, image.COLOR_RED) disp.show(img) ``` You can also find the code in the [MaixPy/examples/vision](https://github.com/sipeed/MaixPy/tree/main/examples/vision/ai_vision) directory. Since `YOLOv8 Pose` is used here, the `YOLOv8` class is also used, with the only difference being the model file compared to `YOLOv8` object detection. The same applies to `YOLO11`. The `detect` function returns an additional `points` value, which is a list of `int` containing `17` keypoints. The points are arranged in order; for example, the first value is the x coordinate of the nose, the second value is the y coordinate of the nose, and so on: ```python 1. Nose 2. Left Eye 3. Right Eye 4. Left Ear 5. Right Ear 6. Left Shoulder 7. Right Shoulder 8. Left Elbow 9. Right Elbow 10. Left Wrist 11. Right Wrist 12. Left Hip 13. Right Hip 14. Left Knee 15. Right Knee 16. Left Ankle 17. Right Ankle ``` If any of these parts are occluded, the value will be ` 1`. ## Models with More Resolutions The default model input resolution is `320x224`. If you want to use models with higher resolution, you can download and transfer them from the MaixHub model library: * YOLOv8 Pose: [https://maixhub.com/model/zoo/401](https://maixhub.com/model/zoo/401) * YOLO11 Pose: [https://maixhub.com/model/zoo/454](https://maixhub.com/model/zoo/454) Higher resolution generally provides better accuracy but at the cost of lower processing speed. Choose the model based on your application needs. If the provided resolution does not meet your requirements, you can train your own model using the source code from [YOLOv8 Pose / YOLO11 Pose](https://github.com/ultralytics/ultralytics) and export your own ONNX model, then convert it to a format supported by MaixCAM (methods are covered in later articles). ## dual_buff for Double Buffering Acceleration You may notice that `dual_buff` is used for model initialization (default value is `True`). Enabling the `dual_buff` parameter can improve efficiency and increase the frame rate. For more details and considerations, refer to the [dual_buff Introduction](./dual_buff.html)."},"/maixpy/doc/en/vision/face_recognition.html":{"title":"MaixCAM MaixPy Face Recognition","content":" title: MaixCAM MaixPy Face Recognition ## Introduction to Face Recognition ![face_recognize](../../assets/face_recognize.jpg) Face recognition involves identifying the location of faces in the current view and who they are. Thus, in addition to detecting faces, face recognition typically involves a database to store known and unknown individuals. ## Recognition Principles * Use AI models to detect faces, obtaining coordinates and features of facial components. * Use the coordinates of these features for affine transformation to align the face in the image to a standard face orientation, facilitating the extraction of facial features by the model. * Employ a feature extraction model to derive facial feature values. * Compare these features with those stored in the database (by calculating the cosine distance between the saved and the current facial features, identifying the face in the database with the smallest distance; if it's below a predefined threshold, it is recognized as the person in the database). ## Using MaixPy MaixPy's `maix.nn` module provides a face recognition API, ready to use with built in models. Additional models can also be downloaded from the [MaixHub model repository](https://maixhub.com/model/zoo) (select the appropriate hardware platform, such as maixcam). Recognition: ```python from maix import nn, camera, display, image import os import math recognizer nn.FaceRecognizer(detect_model \"/root/models/yolov8n_face.mud\", feature_model \"/root/models/insghtface_webface_r50.mud\", dual_buff True) # recognizer nn.FaceRecognizer(detect_model \"/root/models/retinaface.mud\", feature_model \"/root/models/face_feature.mud\", dual_buff True) if os.path.exists(\"/root/faces.bin\"): recognizer.load_faces(\"/root/faces.bin\") cam camera.Camera(recognizer.input_width(), recognizer.input_height(), recognizer.input_format()) disp display.Display() while 1: img cam.read() faces recognizer.recognize(img, 0.5, 0.45, 0.85) for obj in faces: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) radius math.ceil(obj.w / 10) img.draw_keypoints(obj.points, image.COLOR_RED, size radius if radius < 5 else 4) msg f'{recognizer.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) disp.show(img) ``` When you first run this code, it can detect faces but will not recognize them. We need to enter a mode to learn faces. > Here `recognizer.labels[0]` is by default `unknown`, and every new face added will automatically append to `labels`. For example, you can learn faces when a user presses a button: ```python faces recognizer.recognize(img, 0.5, 0.45, 0.85, True) for face in faces: print(face) # This accounts for the scenario where multiple faces are present in one scene; obj.class_id of 0 means the face is not registered # Write your own logic here # For instance, based on face’s class_id and coordinates, you can decide whether to add it to the database and facilitate user interaction, like pressing a button to register recognizer.add_face(face, label) # label is the name you assign to the face recognizer.save_faces(\"/root/faces.bin\") ``` Here, `0.5` is the threshold for face detection, where a higher value indicates stricter detection. `0.45` is the `IOU` threshold, used to filter overlapping face results. `0.85` is the threshold for face comparison, indicating similarity with stored faces in the database. If a face comparison score exceeds this threshold, it is considered a match. A higher threshold improves filtering accuracy, while a lower threshold increases the risk of misidentification and can be adjusted according to practical needs. The detection model here supports three types: `yolov8n_face`, `retinaface`, and `face_detector`, each differing slightly in speed and accuracy, allowing for selection based on specific requirements. ## Complete Example A complete example is provided for recording unknown faces and recognizing faces with a button press. This can be found in the [MaixPy example directory](https://github.com/sipeed/MaixPy/tree/main/examples) under `nn_face_recognize.py`. ## dual_buff Dual Buffer Acceleration You may have noticed that the model initialization uses `dual_buff` (which defaults to `True`). Enabling the `dual_buff` parameter can improve running efficiency and increase the frame rate. For detailed principles and usage notes, see [dual_buff Introduction](./dual_buff.html). ## Replacing Other Default Recognition Models The recognition model (for distinguishing different individuals) uses `mobilenetv2` and the [insight face resnet50](https://maixhub.com/model/zoo/462) model. If these do not meet accuracy requirements, other models can be substituted. You may need to train a new model or find a pre trained model compatible with MaixCAM, such as other models from [insightface](https://github.com/deepinsight/insightface). For conversion instructions, refer to the [MaixCAM model conversion documentation](../ai_model_converter/maixcam.html), and follow existing `.mud` files as examples."},"/maixpy/doc/en/vision/maixhub_train.html":{"title":"Using MaixHub to Train AI Models for MaixCAM MaixPy","content":" title: Using MaixHub to Train AI Models for MaixCAM MaixPy update: date: 2024 04 03 author: neucrack version: 1.0.0 content: Initial document ## Introduction MaixHub offers the functionality to train AI models online, directly within a browser. This eliminates the need for expensive hardware, complex development environments, or coding skills, making it highly suitable for beginners as well as experts who prefer not to delve into code. ## Basic Steps to Train a Model Using MaixHub ### Identify the Data and Model Types To train an AI model, you first need to determine the type of data and model. As of April 2024, MaixHub provides models for image data including `Object Classification Models` and `Object Detection Models`. Object classification models are simpler than object detection models, as the latter require marking the position of objects within images, which can be more cumbersome. Object classification merely requires identifying what is in the image without needing coordinates, making it simpler and recommended for beginners. ### Collect Data As discussed in AI basics, training a model requires a dataset for the AI to learn from. For image training, you need to create a dataset and upload images to it. Ensure the device is connected to the internet (WiFi). Open the MaixHub app on your device and choose to collect data to take photos and upload them directly to MaixHub. You need to create a dataset on MaixHub first, then click on device upload data, which will display a QR code. Scan this QR code with your device to connect to MaixHub. It's important to distinguish between training and validation datasets. To ensure the performance during actual operation matches the training results, the validation dataset must be of the same image quality as those taken during actual operation. It's also advisable to use images taken by the device for the training set. If using internet images, restrict them to the training set only, as the closer the dataset is to actual operational conditions, the better. ### Annotate Data For classification models, images are annotated during upload by selecting the appropriate category for each image. For object detection models, after uploading, you need to manually annotate each image by marking the coordinates, size, and category of the objects to be recognized. This annotation process can also be done offline on your own computer using software like labelimg, then imported into MaixHub using the dataset import feature. Utilize shortcuts during annotation to speed up the process. MaixHub will also add more annotation aids and automatic annotation tools in the future (there is already an automatic annotation tool available for videos that you can try). ### Train the Model Select training parameters, choose the corresponding device platform, select maixcam, and wait in the training queue. You can monitor the training progress in real time and wait for it to complete. ### Deploy the Model Once training is complete, you can use the deploy function in the MaixHub app on your device to scan a code and deploy. The device will automatically download and run the model, storing it locally for future use. If you find the recognition results satisfactory, you can share the model to the model library with a single click for others to use. ## How to Use Please visit [MaixHub](https://maixhub.com) to register an account, then log in. There are video tutorials on the homepage for learning. Note that if the tutorial uses the M2dock development board, the process is similar for MaixCAM, although the MaixHub application on the device might differ slightly. The overall process is the same, so please apply the knowledge flexibly."},"/maixpy/doc/en/vision/body_pose_classification.html":{"title":"MaixCAM MaixPy YOLO11-based Pose Estimation and Preliminary Human Posture Classification","content":" title: MaixCAM MaixPy YOLO11 based Pose Estimation and Preliminary Human Posture Classification ## Introduction The `MaixCAM MaixPy Pose Estimation` can estimate `17` human keypoints. ![](../../assets/body_keypoints.jpg) The connections between specific keypoints can simulate the human body, such as: ```markdown 3 1 0 2 4 form the head (Head) 5 6 12 11 5 form the torso (Torso) 5 7 9 or 6 8 10 form the upper limbs (Upper Limbs) 11 13 15 or 12 14 16 form the lower limbs (Lower Limbs) \"Thigh\": \"大腿\", \"Shin\": \"小腿\", \"Upper Arm\": \"大臂\", \"Forearm\": \"小臂\", ``` Each limb can be represented as a vector, and the angle between different limbs can be calculated. For example, the angle between the thigh and shin can be used to determine whether the shin is straight or bent. Since a human cannot stand upright with a bent shin, similar logic can be applied to classify the posture of the human body. Current classifications include: 1. \"Lying Down\": \"躺下\", 2. \"Standing Up\": \"直立\", 3. \"Sitting Down\": \"坐下\", 4. \"Reclining\": \"斜躺\", 5. \"To Left 1\": \"向左1\", 6. \"To Right 1\": \"向右1\", 7. \"Both Hands Raised Horizontally\": \"双手平举\", 8. \"Left Hand Raised\": \"举左手\", 9. \"Right Hand Raised\": \"举右手\", 10. \"Both Hands Raised Up\": \"举双手\", 11. \"Both Hands Forming a Heart\": \"双手比心\", 12. \"Big 'T' Shape\": \"大字型\", Example Image ![](../../assets/body_pose_classification.jpg) ## Usage The app `Human Pose Classifier` packaged in `projects/app_human_pose_classifier/` can be directly run. The file `nn_yolo11_pose_cls.py` under `examples/vision/ai_vision/` is a standalone implementation and can be run directly on MaixVision by just clicking the run button. It is recommended to refer to `PoseEstimation.py` for customization."},"/maixpy/doc/en/index.html":{"title":"MaixCAM MaixPy Quick Start","content":" title: MaixCAM MaixPy Quick Start <style> #head_links table { width: 100%; display: table; } @media screen and (max width: 900px){ #head_links th, #head_links td { /* padding: 8px; */ font size: 0.9em; padding: 0.1em 0.05em; } } </style> <div id \"head_links\"> Resource Summary Link : : : : Tutorial Documentation 📖 [wiki.sipeed.com/maixpy/en/](https://wiki.sipeed.com/maixpy/en/) Examples and Source Code <img src \"/static/image/github fill.svg\" style \"height: 1.5em;vertical align: middle;\"> [github.com/sipeed/MaixPy](https://github.com/sipeed/MaixPy) MaixCAM Hardware 📷 [wiki.sipeed.com/maixcam](https://wiki.sipeed.com/maixcam) </br> [wiki.sipeed.com/maixcam pro](https://wiki.sipeed.com/maixcam pro) </br> [wiki.sipeed.com/maixcam2](https://wiki.sipeed.com/maixcam2) API Documentation 📚 [wiki.sipeed.com/maixpy/api/](https://wiki.sipeed.com/maixpy/api/index.html) MaixHub App Store 📦 [maixhub.com/app](https://maixhub.com/app) MaixHub Sharing Square 🎲 [maixhub.com/share](https://maixhub.com/share) Open Source Projects 📡 GitHub Search：[MaixCAM](https://github.com/search?q maixcam&type repositoriese) / [MaixCAM2](https://github.com/search?q maixcam2&type repositoriese) / [MaixPy](https://github.com/search?q maixpy&type repositoriese) </div> <div style \"font size: 1.2em;padding:1em; text align:center; color: white\"> <div style \"padding: 1em 0 0 0\"> <a target \"_blank\" style \"color: white; font size: 0.9em; border radius: 0.3em; padding: 0.5em; background color: #c33d45\" href \"https://item.taobao.com/item.htm?id 784724795837\">Taobao(MaixCAM)</a> <a target \"_blank\" style \"color: white; font size: 0.9em; border radius: 0.3em; padding: 0.5em; background color: #c33d45\" href \"https://item.taobao.com/item.htm?id 846226367137\">Taobao(MaixCAM Pro)</a> <a target \"_blank\" style \"color: white; font size: 0.9em; border radius: 0.3em; padding: 0.5em; background color: #c33d45\" href \"https://www.aliexpress.com/store/911876460\">AliExpress</a> </div> </div> <br> > For an introduction to MaixPy, please see the [MaixPy official website homepage](../../index.html) > Please give the [MaixPy project](https://github.com/sipeed/MaixPy) a Star ⭐️ to encourage us to develop more features if you like MaixPy. <iframe style \"width:100%;min height:30em\" src \"https://www.youtube.com/embed/qV1lw0UVUYI?si g3xUX5v3iT9r7RxJ\" title \"YouTube video player\" frameborder \"0\" allow \"accelerometer; autoplay; clipboard write; encrypted media; gyroscope; picture in picture; web share\" referrerpolicy \"strict origin when cross origin\" allowfullscreen></iframe> ## Quick Preview <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM2(The Upgraded Version of MaixCAM~)</span></div> <iframe src \"//player.bilibili.com/player.html?isOutside true&aid 115547387727388&bvid BV1veCTBsEZa&cid 33995951833&p 1\" scrolling \"no\" border \"0\" frameborder \"no\" framespacing \"0\" allowfullscreen \"true\"></iframe> <div align \"center\"><span style \"color: #c33d45; font weight: bold;\">MaixCAM</span></div> <iframe src \"//player.bilibili.com/player.html?isOutside true&aid 113485669204279&bvid BV1ncmRYmEDv&cid 26768769718&p 1\" scrolling \"no\" border \"0\" frameborder \"no\" framespacing \"0\" allowfullscreen \"true\" class \"biliiframe\"></iframe> ## Before Start * Please **carefully** follow the steps outlined in this document. Do not skip any sections, and compare your actions accordingly. * **Pay close attention** to the table of contents on the left. Be sure to read through the basic sections thoroughly and patiently. * **Before asking questions**, first search the documentation in the left hand table of contents and review the [FAQ](./faq.html). * This document is the `MaixPy v4 Tutorial`. Be mindful not to confuse it with the [MaixPy v1](https://wiki.sipeed.com/soft/maixpy/zh/index.html) (K210 series), and ensure you are referring to the correct documentation. ## Get a MaixCAM/MaixCAM2 Device ![maixcam2](https://wiki.sipeed.com/static/image/maixcam2_front_back.png) Basic Information:[MaixCAM2 Introduction & Resources](https://wiki.sipeed.com/hardware/en/maixcam/maixcam2.html) Purchase Links:[Sipeed Taobao](https://item.taobao.com/item.htm?id 846226367137) or [Sipeed AliExpress](https://www.aliexpress.com/store/911876460) <br> ![maixcam_pro](../../static/image/maixcam_pro.png) **MaixCAM** currently has several versions. Choose according to your needs: **MaixCAM Pro**(Recommended)Baisc Infomation:[MaixCAM Pro Introduction & Resources](https://wiki.sipeed.com/maixcam pro)<br>Purchase Links:[Sipeed Taobao](https://item.taobao.com/item.htm?id 846226367137) or [Sipeed AliExpress](https://www.aliexpress.com/store/911876460) **MaixCAM**Baisc Infomation:[MaixCAM Introduction & Resources](https://wiki.sipeed.com/maixcam)<br>Purchase Links:[Sipeed Taobao](https://item.taobao.com/item.htm?id 784724795837) or [Sipeed AliExpress](https://www.aliexpress.com/store/911876460) **MaixCAM Lite**(Not Recommended)Screenless and caseless version, more affordable. Not recommended for learning/development, but may be considered for mass production. <br> >! Note that currently only the MaixCAM development board is supported. Other development boards with the same chip are not supported, including Sipeed's development boards with the same chip. Please be careful not to purchase the wrong board, which could result in unnecessary waste of time and money. ## Getting Started Please select the documentation corresponding to your hardware platform to proceed: 硬件平台上手文档 MaixCAM Lite[Quick Start MaixCAM(Screenless Version)](./README_no_screen.html) MaixCAM/MaixCAM Pro[Quick Start MaixCAM](./README_MaixCAM.html) MaixCAM2[Quick Start MaixCAM2](./README_MaixCAM2.html) ## Next Steps If you like what you've seen so far, **please be sure to give the MaixPy open source project a star on [GitHub](https://github.com/sipeed/MaixPy) (you need to log in to GitHub first). Your star and recognition is the motivation for us to continue maintaining and adding new features!** Up to this point, you've experienced the usage and development workflow. Next, you can learn about `MaixPy` syntax and related features. Please follow the left sidebar to learn. If you have any questions about using the API, you can look it up in the [API documentation](/api/). It's best to learn with a specific purpose in mind, such as working on an interesting small project. This way, the learning effect will be better. You can share your projects and experiences on the [MaixHub Share Plaza](https://maixhub.com/share) and receive cash rewards! ## Frequently Asked Questions (FAQ) If you encounter any problems, please check the [FAQ](./faq.html) first. If you cannot find a solution there, you can ask in the forums or groups below, or submit a source code issue on [MaixPy issue](https://github.com/sipeed/MaixPy/issues). ## Share and Discuss * **[MaixHub Project and Experience Sharing](https://maixhub.com/share)**: Share your projects and experiences, and receive cash rewards. The basic requirements for receiving official rewards are: * **Reproducible**: A relatively complete process for reproducing the project. * **Showcase**: No detailed project reproduction process, but an attractive project demonstration. * **Bug solving experience**: Sharing the process and specific solution for resolving a particular issue. * [MaixPy Official Forum](https://maixhub.com/discussion/maixpy) (for asking questions and discussion) * Telegram: [MaixPy](https://t.me/maixpy) * MaixPy Source Code Issues: [MaixPy issue](https://github.com/sipeed/MaixPy/issues) * For business cooperation or bulk purchases, please contact support@sipeed.com."},"/maixpy/doc/en/ai_model_converter/maixcam.html":{"title":"Convert ONNX Model to a Format Usable by MaixCAM / MaixPy (MUD)","content":" title: Convert ONNX Model to a Format Usable by MaixCAM / MaixPy (MUD) > For MaixCAM2 model conversion, please refer to the [MaixCAM2 Model Conversion Documentation](./maixcam2.html). ## Introduction Models trained on a computer cannot be directly used by MaixCAM due to its limited hardware performance. Generally, we need to perform `INT8` quantization to reduce computation and convert the model into a format supported by MaixCAM. This article explains how to convert an ONNX model into a format that MaixCAM can use (MUD model). ## Model File Formats Supported by MaixCAM MUD (Model Universal Description file) is a model description file supported by MaixPy, used to unify model files across different platforms, making MaixPy code cross platform compatible. It is essentially a text file in `ini` format and can be edited with a text editor. Typically, a MUD file is accompanied by one or more actual model files. For MaixCAM, the actual model file is in `.cvimodel` format, with the MUD file providing some descriptive information. For example, a `YOLOv8` model consists of two files: `yolov8n.mud` and `yolov8n.cvimodel`. The former contains: ```ini [basic] type cvimodel model yolov8n.cvimodel [extra] model_type yolov8 input_type rgb mean 0, 0, 0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair dryer, toothbrush ``` This file specifies the model type as `cvimodel` and the model path relative to the MUD file as `yolov8n.cvimodel`. It also includes information such as preprocessing `mean` and `scale` (which should match the preprocessing method used during training), and `labels` representing the 80 categories for object detection. When using this model, place both files in the same directory. ## Preparing the ONNX Model Prepare your ONNX model and view it on [https://netron.app/](https://netron.app/) to ensure that the operators used in your model are supported by the conversion tool. The list of supported operators can be found in the **CVITEK_TPU_SDK Developer Guide.pdf** available from [Sophgo's TPU SDK](https://developer.sophgo.com/thread/473.html). ## Identify Appropriate Quantization Output Nodes Models usually have post processing nodes that are handled by the CPU. We need to strip these out as they can affect quantization quality and potentially cause quantization to fail. For example, in `YOLOv5`: ![YOLOv5 ONNX Model](../../assets/yolov5s_onnx.jpg) There are three `conv` layers, with subsequent calculations handled by the CPU. For quantization, use the outputs of these `conv` layers as the final outputs of the model. The output names in this case are `/model.24/m.0/Conv_output_0,/model.24/m.1/Conv_output_0,/model.24/m.2/Conv_output_0`. YOLO11/YOLOv8: Please refer to [Offline Training YOLO11/YOLOv8](../vision/customize_model_yolov8.html). For classification models, it is generally sufficient to take the name of the last output layer. However, if there is a `softmax` layer, it is recommended not to include `softmax` in the model. Instead, take the output of the layer before `softmax`. In the diagram below, there is no `softmax` layer, so the final layer can be used directly. ![](../../assets/mobilenet_top.png) ## Setting Up the Model Conversion Environment The model conversion uses Sophgo's [https://github.com/sophgo/tpu mlir](https://github.com/sophgo/tpu mlir). We will install it in a Docker environment to avoid compatibility issues with the host machine. ### Install Docker Follow the [official Docker installation documentation](https://docs.docker.com/engine/install/ubuntu/). For example: ```shell # Install dependencies for Docker sudo apt get update sudo apt get install apt transport https ca certificates curl gnupg agent software properties common # Add the official Docker source curl fsSL https://download.docker.com/linux/ubuntu/gpg sudo apt key add sudo add apt repository \"deb [arch amd64] https://download.docker.com/linux/ubuntu $(lsb_release cs) stable\" # Install Docker sudo apt get update sudo apt get install docker ce docker ce cli containerd.io ``` ### Pull the Docker Image ```shell docker pull sophgo/tpuc_dev:latest ``` > If pulling from within China, you may experience slow speeds. Consider setting up a local mirror. You can search for instructions or refer to [Docker Proxy and Mirror Setup](https://neucrack.com/p/286). ### Run the Container ```shell docker run privileged name tpu env v /home/$USER/data:/home/$USER/data it sophgo/tpuc_dev ``` This command starts a container named `tpu env`, mounting the `~/data` directory from the host to the container's `~/data`, enabling file sharing and path consistency. To start the container next time, use `docker start tpu env && docker attach tpu env`. ### Install tpu mlir Download the `whl` file from [GitHub](https://github.com/sophgo/tpu mlir/releases) and place it in the `~/data` directory. Install it in the container: ```shell pip install tpu_mlir*.whl # Replace with the downloaded file name ``` Type `model_transform.py` end with Enter key in container should display help information, indicating a successful installation. ## Writing the Conversion Script The conversion mainly involves two commands: `model_transform.py` and `model_deploy.py`. To simplify the process, create a script `convert_yolov5_to_cvimodel.sh`: ```shell #!/bin/bash set e net_name yolov5s input_w 640 input_h 640 # mean: 0, 0, 0 # std: 255, 255, 255 # mean # 1/std # mean: 0, 0, 0 # scale: 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 mkdir p workspace cd workspace # convert to mlir model_transform.py \\ model_name ${net_name} \\ model_def ../${net_name}.onnx \\ input_shapes [[1,3,${input_h},${input_w}]] \\ mean \"0,0,0\" \\ scale \"0.00392156862745098,0.00392156862745098,0.00392156862745098\" \\ keep_aspect_ratio \\ pixel_format rgb \\ channel_format nchw \\ output_names \"/model.24/m.0/Conv_output_0,/model.24/m.1/Conv_output_0,/model.24/m.2/Conv_output_0\" \\ test_input ../dog.jpg \\ test_result ${net_name}_top_outputs.npz \\ tolerance 0.99,0.99 \\ mlir ${net_name}.mlir # export bf16 model # not use quant_input, use float32 for easy coding model_deploy.py \\ mlir ${net_name}.mlir \\ quantize BF16 \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ model ${net_name}_bf16.cvimodel echo \"calibrate for int8 model\" # export int8 model run_calibration.py ${net_name}.mlir \\ dataset ../images \\ input_num 200 \\ o ${net_name}_cali_table echo \"convert to int8 model\" # export int8 model # add quant_input, use int8 for faster processing in maix.nn.NN.forward_image model_deploy.py \\ mlir ${net_name}.mlir \\ quantize INT8 \\ quant_input \\ calibration_table ${net_name}_cali_table \\ processor cv181x \\ test_input ${net_name}_in_f32.npz \\ test_reference ${net_name}_top_outputs.npz \\ tolerance 0.9,0.6 \\ model ${net_name}_int8.cvimodel ``` Key parameters include: `output_names`: Names of the output nodes we identified earlier. `mean, scale`: Preprocessing methods used during training. For instance, `YOLOv5` preprocesses the image by subtracting `mean` and dividing by `std`. In this example, `mean` is `0` and `std` is `255`, meaning the scale is `1/std`. Modify these according to your model's preprocessing method. `test_input`: The image used for testing during conversion. In this script, it's `../dog.jpg`, so ensure this image is placed in the same directory as the script. Replace it according to your model. `tolerance`: Allowed error margin before and after quantization. If errors during conversion indicate values lower than this threshold, it means the converted model might have significant deviation from the ONNX model. If acceptable, you can lower this threshold. Often, this requires optimizing the model and carefully examining post processing. `quantize`: The data type for quantization. Generally, `INT8` models are used on MaixCAM. Although a BF16 model is also converted here, INT8 is preferred for speed, while BF16 can be considered if INT8 conversion is not feasible or if precision is critical. `dataset`: The dataset used for quantization. For `YOLOv5`, it's a folder of images. Copy a subset of typical images from the coco dataset. Use ` input_num` to specify the number of images used (should be ≤ the actual number in the images directory). ## Running the Conversion Script Run the script with: ```shell chmod +x convert_yolov5_to_cvimodelsh && ./convert_yolov5_to_cvimodel.sh ``` Wait for the conversion to complete. If errors occur, carefully review the previous explanations for potential issues with parameters or output layers. Upon successful conversion, the `workspace` folder will contain a `**_int8.cvimodel` file. ## Writing the MUD File Modify the MUD file according to your model. For `YOLOv5`, the MUD file looks like this. Change `labels` to match your trained model: ```ini [basic] type cvimodel model yolov5s.cvimodel [extra] model_type yolov5 input_type rgb mean 0, 0, 0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 anchors 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair dryer, toothbrush ``` The `basic` section specifies the model file type and path, necessary for loading and running the model using the `maix.nn.NN` class in `MaixPy` or `MaixCDK`. The `extra` section varies based on the model. It includes parameters such as preprocessing, post processing, and labels. For `YOLOv5`, you can download its model, copy, and modify it. If you need to support a new model not currently supported by `MaixPy`, define the `extra` parameters based on the model's preprocessing and post processing requirements, then write the corresponding decoding class. If you prefer not to modify the `MaixPy` C++ source code, you can use the `maix.nn.NN` class to load the model and handle post processing in Python, though this is less efficient. ## Writing Post processing Code If you modify the `mud` file based on supported models, you can directly use the corresponding code in `MaixPy` or `MaixCDK`. If you need to support new models, design the `mud` file and write the preprocessing and post processing code: 1. **Option 1:** Use `maix.nn.NN` in `MaixPy` to load the model, then use the `forward` or `forward_image` function to run the model and process the output with Python functions. 2. **Option 2:** In `MaixCDK`, refer to [YOLOv5 source code](https://github.com/sipeed/MaixCDK/blob/71d5b3980788e6b35514434bd84cd6eeee80d085/components/nn/include/maix_nn_yolov5.hpp), add a new `hpp` file, and create a class to process your model. Modify all functions and class `@maixpy` annotations, compile the `MaixPy` project, and call the new class to run the model in `MaixPy`. You can submit the source code (Pull Request) to the main `MaixPy` repository to contribute to the community and share new models on [MaixHub](https://maixhub.com/share) for rewards ranging from 30 to 2000 yuan based on quality!"},"/maixpy/doc/en/ai_model_converter/maixcam2.html":{"title":"Convert ONNX Model to a Model Usable by MaixCAM2 MaixPy (MUD)","content":" title: Convert ONNX Model to a Model Usable by MaixCAM2 MaixPy (MUD) > For MaixCAM / MaixCAM Pro model conversion, please refer to the [MaixCAM Model Conversion Documentation](./maixcam.html) ## Introduction Models trained on a computer cannot be directly used on MaixCAM2 due to its limited hardware capabilities. Typically, the model needs to be quantized to `INT8` to reduce computation, and converted to a format supported by MaixCAM2. This article describes how to convert an ONNX model into a model usable by MaixCAM2 (MUD model). ## Supported Model File Format for MaixCAM2 MUD (Model Universal Description) is a model description file supported by MaixPy that unifies models across different platforms, making MaixPy code cross platform. It is a text file in `ini` format and can be edited with a text editor. A MUD file is usually accompanied by one or more actual model files. For MaixCAM2, the actual model file is in `.axmodel` format, and the MUD file provides meta information about it. For example, for a `YOLOv8` model, the files are `yolov8n.mud`, `yolo11n_640x480_vnpu.axmodel`, and `yolo11n_640x480_npu.axmodel`. The `.mud` file contains: ```ini [basic] type axmodel model_npu yolo11n_640x480_npu.axmodel model_vnpu yolo11n_640x480_vnpu.axmodel [extra] model_type yolo11 type detector input_type rgb labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush input_cache true output_cache true input_cache_flush false output_cache_inval true mean 0,0,0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 ``` As you can see, the model type is `axmodel`, and the paths to the model files are relative to the `.mud` file. Also included are important attributes: * `labels`: 80 categories of detection targets. * `input_cache`/`output_cache`: Whether input/output uses cached memory. Enables faster reading when data is accessed repeatedly (e.g., in post processing). * `input_cache_flush`: Whether to flush cache to DDR before model execution. If the first layer is an NPU operator, this should be `true`. For YOLO11, preprocessing is embedded (CPU first layer), so set to `false`. If unsure, use `true`. * `output_cache_inval`: Whether to invalidate output buffer after model execution to ensure DDR is accessed. If the last layer is NPU, set to `true`. For CPU, `false` is okay. If unsure, use `true`. * `mean`/`scale`: Although preprocessing is embedded during conversion, these are shown for reference and must match preprocessing used during training. Just place all three files in the same directory for use. ## Prepare the ONNX Model Prepare your ONNX model, and use [https://netron.app/](https://netron.app/) to view it. Ensure its operators are supported by the conversion tool, as listed in the [Pulsar2 Toolchain Documentation](https://pulsar2 docs.readthedocs.io/). For MaixCAM2, refer to the `AX620E` platform section. ## Find Appropriate Quantization Output Nodes Most models have post processing nodes handled by the CPU. These can affect quantization accuracy and cause failure, so we separate them. Example: `YOLOv5` ![](../../assets/yolov5s_onnx.jpg) Here we see three `conv` layers. Post conv computations are handled by the CPU. Thus, we set these three `conv` outputs as model outputs: `/model.24/m.0/Conv_output_0,/model.24/m.1/Conv_output_0,/model.24/m.2/Conv_output_0`. For YOLO11/YOLOv8, see [Offline Training YOLO11/YOLOv8](../vision/customize_model_yolov8.html). For classification models, the final output is usually enough. If there’s a `softmax`, it’s recommended to exclude it and take the output before `softmax`: ![](../../assets/mobilenet_top.png) ## Install the Model Conversion Environment Follow the [Pulsar2 Toolchain Documentation](https://pulsar2 docs.readthedocs.io/) to install it. Use Docker to avoid host environment conflicts. If you're new to Docker, think of it as a lightweight virtual machine. ### Install Docker Follow [Docker's official documentation](https://docs.docker.com/engine/install/ubuntu/). Example: ```shell # Install Docker dependencies sudo apt get update sudo apt get install apt transport https ca certificates curl gnupg agent software properties common # Add Docker repo curl fsSL https://download.docker.com/linux/ubuntu/gpg sudo apt key add sudo add apt repository \"deb [arch amd64] https://download.docker.com/linux/ubuntu $(lsb_release cs) stable\" # Install Docker sudo apt get update sudo apt get install docker ce docker ce cli containerd.io ``` ### Pull the Docker Image Follow the instructions in the [Pulsar2 Documentation](https://pulsar2 docs.readthedocs.io/) or download the latest version from [Hugging Face](https://huggingface.co/AXERA TECH/Pulsar2/tree/main). After downloading, load it with: ```shell docker load i pulsar2_vxx.tar.gz ``` ### Run the Container ```shell docker run it privileged name pulsar2 v /home/$USER/data:/home/$USER/data pulsar2 ``` This runs a container named `pulsar2` and mounts `~/data` into the container. To start next time: ```shell docker start pulsar2 && docker attach pulsar2 ``` Run `pulsar2` inside the container to view the help message. ## Convert the Model Refer to Pulsar2 documentation. The main command: ```shell pulsar2 build target_hardware AX620E input onnx_path output_dir out_dir config config_path ``` Key components are the `onnx` model and the `config` JSON file. Extract output nodes using this script `extract_onnx.py`: ```python # same as original code ``` Use `onnxsim` to simplify the model. Example `config.json` for YOLO11: ```json # same as original ``` Note: * `calibration_dataset`: Calibration data, sample some images from your dataset. * `npu_mode`: Set to `NPU2` to use full NPU. Set to `NPU1` if using AI ISP (divides NPU for two purposes). Convert both modes for flexibility (`model_npu` and `model_vnpu` in MUD file). ## Write Conversion Scripts Helpful scripts: * `extract_onnx.py`: Extract ONNX submodel (see above). * `gen_cali_images_tar.py`: Package image dataset into `tar` file for calibration. ```python # same as original code ``` * `convert.sh`: One click conversion for both NPU and VNPU models. ```shell # same as original script ``` After successful execution, you'll get `*_npu.axmodel` and `*_vnpu.axmodel`. ## Write the `mud` File Edit as per your model. For YOLO11: ```ini # same as previous MUD example ``` The `[basic]` section is required. Once set, you can load and run the model using `maix.nn.NN` in `MaixPy` or `MaixCDK`. If MaixPy doesn't support your model, define your own `extra` fields and write decoding logic. You can either: * Use Python in `MaixPy` to load the model via `maix.nn.NN`, run `forward`/`forward_image`, and process the outputs in Python (easier but slower); * Or, for better performance and reusable integration, write C++ logic in `MaixCDK`, see [YOLOv5 example](https://github.com/sipeed/MaixCDK/blob/71d5b3980788e6b35514434bd84cd6eeee80d085/components/nn/include/maix_nn_yolov5.hpp). Once done, consider submitting a PR to `MaixPy` or share your model on [MaixHub](https://maixhub.com/share) to earn rewards ranging from ¥30 to ¥2000!"},"/maixpy/doc/en/audio/deploy_online_recognition.html":{"title":"MaixCAM MaixPy Deploy online speech recognition","content":" title: MaixCAM MaixPy Deploy online speech recognition update: date: 2024 12 23 author: lxowalle version: 1.0.0 content: Initial document ## Introduction Deploying online speech recognition locally is a solution for real time processing of speech input. By running a speech recognition model on a local server and interacting with `MaixCAM`, it enables instant processing and result return of audio data without relying on external cloud services. This approach not only improves response speed but also protects user privacy, making it ideal for applications requiring high data security and real time performance, such as smart hardware, industrial control, and real time subtitle generation. This document uses the open source framework [`sherpa onnx`](https://github.com/k2 fsa/sherpa onnx) for deployment. `sherpa onnx` is a subproject of `sherpa`, supporting various tasks like streaming and non streaming speech recognition, text to speech, speaker classification, speaker recognition, speaker verification, and spoken language recognition. Below, we mainly introduce how to achieve streaming speech recognition using `MaixCAM` and `sherpa onnx`. > Note: Streaming speech recognition features high real time performance, allowing recognition during speech. It is commonly used in real time translation and voice assistants. Non streaming recognition requires processing a complete sentence at a time and is known for its high accuracy. ## Deploying the Speech Recognition Server `sherpa onnx` supports deployment in multiple languages, including `C/C++`, `Python`, `Java`, and more. For simplicity, we will use `Python` for deployment. If you encounter any issues during the process, you can refer to the `sherpa` [documentation](https://k2 fsa.github.io/sherpa/intro.html). Let's get started! #### Download the `sherpa onnx` Repository ```shell git clone https://github.com/k2 fsa/sherpa onnx.git ``` #### Install Dependencies ```python pip install numpy pip install websockets ``` #### Install the `sherpa onnx` Package ```python pip install sherpa onnx ``` If GPU support is required, install the CUDA enabled package: ```python pip install sherpa onnx 1.10.16+cuda f https://k2 fsa.github.io/sherpa/onnx/cuda.html # For users in China # pip install sherpa onnx 1.10.16+cuda f https://k2 fsa.github.io/sherpa/onnx/cuda cn.html ``` If the package is unavailable or installation fails, build and install from the source: ```python cd sherpa onnx export SHERPA_ONNX_CMAKE_ARGS \" DSHERPA_ONNX_ENABLE_GPU ON\" python3 setup.py install ``` If a GPU is available but `CUDA` is not installed, refer to the installation guide [`here`](https://k2 fsa.github.io/k2/installation/cuda cudnn.html) #### Verify the Installation of `sherpa onnx` ```python python3 c \"import sherpa_onnx; print(sherpa_onnx.__version__)\" # Expected output: # sherpa onnx or 1.10.16+cuda ``` #### Download the Model [`Zipformer Bilingual Model for Mandarin and English:sherpa onnx streaming zipformer bilingual zh en 2023 02 20 mobile`](https://github.com/k2 fsa/sherpa onnx/releases/download/asr models/.tar.bz2) [`Paraformer Trilingual Model for Mandarin, Cantonese, and English:sherpa onnx streaming paraformer trilingual zh cantonese en`](https://github.com/k2 fsa/sherpa onnx/releases/download/asr models/sherpa onnx streaming paraformer trilingual zh cantonese en.tar.bz2) > Note： > For Chinese recognition, it is recommended to use the `sherpa onnx streaming zipformer bilingual zh en 2023 02 20 mobile` model > > For English recognition, it is recommended to use the `sherpa onnx streaming paraformer trilingual zh cantonese en` model #### Run the Server `sherpa onnx` provides a server example, so there's no need to write additional code. Follow these steps to start the server. ##### Run the `zipformer` Model ```shell cd sherpa onnx export MODEL_PATH \"sherpa onnx streaming zipformer bilingual zh en 2023 02 20\" python3 ./python api examples/streaming_server.py \\ encoder ./${MODEL_PATH}/encoder epoch 99 avg 1.onnx \\ decoder ./${MODEL_PATH}/decoder epoch 99 avg 1.onnx \\ joiner ./${MODEL_PATH}/joiner epoch 99 avg 1.onnx \\ tokens ./${MODEL_PATH}/tokens.txt \\ provider \"cuda\" ``` ##### Run the `paraformer` Model ```shell cd sherpa onnx export MODEL_PATH \"sherpa onnx streaming paraformer trilingual zh cantonese en\" python3 ./python api examples/streaming_server.py \\ paraformer encoder ./${MODEL_PATH}/encoder.onnx \\ paraformer decoder ./${MODEL_PATH}/decoder.onnx \\ tokens ./${MODEL_PATH}/tokens.txt \\ provider \"cuda\" ``` ##### Example Log Output ```shell 2024 12 23 09:25:17,557 INFO [streaming_server.py:667] No certificate provided 2024 12 23 09:25:17,561 INFO [server.py:715] server listening on [::]:6006 2024 12 23 09:25:17,561 INFO [server.py:715] server listening on 0.0.0.0:6006 2024 12 23 09:25:17,561 INFO [streaming_server.py:693] Please visit one of the following addresses: http://localhost:6006 Since you are not providing a certificate, you cannot use your microphone from within the browser using public IP addresses. Only localhost can be used.You also cannot use 0.0.0.0 or 127.0.0.1 ``` At this point, the ASR model server is up and running. #### Communication Between `MaixCAM` and the Server For brevity, example client code is provided via the following links. Note that most cases require audio data with a sampling rate of 16000Hz and a single channel: [`MaixCAMMaixCAM` Streaming Recognition](https://github.com/sipeed/MaixPy/blob/main/examples/audio/asr/maix_speech/asr_streaming_websocket_client.py) [`MaixCAM` Non Streaming Recognition](https://github.com/sipeed/MaixPy/blob/main/examples/audio/asr/maix_speech/asr_non_streaming_websocket_client.py) ```shell # Update server address SERVER_ADDR \"127.0.0.1\" SERVER_PORT 6006 ``` After updating the server address and port, use maixvision to run the client. If using the streaming recognition script, try interacting with MaixCAM. > Note: This document does not elaborate on the communication protocol because it is straightforward—essentially raw data exchange via WebSocket. It is recommended to first experience the setup and then delve into the code for further details. The deployment process is now complete."},"/maixpy/doc/en/audio/play.html":{"title":"MaixCAM MaixPy Playback Audio","content":" title: MaixCAM MaixPy Playback Audio update: date: 2024 05 20 author: lxowalle version: 1.0.0 content: Initial document ## Introduction This document provides instructions on how to play audio ## How to use ### Hardware Support Device Microphone Speaker MaixCAM ✅ ❌ MaixCAM2 ✅ ✅ MaixCAM Pro ✅ ✅ ### Hardware operation ![image 20240520134637905](../../../static/image/maixcam_hardware_back.png) The `MaixCAM` does not have a built in speaker, so you will need to solder a `1W` speaker yourself. The pins for soldering the speaker are shown in the diagram above on the `VOP` and `VON` pins corresponding to the Speaker. Note: If the `MaixCAM` has copper posts attached to these pins, they can be soldered directly to the posts, or on the other side of the board for aesthetic reasons. ### Code #### Playing a `WAV` file ```python from maix import audio, time, app p audio.Player(\"/root/output.wav\") p.volume(80) p.play() while not app.need_exit(): time.sleep_ms(10) print(\"play finish!\") ``` Steps： 1. Import the audio, time and app modules: ```python from maix import audio, time, app ``` 2. Initialize the player: ```python p audio.Player(\"/root/output.wav\") ``` Note that the default sample rate is 48k, the sample format is little endian format signed 16 bit, and the sample channel is 1. You can also customise the parameters like this `p audio.Player(sample_rate 48000, format audio.Format.FMT_S16_LE, channel 1)`. So far only tested with sample rate 48000, format `FMT_S16_LE`, and number of sampling channels 1. If it is a `.wav` file, the sample rate, sample format and sample channel are automatically obtained. `p.volume(80)` set volume value to 80, range is [0~100]. 3. Playing audio ```python p.play() ``` This operation will block until all audio data is written, but not until all audio data is actually played. If you exit the programme after calling `play()`, some of the audio data to be played may be lost. 4. Done #### Playback with `PCM` data ```python from maix import audio, time, app p audio.Player() with open('/root/output.pcm', 'rb') as f: ctx f.read() p.play(bytes(ctx)) while not app.need_exit(): time.sleep_ms(10) print(\"play finish!\") ``` Steps： 1. Import the audio, time and app modules: ```python from maix import audio, time, app ``` 2. Initialize the player: ```python p audio.Player() ``` Note that the default sample rate is 48k, the sample format is little endian format signed 16 bit, and the sample channel is 1. You can also customise the parameters like this `p audio.Player(sample_rate 48000, format audio.Format.FMT_S16_LE, channel 1)`. So far only tested with sample rate 48000, format `FMT_S16_LE`, and number of sampling channels 1. 3. Open and playback a PCM file ```python with open('/root/output.pcm', 'rb') as f: ctx f.read() p.play(bytes(ctx)) while not app.need_exit(): time.sleep_ms(10) ``` `with open(‘xxx’,‘rb’) as f:` open file `xxx` and get file object `f` `ctx f.read()` reads the contents of the file into `ctx` `p.play(bytes(ctx))` plays the audio, `p` is the opened player object, `ctx` is the `PCM` data converted to type bytes `time.sleep_ms(10)` Here there is a loop to wait for the playback to complete, as the playback operation is performed asynchronously, and if the program exits early, then it may result in the audio not being played completely. 4. Done #### Non blocking Playback In scenarios like voice assistants or real time communication, audio playback usually cannot block the main thread. Therefore, the `Player` can be set to non blocking mode, and some application level code can be added to support playback without blocking the main thread. A reference example is shown below: ```python from maix import audio, app, time import threading from queue import Queue, Empty class StreamPlayer: def __init__(self, sample_rate 16000, channel 1, block:bool False): self.p audio.Player(sample_rate sample_rate, channel channel, block block) self.p.volume(50) zero_data bytes([0] * 4096) self.p.play(zero_data) self.queue Queue(maxsize 250) self.t threading.Thread(target self.__thread, daemon True) self.t.start() def wait_idle_size(self, size:int): while not app.need_exit(): idle_frames self.p.get_remaining_frames() write_frames size / self.p.frame_size() if idle_frames > write_frames: break time.sleep_ms(10) def __thread(self): while not app.need_exit(): try: pcm self.queue.get(timeout 500) # wait player is idle self.wait_idle_size(len(pcm)) self.p.play(pcm) except Empty: continue def write(self, pcm:bytes): remain_len len(pcm) period_bytes self.p.frame_size() * self.p.period_size() offset 0 while remain_len > 0: write_bytes period_bytes if period_bytes < remain_len else period_bytes remain_len new_pcm pcm[offset:offset+write_bytes] self.queue.put(new_pcm) remain_len write_bytes offset + write_bytes def wait_finish(self): total_frames self.p.period_count() * self.p.period_size() while not app.need_exit(): idle_frames self.p.get_remaining_frames() if idle_frames total_frames: break time.sleep_ms(10) if __name__ '__main__': stream_player StreamPlayer() with open('/maixapp/share/audio/demo.wav', 'rb') as f: pcm f.read() t time.ticks_ms() stream_player.write(pcm) print(f'write pcm data cost {time.ticks_ms() t} ms') t time.ticks_ms() stream_player.wait_finish() print(f'write play finish cost {time.ticks_ms() t} ms') ``` In this example, the `Player` object is set to non blocking mode via the `block` parameter. Therefore, calling the `play()` method does not block the main thread. Due to internal buffer size limitations, if the amount of data to play within a certain period exceeds the buffer capacity, `play` may still block. To handle this, you can use the `get_remaining_frames()` method to get the available space in the buffer. Note that this method returns the size in **frames**, which can be converted to **bytes** using `frame_size()`: ```python remaining_frames p.get_remaining_frames()\t\t# unit:frame remaining_bytes p.frame_size(remaining_frames) # unit: bytes ``` You can move the playback operation to a separate thread and, before calling `play()`, check whether there is enough remaining space in the buffer. This ensures that the main thread will never be blocked."},"/maixpy/doc/en/audio/keyword.html":{"title":"MaixCAM MaixPy Keyword recognition","content":" title: MaixCAM MaixPy Keyword recognition update: date: 2024 10 08 author: 916BGAI version: 1.0.0 content: Initial document ## Introduction `MaixCAM` has ported the `Maix Speech` offline speech library, enabling continuous Chinese numeral recognition, keyword recognition, and large vocabulary speech recognition capabilities. It supports audio recognition in `PCM` and `WAV` formats, and can accept input recognition via the onboard microphone. ## Maix Speech [`Maix Speech`](https://github.com/sipeed/Maix Speech) is an offline speech recognition library specifically designed for embedded environments. It has been deeply optimized for speech recognition algorithms, significantly reducing memory usage while maintaining excellent recognition accuracy. For detailed information, please refer to the [Maix Speech Documentation](https://github.com/sipeed/Maix Speech/blob/master/usage_zh.md). ## Keyword recognition ```python from maix import app, nn speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") speech.init(nn.SpeechDevice.DEVICE_MIC) kw_tbl ['xiao3 ai4 tong2 xue2', 'ni3 hao3', 'tian1 qi4 zen3 me yang4'] kw_gate [0.1, 0.1, 0.1] def callback(data:list[float], len: int): for i in range(len): print(f\"\\tkw{i}: {data[i]:.3f};\", end ' ') print(\"\\n\") speech.kws(kw_tbl, kw_gate, callback, True) while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` ### Usage 1. Import the `app` and `nn` modules ```python from maix import app, nn ``` 2. Load the acoustic model ```python speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") ``` You can also load the `am_7332` acoustic model; larger models provide higher accuracy but consume more resources. 3. Choose the corresponding audio device ```python speech.init(nn.SpeechDevice.DEVICE_MIC) speech.init(nn.SpeechDevice.DEVICE_MIC, \"hw:0,0\") # Specify the audio input device ``` This uses the onboard microphone and supports both `WAV` and `PCM` audio as input. ```python speech.init(nn.SpeechDevice.DEVICE_WAV, \"path/audio.wav\") # Using WAV audio input ``` ```python speech.init(nn.SpeechDevice.DEVICE_PCM, \"path/audio.pcm\") # Using PCM audio input ``` Note that `WAV` must be `16KHz` sample rate with `S16_LE` storage format. You can use the `arecord` tool for conversion. ```shell arecord d 5 r 16000 c 1 f S16_LE audio.wav ``` When recognizing `PCM/WAV` , if you want to reset the data source, such as for the next WAV file recognition, you can use the `speech.device` method, which will automatically clear the cache: ```python speech.device(nn.SpeechDevice.DEVICE_WAV, \"path/next.wav\") ``` 4. Set up the decoder ```python kw_tbl ['xiao3 ai4 tong2 xue2', 'ni3 hao3', 'tian1 qi4 zen3 me yang4'] kw_gate [0.1, 0.1, 0.1] def callback(data:list[float], len: int): for i in range(len): print(f\"\\tkw{i}: {data[i]:.3f};\", end ' ') print(\"\\n\") speech.kws(kw_tbl, kw_gate, callback, True) ``` The user can configure multiple decoders simultaneously. `kws` decoder is registered to output a list of probabilities for all registered keywords from the last frame. Users can observe the probability values and set their own thresholds for activation. When setting up the `kws` decoder, you need to provide a `keyword list` separated by spaces in Pinyin, a `keyword probability threshold list` arranged in order, and specify whether to enable `automatic near sound processing`. If set to `True`, different tones of the same Pinyin will be treated as similar words to accumulate probabilities. Finally, you need to set a callback function to handle the decoded data. Users can also manually register near sound words using the `speech.similar` method, with a maximum of `10` near sound words registered for each Pinyin. (Note that using this interface to register near sound words will override the near sound table generated by enabling `automatic near sound processing`.) ```python similar_char ['zhen3', 'zheng3'] speech.similar('zen3', similar_char) ``` If a decoder is no longer needed, you can deinitialize it by calling the `speech.dec_deinit` method. ```python speech.dec_deinit(nn.SpeechDecoder.DECODER_KWS) ``` 5. Recognition ```python while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` Use the `speech.run` method to run speech recognition. The parameter specifies the number of frames to run each time, returning the actual number of frames processed. Users can choose to run 1 frame each time and then perform other processing, or run continuously in a single thread, stopping it with an external thread. To clear the cache of recognized results, you can use the `speech.clear` method. When switching decoders during recognition, the first frame after the switch may produce incorrect results. You can use `speech.skip_frames(1)` to skip the first frame and ensure the accuracy of subsequent results. ### Recognition Results If the above program runs successfully, speaking into the onboard microphone will yield keyword recognition results, such as: ```shell kws log 2.048s, len 24 decoder_kws_init get 3 kws 00, xiao3 ai4 tong2 xue2 01, ni3 hao3 02, tian1 qi4 zen3 me yang4 find shared memory(491520), saved:491520 kw0: 0.959; \tkw1: 0.000; \tkw2: 0.000; # xiao3 ai4 tong2 xue2 kw0: 0.000; \tkw1: 0.930; \tkw2: 0.000; # ni3 hao3 kw0: 0.000; \tkw1: 0.000; \tkw2: 0.961; # tian1 qi4 zen3 me yang4 ```"},"/maixpy/doc/en/audio/record.html":{"title":"MaixCAM MaixPy Audio Record","content":" title: MaixCAM MaixPy Audio Record update: date: 2024 05 20 author: lxowalle version: 1.0.0 content: Initial document date: 2025 01 24 author: lxowalle version: 1.0.1 content: Update the usage instructions for the audio module. ## Introduction This document provides instructions on how to use the recording feature, supporting the recording of audio in both `PCM` and `WAV` formats. `PCM (Pulse Code Modulation)` is a digital audio encoding format used to convert analog audio signals into digital signals. It is also the commonly required format for general hardware processing. `WAV (Waveform Audio File Format)` is a popular audio file format. It is typically used to store uncompressed `PCM` audio data but also supports other encoding formats. The MaixCAM board comes with a built in microphone, so you can directly use the recording feature. ### Hardware Support Device Microphone Speaker MaixCAM ✅ ❌ MaixCAM2 ✅ ✅ MaixCAM Pro ✅ ✅ ### How to use #### Record an Audio File in `PCM`/`WAV` Format If you don't pass `path` when constructing a `Recorder` object, it will only record audio and not save it to a file, but you can save it to a file manually. ```python from maix import audio r audio.Recorder(\"/root/test.wav\") r.volume(100) print(f\"channel: {r.channel()}\") print(f\"sample rate: {r.sample_rate()}\") r.record(3000) ``` Steps： 1. Import the audio, time and app modules: ```python from maix import audio, time, app ``` 2. Initialize Recorder ```python r audio.Recorder(\"/root/test.wav\") r.volume(100) ``` Note that the default sample rate is 48k, the sample format is little endian format signed 16 bit, and the sample channel is 1. You can also customise the parameters like this `r audio.Recorder(sample_rate 48000, format audio.Format.FMT_S16_LE, channel 1)`. So far only tested with sample rate 16000 and 48000, format `FMT_S16_LE`, and number of sampling channels 1. `r.volume(100)` is used to set the volume, the volume range is [0,100] 3. Start recording ```python r.record(3000) ``` Record audio for 3000 milliseconds. This function will block until the recording is complete. 4. Done #### Record an Audio File in `PCM`/`WAV` Format (Non blocking) When developing applications, if you need to record audio but do not want the recording function to occupy time for other applications, you can enable non blocking mode. ```python from maix import audio, app, time r audio.Recorder(\"/root/test.wav\", block False) r.volume(100) r.reset(True) while not app.need_exit(): data r.record(50) // Your application time.sleep_ms(50) print(\"finish!\") ``` **Notes:** 1. In non blocking recording, you need to use the `reset(True)` function to enable the audio stream and the `reset(False)` function to stop the audio stream. 2. The length of the audio data returned by `record` may not match the input time. For example, if you request to record `50ms` of audio but only `20ms` of data is ready in the audio buffer, then `record(50)` will only return `20ms` of audio data. 3. If you want the audio data returned by `record()` to match the input parameter, you can wait until the buffer has enough audio data before reading. ```python remaining_frames r.get_remaining_frames() need_frames 50 * r.sample_rate() / 1000 if remaining_frames > need_frames: data r.record(50) ``` Use the `get_remaining_frames()` function to get the number of remaining frames in the receive buffer. Note that this returns the number of frames, not bytes. Use `sample_rate()` to get the audio sample rate and calculate the actual number of frames to read. #### Obtain Real time `PCM` Audio Stream When developing applications that need to process audio data, you may not need to save files but only require the raw `PCM` stream. To achieve this, simply do not provide a path when creating the `Recorder`. Of course, you can also enable non blocking mode. ```python from maix import audio, app, time r audio.Recorder(block False) r.volume(100) r.reset(True) while not app.need_exit(): data r.record(50) print(f'record {len(data)} bytes') // Your application time.sleep_ms(50) ``` The code logic is essentially the same as above."},"/maixpy/doc/en/audio/digit.html":{"title":"MaixCAM MaixPy Continuous Chinese digit recognition","content":" title: MaixCAM MaixPy Continuous Chinese digit recognition update: date: 2024 10 08 author: 916BGAI version: 1.0.0 content: Initial document ## Introduction `MaixCAM` has ported the `Maix Speech` offline speech library, enabling continuous Chinese numeral recognition, keyword recognition, and large vocabulary speech recognition capabilities. It supports audio recognition in `PCM` and `WAV` formats, and can accept input recognition via the onboard microphone. ## Maix Speech [`Maix Speech`](https://github.com/sipeed/Maix Speech) is an offline speech recognition library specifically designed for embedded environments. It has been deeply optimized for speech recognition algorithms, significantly reducing memory usage while maintaining excellent recognition accuracy. For detailed information, please refer to the [Maix Speech Documentation](https://github.com/sipeed/Maix Speech/blob/master/usage_zh.md). ## Continuous Chinese digit recognition ```python from maix import app, nn speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") speech.init(nn.SpeechDevice.DEVICE_MIC) def callback(data: str, len: int): print(data) speech.digit(640, callback) while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` ### Usage 1. Import the `app` and `nn` modules ```python from maix import app, nn ``` 2. Load the acoustic model ```python speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") ``` You can also load the `am_7332` acoustic model; larger models provide higher accuracy but consume more resources. 3. Choose the corresponding audio device ```python speech.init(nn.SpeechDevice.DEVICE_MIC) speech.init(nn.SpeechDevice.DEVICE_MIC, \"hw:0,0\") # Specify the audio input device ``` This uses the onboard microphone and supports both `WAV` and `PCM` audio as input. ```python speech.init(nn.SpeechDevice.DEVICE_WAV, \"path/audio.wav\") # Using WAV audio input ``` ```python speech.init(nn.SpeechDevice.DEVICE_PCM, \"path/audio.pcm\") # Using PCM audio input ``` Note that `WAV` must be `16KHz` sample rate with `S16_LE` storage format. You can use the `arecord` tool for conversion. ```shell arecord d 5 r 16000 c 1 f S16_LE audio.wav ``` When recognizing `PCM/WAV` , if you want to reset the data source, such as for the next WAV file recognition, you can use the `speech.device` method, which will automatically clear the cache: ```python speech.device(nn.SpeechDevice.DEVICE_WAV, \"path/next.wav\") ``` 4. Set up the decoder ```python def callback(data: str, len: int): print(data) speech.digit(640, callback) ``` The user can configure multiple decoders simultaneously. `digit` decoder is registered to output the Chinese digit recognition results from the last 4 seconds. The returned recognition results are in string format and support `0123456789 .(dot) S(ten) B(hundred) Q(thousand) W(thousand)`. When setting the `digit` decoder, you need to specify a `blank` value; exceeding this value (in ms) will insert a `_` in the output results to indicate idle silence. If a decoder is no longer needed, you can deinitialize it by calling the `speech.dec_deinit` method. ```python speech.dec_deinit(nn.SpeechDecoder.DECODER_DIG) ``` 5. Recognition ```python while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` Use the `speech.run` method to run speech recognition. The parameter specifies the number of frames to run each time, returning the actual number of frames processed. Users can choose to run 1 frame each time and then perform other processing, or run continuously in a single thread, stopping it with an external thread. To clear the cache of recognized results, you can use the `speech.clear` method. When switching decoders during recognition, the first frame after the switch may produce incorrect results. You can use `speech.skip_frames(1)` to skip the first frame and ensure the accuracy of subsequent results. ### Recognition Results If the above program runs successfully, speaking into the onboard microphone will yield continuous Chinese digit recognition results, such as: ```shell _0123456789 ```"},"/maixpy/doc/en/audio/recognize.html":{"title":"MaixCAM MaixPy Real-time voice recognition","content":" title: MaixCAM MaixPy Real time voice recognition update: date: 2024 10 08 author: 916BGAI version: 1.0.0 content: Initial document date: 2025 05 13 author: lxowalle version: 1.0.1 content: Added usage instructions for Whisper ## Introduction `MaixCAM` has ported the `Maix Speech` offline speech library, enabling continuous Chinese numeral recognition, keyword recognition, and large vocabulary speech recognition capabilities. It supports audio recognition in `PCM` and `WAV` formats, and can accept input recognition via the onboard microphone. Speech recognition model support list: MaixCAM MaixCAM Pro MaixCAM2 Whisper ❌ ❌ ✅ SenseVoice ❌ ❌ ✅ Speech ✅ ✅ ✅ In addition, we have ported OpenAI's Whisper speech recognition model to the `MaixCAM2`, enabling powerful speech to text functionality even on resource constrained devices. ## Using Whisper for Speech to Text The usage of Whisper can be found in [Whisper Speech Recognition Model](../mllm/asr_whisper.html) ## Using SenseVoice for Speech to Text The usage of Sensevoice can be found in [Sensevoice Speech Recognition Model](../mllm/asr_sensevoice.html) ## Maix Speech [`Maix Speech`](https://github.com/sipeed/Maix Speech) is an offline speech recognition library specifically designed for embedded environments. It has been deeply optimized for speech recognition algorithms, significantly reducing memory usage while maintaining excellent recognition accuracy. For detailed information, please refer to the [Maix Speech Documentation](https://github.com/sipeed/Maix Speech/blob/master/usage_zh.md). ### Continuous Large Vocabulary Speech Recognition ```python from maix import app, nn speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") speech.init(nn.SpeechDevice.DEVICE_MIC) def callback(data: tuple[str, str], len: int): print(data) lmS_path \"/root/models/lmS/\" speech.lvcsr(lmS_path + \"lg_6m.sfst\", lmS_path + \"lg_6m.sym\", \\ lmS_path + \"phones.bin\", lmS_path + \"words_utf.bin\", \\ callback) while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` ### Usage 1. Import the `app` and `nn` modules ```python from maix import app, nn ``` 2. Load the acoustic model ```python speech nn.Speech(\"/root/models/am_3332_192_int8.mud\") ``` You can also load the `am_7332` acoustic model; larger models provide higher accuracy but consume more resources. 3. Choose the corresponding audio device ```python speech.init(nn.SpeechDevice.DEVICE_MIC) speech.init(nn.SpeechDevice.DEVICE_MIC, \"hw:0,0\") # Specify the audio input device ``` This uses the onboard microphone and supports both `WAV` and `PCM` audio as input. ```python speech.init(nn.SpeechDevice.DEVICE_WAV, \"path/audio.wav\") # Using WAV audio input ``` ```python speech.init(nn.SpeechDevice.DEVICE_PCM, \"path/audio.pcm\") # Using PCM audio input ``` Note that `WAV` must be `16KHz` sample rate with `S16_LE` storage format. You can use the `arecord` tool for conversion. ```shell arecord d 5 r 16000 c 1 f S16_LE audio.wav ``` When recognizing `PCM/WAV` , if you want to reset the data source, such as for the next WAV file recognition, you can use the `speech.device` method, which will automatically clear the cache: ```python speech.device(nn.SpeechDevice.DEVICE_WAV, \"path/next.wav\") ``` 4. Set up the decoder ```python def callback(data: tuple[str, str], len: int): print(data) lmS_path \"/root/models/lmS/\" speech.lvcsr(lmS_path + \"lg_6m.sfst\", lmS_path + \"lg_6m.sym\", \\ lmS_path + \"phones.bin\", lmS_path + \"words_utf.bin\", \\ callback) ``` The user can configure multiple decoders simultaneously. `lvcsr` decoder is registered to output continuous speech recognition results (for fewer than 1024 Chinese characters). When setting up the `lvcsr` decoder, you need to specify the paths for the `sfst` file, the `sym` file (output symbol table), the path for `phones.bin` (phonetic table), and the path for `words.bin` (dictionary). Lastly, a callback function must be set to handle the decoded data. If a decoder is no longer needed, you can deinitialize it by calling the `speech.dec_deinit` method. ```python speech.dec_deinit(nn.SpeechDecoder.DECODER_LVCSR) ``` 5. Recognition ```python while not app.need_exit(): frames speech.run(1) if frames < 1: print(\"run out\\n\") break ``` Use the `speech.run` method to run speech recognition. The parameter specifies the number of frames to run each time, returning the actual number of frames processed. Users can choose to run 1 frame each time and then perform other processing, or run continuously in a single thread, stopping it with an external thread. To clear the cache of recognized results, you can use the `speech.clear` method. When switching decoders during recognition, the first frame after the switch may produce incorrect results. You can use `speech.skip_frames(1)` to skip the first frame and ensure the accuracy of subsequent results. ### Recognition Results If the above program runs successfully, speaking into the onboard microphone will yield real time speech recognition results, such as: ```shell ### SIL to clear decoder! ('今天天气 怎么样 ', 'jin1 tian1 tian1 qi4 zen3 me yang4 ') ```"},"/maixpy/doc/en/audio/synthesis.html":{"title":"MaixCAM MaixPy speech synthesis","content":" title: MaixCAM MaixPy speech synthesis update: date: 2025 08 15 author: lxowalle version: 1.0.0 content: Initial document ## Introduction This document provides instructions on using the built in TTS functionality to convert text into speech. TTS Support List: MaixCAM MaixCAM Pro MaixCAM2 MeloTTS ❌ ❌ ✅ ## About TTS TTS (Text to Speech) converts text into speech. You can write a piece of text and feed it to a TTS supported model. After running the model, it will output an audio data containing the spoken version of the text. In practice, TTS is commonly used for video dubbing, navigation guidance, public announcements, and more. Simply put, TTS is “technology that reads text aloud.” ## MeloTTS The usage of MeloTTS can be found in [MeloTTS Text to Speech Model](../mllm/tts_melotts.html)."},"/maixpy/doc/en/audio/ai_classify.html":{"title":"MaixCAM MaixPy AI voice classify","content":" title: MaixCAM MaixPy AI voice classify TODO: To be completed. If you need it urgently, you can first port the model yourself or process the audio into a spectrogram using FFT, and then train an AI classification model based on the image representation."},"/maixpy/doc/en/faq.html":{"title":"MaixCAM MaixPy FAQ (Frequently Asked Questions)","content":" title: MaixCAM MaixPy FAQ (Frequently Asked Questions) >! This page lists common questions and solutions related to MaixPy. If you encounter any issues, please search for answers here first. > Additionally, there are other resources: > * [MaixHub Discussion Forum](https://maixhub.com/discussion): A platform for discussions, with support for tip rewards. > * [MaixPy Issues](https://github.com/sipeed/MaixPy/issues?q ): For source code related issues. > * [MaixCAM Hardware FAQ](https://wiki.sipeed.com/hardware/zh/maixcam/faq.html): Frequently asked questions about MaixCAM hardware. ## MaixVision cannot find the device? First, confirm whether the connection method is WiFi or USB cable. **WiFi**: * Ensure that WiFi is correctly connected and has obtained an IP address. You can view the `ip` in `Settings > Device Info` or `Settings > WiFi`. **USB Cable**: * Ensure that the device is connected to the computer via a Type C data cable, and the device is powered on and has entered the function selection interface. * Ensure that the device driver is installed: * On Windows, check if there is a USB virtual network adapter device in `Device Manager`. If there is an exclamation mark, it means the driver is not installed properly. Follow the instructions in [Quick Start](./index.html) to install the driver. * On Linux, you can check if there is a `usb0` device by running `ifconfig` or `ip addr`, or check all USB devices with `lsusb`. Linux already includes the driver, so if the device is not recognized, check the hardware connection, ensure the device system is up to date, and ensure the device has booted up properly. * On macOS, follow the same steps as Linux. * Additionally, check the quality of the USB cable and try using a high quality cable. * Additionally, check the quality of the computer's USB port. For example, some small form factor PCs have poor EMI design on their USB ports, and connecting a good quality USB hub may allow the device to work. You can also try a different USB port or a different computer. ## MaixVision camera example shows choppy video The default GC4653 camera has a maximum frame rate of 30 frames per second (FPS). Under normal circumstances, the MaixVision display should not appear choppy to the naked eye. If choppiness occurs, first consider transmission issues: * Check the network connection quality, such as WiFi. * If using a USB connection, check the USB cable quality, computer USB port quality, and try using a different computer, USB port, or USB cable for comparison. ## MaixVision MacOS can not launch For macOS, since the developer account has not been used for signing during the early stages, you may encounter issues such as insufficient permissions or file corruption. Please refer to [this link](https://maixhub.com/discussion/100301) for a solution (run the command `sudo xattr dr com.apple.quarantine /Applications/ApplicationName.app` in the terminal to remove this attribute). ## What is the difference between MaixPy v4 and v1/v3? * MaixPy v4 uses the Python language and is the culmination of the experiences from v1 and v3, offering better supporting software and ecosystem, more features, simpler usage, and more comprehensive documentation. While the hardware has significant improvements, the pricing is even more affordable compared to the other two versions. Additionally, it provides compatibility with the K210 user experience and API, making it easier for users to migrate quickly from v1 to v4. * v1 used the Micropython language and had many limitations, such as limited third party library support. Additionally, due to the hardware performance limitations of the Maix I (K210), there was not enough memory, limited AI model support, and lack of hardware acceleration for many codecs. * v3 also used the Python language and was based on the Maix II Dock (v831) hardware. However, the hardware had limited AI model support, and the Allwinner ecosystem was not open enough, with an incomplete API. This version was only intended for use with the Maix II Dock (v831) and will not receive further updates. ## Does MaixPy currently only support MaixCAM, or can it work with other boards using the same chipset? MaixPy currently only supports the MaixCAM series of boards. Other boards using the same chipset, including Sipeed's boards like the LicheeRV Nano, are not supported. It is strongly recommended not to attempt using MaixPy with other boards, as it may result in device damage (such as smoke or screen burn), for which you will be solely responsible. In the future, Sipeed's Maix series of products will continue to be supported by MaixPy. If you have any needs that cannot be met by MaixCAM, you can post your requirements on the [MaixHub Discussion Forum](https://maixhub.com/discussion) or send an email to support@sipeed.com. ## Can I use a camera or screen other than the officially bundled ones? It is not recommended to use cameras or screens other than the officially bundled ones, unless you have sufficient software and hardware knowledge and experience. Otherwise, it may result in device damage. The officially bundled accessories have been fine tuned for both software and hardware, ensuring the best performance and allowing for out of the box usage. Other accessories may have different interfaces, drivers, and software, requiring you to calibrate them yourself, which is an extremely complex process. However, if you are an expert, we welcome you to submit a pull request! ## Model running error: cvimodel built for xxxcv181x CANNOT run on platform cv181x. Failure to parse the model file is generally caused by file corruption. Ensure that your model file is not damaged. For example: * Editing a binary file with an editor caused the file to become corrupted. For example, opening a `cvimodel` file with MaixVision can corrupt the binary file due to MaixVision's auto save feature. Therefore, do not open and save binary files with text editors like MaixVision (this issue will be fixed in a future update of MaixVision by removing the auto save feature). * If it was downloaded from the internet, make sure the download was not corrupted. Typically, files on the internet provide sha256sum/md5 checksums. After downloading, you can compare these values; for specific methods, please search online or ask ChatGPT. * If it comes from a compressed archive, ensure that the decompression process was error free. You can decompress the archive again to make sure there were no errors in the process. * Ensure that the file was not damaged during the transfer to the device. You can compare the sha256sum values of the file on the device and on your computer; for specific methods, please search online or ask ChatGPT. ## Power on Black Screen, No Display on the Screen Refer to [MaixCAM FAQ](https://wiki.sipeed.com/hardware/zh/maixcam/faq.html) ## Why doesn’t the computer detect a serial port when connecting via USB to MaixCAM? The USB port on the MaixCAM is a USB 2.0 interface of the chip, not a USB to serial interface, so it is normal for no serial port to appear when connected to a computer. How do you communicate without a USB to serial connection? By default, the USB will simulate a USB network card. When you connect the USB to your computer, a virtual network card will appear. According to the instructions in the [Quick Start Guide](./index.html), you can use MaixVision to communicate with MaixCAM to run code, preview images, manage files, and other functions. Additionally, since the USB simulates a network card, you can also use standard SSH software to connect to MaixCAM for communication. Alternatively, you can connect via WiFi and communicate within the same local network. If you need to use the serial port, there are two situations: 1. **Serial communication with a computer**: You need to purchase any USB to serial module to connect the computer's USB port with the board's serial port (for MaixCAM, it's the UART0 pins A16 (TX) and A17 (RX), or you can use the TX and RX pins on the USB adapter board that comes with the MaixCAM package, which are also the A16 and A17 pins and are functionally equivalent). 2. **Serial communication with another MCU/SOC**: Directly connect MaixCAM's A16 (TX) and A17 (RX) to the MCU's RX and TX pins. ## Red Screen, Initialization Display Failed, Please Check FAQ The message indicates that the display driver initialization failed. As of July 2024, the underlying display driver for MaixCAM is initialized together with the camera driver. Therefore, this issue is most likely caused by a failure in the camera driver initialization. To resolve this issue: * Try updating to the latest system and install the latest runtime libraries (very important!!!). The runtime libraries need to work in conjunction with the system drivers, and version mismatches may cause errors. Updating to the latest system image and installing the latest runtime libraries should generally resolve the issue. * Maybe multiple process try to occupy driver, easiest way is reboot. * Check for hardware connection issues with the camera. Ensure that the camera is properly connected and not damaged. ## What are the differences between Runtime, MaixPy, and system image? Which one should I upgrade? * **Runtime** is the runtime environment. Many system functions depend on it, including MaixPy. If you encounter the problem of being unable to run the program, first check and update it online. * The system image includes the basic operating system, hardware drivers, built in applications, and MaixPy firmware, etc. It is the basic environment. It is best to keep it up to date, especially in the [Release](https://github.com/sipeed/MaixPy/releases) page. If the version update mentions that the system has been updated, it is strongly recommended to update the system, because some MaixPy functions may depend on the drivers in the system. > Updating the system will format all previous data. Please back up useful data in the device system before updating. * **MaixPy** is a dependent library for running the MaixPy program. If you do not need to update the system function, and the update log does not mention that the system has important updates such as drivers, you can update MaixPy alone. ## Error Loading MUD Model File: *****.cvimodel not exists, load model failed * Check if the .mud file you are trying to load really exists on the device (note, it should be on the device, not on the computer, it needs to be transferred to the device). * Verify that the model path you wrote is correct. * If you have changed the file name, note that the MUD file is a model description file and can be edited with a text editor. The actual model file is the .cvimodel file (for MaixCAM). The .mud file specifies the file name and path of the .cvimodel. Therefore, if you have changed the file name of `.cvimodel`, you also need to modify the `model` path in the `.mud` file. For example, here is the mud file for the Yolov5 model: ```ini [basic] type cvimodel model yolov5s_224_int8.cvimodel [extra] model_type yolov5 input_type rgb mean 0, 0, 0 scale 0.00392156862745098, 0.00392156862745098, 0.00392156862745098 anchors 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 labels person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair dryer, toothbrush ``` Here, the `model` is specified as the `yolov5s_224_int8.cvimodel` file relative to the directory of this `.mud` file. If you have changed `yolov5s_224_int8.cvimodel` to another name, you need to update it here as well. ## MaixVision Shows Red Wavy Line on `import maix` MaixVision show `no maix moudle` `\"maix\" is not accessed` or `Import \"maix\" could not be resolved`: This error occurs because MaixVision's code hinting feature cannot find the `maix` module. It's important to understand that MaixVision's code hinting relies on the local Python packages on your computer, while the code execution depends on the Python packages on the device. To enable MaixVision's code hinting, you need to install Python and the `MaixPy` package on your computer. For more details, refer to the [MaixVision User Documentation](./basic/maixvision.html). ## MaixVision how to import from another .py file Read documentation of [MaixVision](./basic/maixvision.html) carefully. ## MaixCAM starts very slowly, even exceeding 1 minute, or the screen flickers This is mostly due to insufficient power supply. MaixCAM requires a voltage of around 5V and a current between 150mA and 500mA. If you encounter this issue, you can use a USB to TTL module to connect MaixCAM's serial port to a computer. You may see a message like `Card did not respond to voltage select! : 110`, indicating insufficient power supply. Simply switch to a more stable power supply to resolve the problem. For MaixCAM, it draws 400mA during startup, 250mA in standby mode with the screen on, and 400mA~500mA when running AI models at full speed. Therefore, ensuring a stable power supply is very important! ## MaixCAM Black screen and not boot up, or stock in LOGO screen Refer to [MaixCAM FAQ](https://wiki.sipeed.com/hardware/en/maixcam/faq.html) ## MaixVision Program Stuck on \"start running ...\" When the MaixVision log output window prints the message `start running ...`, it indicates that the program has been sent to the device and has begun executing. What gets printed afterward depends on your program. For instance, if you call `print(\"hello\")`, it will print `hello`. If your program doesn't include any print statements, then there will be no logs displayed. So, the program isn't actually stuck; it's just that your program hasn't output anything, so no logs are shown. You can try adding `print(\"xxx\")` in your code to generate output, which is the simplest way to debug your program. ## Why Does the Hardware Have 256MB of Memory, But Only 128MB is Available in the System? The remaining memory is reserved for low level drivers and the kernel, which are used for operating the camera, display, hardware encoding/decoding, NPU, and other drivers. You can check the memory used by these drivers (known as ION memory in CVITEK systems) by running `cat /sys/kernel/debug/ion/cvi_carveout_heap_dump/summary`. For other memory usage, you can run `cat /proc/meminfo`. If you want to adjust the memory allocation, you would need to compile the system yourself and modify the `ION_SIZE` in the `memmap.py` file located in the `LicheeRV Nano Build/build/boards/sg200x/sg2002_licheervnano_sd/` directory(refer to [customize system doc](./pro/compile_os.html)). ## Why Am I Unable to Install the Runtime Library, and an Error \"Request Failed\" Is Displayed? * Ensure that the device is successfully connected to the internet. You can try connecting to a different mobile hotspot. * Verify that the system image you flashed is the latest version. * If you see an error related to DNS resolution failure, it might be due to DNS settings issues on your network. You can try connecting to a different mobile hotspot or manually modify the DNS server settings in `/boot/resolv.conf` (modifying this file requires a reboot) and `/etc/resolv.conf` (modifying this file does not require a reboot, but rebooting will overwrite it with the contents of the former). * Make sure you have purchased a genuine MaixCAM from Sipeed. * Contact customer service, providing the system version and device_key (which can be found after disconnecting from MaixVision or, if you have a screen, in `System Settings > System Information`). Translation: ## Compile error: type not registered yet? ``` from ._maix.peripheral.key import add_default_listener ImportError: arg(): could not convert default argument into a Python object (type not registered yet?). #define ``` The error indicates that an object has not been defined as a Python object. In MaixPy, this is usually caused by an issue with the order of automatic API generation. For example, if there is an API declared with `@maixpy` in `a.hpp`, and another API in `b.hpp` that uses a definition from `a.hpp` as a parameter, then `b.hpp` depends on `a.hpp`. However, the current MaixPy compilation script does not perform dependency scanning. To resolve this, you need to manually specify the scan order in the `components/maix/headers_priority.txt` file in the MaixPy project, ensuring that `a.hpp` is scanned before `b.hpp`. ## MaixVision Display Lag The lag is typically due to using WiFi transmission. When the signal is weak or the image resolution is too high, delays can occur. Here are solutions to reduce lag: * Switch to a wired connection; refer to the Quick Start Guide for details. * Lower the image resolution by reducing the size of `img` in the code with `disp.show(img)`. ## Error Message When Running Application: Runtime error: mmf vi init failed The error message indicates that camera initialization failed. Possible reasons include:1. The camera is occupied by another application. 2.The camera's ribbon cable may be loose. 3. The runtime library is not installed. Solution Steps: Check if both the maixvision program and the board's built in program are running at the same time. Ensure that only one program is using the camera. (Note: Usually, when maixvision is connected, the built in program on the board will exit automatically.) Remove and reinsert the camera ribbon cable to ensure a proper connection. Update to the latest runtime library. ## When running the application, you see: maix multi media driver released or maix multi media driver destroyed This is not an error message. It is a log message indicating that the multimedia framework is releasing resources, it is usually printed when the program exits. ## Why can't show Chinese charactors By default only support English charactors, if you want to show Chinese, you need to change font, refer to [Custom fonts part of image basic operation](./vision/image_ops.md#Chinese support and custom fonts) ### Program Exit and Message: \"app exit with code: 1, log in /maixapp/tmp/last_run.log\" This indicates that the program encountered an error and exited unexpectedly. You need to check the log to find the issue. #### How to Check the Logs: **Method 1**: View the `/maixapp/tmp/last_run.log` file immediately after the error: 1. On MaixVision, run the script `MaixPy/examples/tools/show_last_run_log.py` to view the log. 2. On an SSH terminal, use the command `cat /maixapp/tmp/last_run.log` to view the log. **Method 2**: First, use MaixVision to connect to the device to exit any programs that are using the display or camera. Then, connect to the device via SSH and enter the SSH terminal. For connection steps, refer to the [Linux Basics](./basic/linux_basic.html). Manually run the program using the following commands: If it's a Python program: `cd /maixapp/apps/xxx && python main.py`, where `xxx` is the ID of the application that encountered the error. If it's not a Python program: `cd /maixapp/apps/xxx && ./xxx`, where `xxx` is the ID of the application that encountered the error. Carefully examine the logs for any errors. Note that the error may not appear on the last line, so check the logs from the bottom upwards. **Method 3**: If the application is written in Python, use MaixVision to run the source code to view runtime errors and fix them. Again, be aware that the error may not appear on the last line, so check the logs carefully from the bottom upwards. ### Method 2: If the application is written in Python, use **MaixVision** to run the source code directly, examine the runtime errors, and make corrections. Be aware that errors may not be at the last line, so inspect the logs carefully from the end upward. ### How to Read/Write to SD/TF Cards and Save Data to Them MaixPy is based on Linux and standard Python 3. The operating system and file system both run on the SD/TF card, so reading and writing data is done through the file system. You can use Python’s standard APIs to perform file operations. For example, to read a file: ```python with open(\"/root/a.txt\", \"r\") as f: content f.read() print(content) ``` Similarly, other functions that are not covered in the documentation can be searched to check if they are included in Python’s built in libraries, which you can call directly. ### Error: camera read timeout during image capture This error might occur when the camera's image buffer does not contain new images, causing a timeout during image capture. In most cases, this happens because the image is read too quickly, or multiple camera channels are attempting to read simultaneously. For instance, if one camera channel is bound to an RTSP service while another thread tries to capture images from a second camera channel. Solution: Catch the exception, wait for a short period, and then retry the capture. Example code: ```python img None try: img cam.read() except: time.sleep_ms(10) continue ``` ## VI_VENC_GetStream failed with 0xc0078012 during program runtime This issue occurs because the program did not exit properly last time, causing the VENC module resources to not be released. As a result, the application cannot obtain the VENC resources when it starts again. The current solutions are: 1. Restart the system. 2. Turn off the MaixVision preview view or switch to the PNG stream. Since this is a legacy issue at the underlying framework level, it can currently only be resolved at the application level. Efforts should be made to ensure that the program exits normally. ## How to uninstall the application refer to [APP Usage Document](./basic/app.html). ## Why is the image so blurry, as if a Gaussian blur has been applied? For the MaixCAM, the lens requires manual focusing. You can adjust the focus physically by twisting the lens. ## No module named 'lcd' or 'sensor': How to port OpenMV Code for Compatibility MaixPy (v4) is compatible with both OpenMV and MaixPy v1 code. All compatible modules are placed under the `maix.v1` module. For example, in OpenMV and MaixPy v1: ```python import lcd, sensor ``` In MaixPy (v4), you should write: ```python from maix.v1 import lcd, sensor ``` Or, to import all compatible modules at once (not recommended due to reduced code readability): ```python from maix.v1 import * import lcd, sensor ``` The latest version of MaixPy provides a compatibility layer for OpenMV/MaixPy v1, built on top of the new API. The source code is available at: [https://github.com/sipeed/MaixPy/tree/main/maix/v1](https://github.com/sipeed/MaixPy/tree/main/maix/v1). Therefore, **it is highly recommended to upgrade to the new MaixPy API**, which offers richer features and better performance. If you must use the old API and find that a specific function is not supported by the official compatibility layer, you can try modifying the source code mentioned above to add support by calling the new API underneath. Then, consider contributing your changes to the community at [https://github.com/sipeed/MaixPy](https://github.com/sipeed/MaixPy). For contribution guidelines, refer to the [MaixPy Contribution Documentation](./source_code/contribute.html). ## Where Are Photos and Videos Taken by the Built in Camera App on MaixCAM Stored, and How to Export Them to a Computer Photos and videos taken using MaixCAM’s built in **Camera** app can be viewed in the **Gallery** app. You can see the file path of a photo by tapping the `Info` button. To export the files to your computer, you can use the [MaixVision file manager feature](./basic/maixvision.html) or other file transfer tools such as `scp` or `WinSCP`. ## No Network, Is There an Offline Version of the Documentation? Yes. An offline HTML version of the documentation is provided. Go to the [MaixPy release page](https://github.com/sipeed/MaixPy/releases) and download the file named `maixpy_v*.*.*_doc.zip`. After extracting the file, you will get a `html` folder. Make sure `Python` is installed on your computer, then run the following command: `chmod +x ./view_doc.sh && ./view_doc.sh` (for Linux/MacOS) or `view_doc.bat` (for Windows). After that, open `http://<your_computer_IP>:8000/maixpy/index.html` in your browser to view the documentation offline. Additionally, if you just want to save a single page for offline viewing, you can press `Ctrl + P` on that page and select \"Print as PDF\" to save the page as a local PDF file. ## When recording is set to non blocking mode, no audio data is captured When recording in non blocking mode on MaixCam2, after starting recording, you need to wait for the audio data preparation to complete before being able to obtain the data. Therefore, there should be an interval between two recording operations. For example, if you need to record 60ms of audio data, you can call the recording function again every 50ms. ## The video cannot be decoded, or an error is reported during the decoding process 1. Ensure the video file format is h264 or mp4 2. If the error persists, try removing the B frames from the video, keeping only I frames and P frames, and then attempt again. You can use the ffmpeg to achieve this: `ffmpeg i input.mp4 c:v libx264 x264 params \"bframes 0\" output_no_b_frames.mp4` ## Can the MaixCam/MaixCam Pro/MaixCam2 auto focus? Not supported, MaixCam series cameras can only support manual focus, which means you need to manually turn the camera to focus."},"/maixpy/doc/en/pro/customize_model.html":{"title":"Adding a New AI Model to MaixCAM MaixPy","content":" title: Adding a New AI Model to MaixCAM MaixPy update: date: 2024 11 01 author: neucrack version: 1.0.0 content: Added migration documentation ## Introduction Besides the built in AI algorithms and models, MaixPy is highly extensible, allowing you to add your own algorithms and models. Due to the prevalence of visual applications, this guide will be divided into sections for visual applications and other applications. ## If MaixPy Already Supports the Framework but the Dataset is Different For instance, if MaixPy already supports YOLO11 detection, but your dataset is different, you only need to prepare your dataset, train the model, and export it. Another quick and lazy method is to first search online to see if someone has already trained or open sourced a model. If you find one, simply download it and convert the format for use, or continue training based on it. For example: If you want to detect fire, a quick search online might lead you to the project [Abonia1/YOLOv8 Fire and Smoke Detection](https://github.com/Abonia1/YOLOv8 Fire and Smoke Detection), which shares a fire and smoke detection model based on YOLOv8. You can download it, export it to the ONNX format, and then convert it to a format supported by MaixPy. You can also upload your model to the [MaixHub Model Library](https://maixhub.com/model/zoo) to share it with more people, or find models shared by others there. ## Adding Visual AI Models and Algorithms in Python For visual applications, the usual task is image recognition, specifically: * Input: Image * Output: Any data, such as classification, probability, image, coordinates, etc. In `MaixPy`, let’s use the common `YOLO11` detection algorithm as an example: ```python from maix import nn, image detector nn.YOLO11(model \"/root/models/yolo11n.mud\", dual_buff True) img image.Image(detector.input_width(), detector.input_height(), detector.input_format()) objs detector.detect(img, conf_th 0.5, iou_th 0.45) for obj in objs: img.draw_rect(obj.x, obj.y, obj.w, obj.h, color image.COLOR_RED) msg f'{detector.labels[obj.class_id]}: {obj.score:.2f}' img.draw_string(obj.x, obj.y, msg, color image.COLOR_RED) ``` In this code, we first construct the `YOLO11` object to load the model, then pass an image to the `detect` method for recognition. The steps include: * `nn.YOLO11()`: Initializes the object, loads the model into memory, and parses it. * `detector.detect()`: * Preprocesses the image, usually standardizing it, such as `(value mean) * scale`, adjusting pixel values to a suitable range like [0,1], which should match the preprocessing used during model training. * Runs the model, sending preprocessed data to the NPU for calculation following the model's network, producing output, typically floating point data. * Postprocesses the output, transforming the model’s output into the final result. To add a new model and algorithm, implement a similar class as `YOLO11`. Pseudocode example: ```python class My_Model: def __init__(self, model: str): pass # Parses the model, potentially custom parsing from a MUD file def recognize(self, img: image.Image): pass # Preprocesses image # Runs model # Postprocesses output # Returns result ``` Using the `nn.NN` class, we can parse and run models; see the [API](http://127.0.0.1:2333/maixpy/api/maix/nn.html#NN) documentation for details. Using `nn.NN`, we can parse our custom `mud` model description file, retrieve preprocessing values like `mean` and `scale`, and run the model with `nn.NN.forward_image()`. This method integrates preprocessing and running steps, reducing memory copy overhead for faster execution. For complex preprocessing, implement custom preprocessing, then run the model using `forward()` to get the output. Here’s an example of implementing a classification model without the built in `nn.Classifier`: ```python from maix import nn, image, tensor import os import numpy as np def parse_str_values(value: str) > list[float]: return [float(v) for v in value.split(\",\")] def load_labels(model_path, path_or_labels : str): path \"\" if not (\",\" in path_or_labels or \" \" in path_or_labels or \"\\n\" in path_or_labels): path os.path.join(os.path.dirname(model_path), path_or_labels) if path and os.path.exists(path): with open(path, encoding \"utf 8\") as f: labels0 f.readlines() else: labels0 path_or_labels.split(\",\") labels [] for label in labels0: labels.append(label.strip()) return labels class My_Classifier: def __init__(self, model : str): self.model nn.NN(model, dual_buff False) self.extra_info self.model.extra_info() self.mean parse_str_values(self.extra_info[\"mean\"]) self.scale parse_str_values(self.extra_info[\"scale\"]) self.labels self.model.extra_info_labels() # self.labels load_labels(model, self.extra_info[\"labels\"]) # same as self.model.extra_info_labels() def classify(self, img : image.Image): outs self.model.forward_image(img, self.mean, self.scale, copy_result False) # 后处理， 以分类模型为例 for k in outs.keys(): out nn.F.softmax(outs[k], replace True) out tensor.tensor_to_numpy_float32(out, copy False).flatten() max_idx out.argmax() return self.labels[max_idx], out[max_idx]def load_labels(model_path, path_or_labels : str): path \"\" if not (\",\" in path_or_labels or \" \" in path_or_labels or \"\\n\" in path_or_labels): path os.path.join(os.path.dirname(model_path), path_or_labels) if path and os.path.exists(path): with open(path, encoding \"utf 8\") as f: labels0 f.readlines() else: labels0 path_or_labels.split(\",\") labels [] for label in labels0: labels.append(label.strip()) return labels class My_Classifier: def __init__(self, model : str): self.model nn.NN(model, dual_buff False) self.extra_info self.model.extra_info() self.mean parse_str_values(self.extra_info[\"mean\"]) self.scale parse_str_values(self.extra_info[\"scale\"]) self.labels self.model.extra_info_labels() # self.labels load_labels(model, self.extra_info[\"labels\"]) # same as self.model.extra_info_labels() def classify(self, img : image.Image): outs self.model.forward_image(img, self.mean, self.scale, copy_result False) # 后处理， 以分类模型为例 for k in outs.keys(): out nn.F.softmax(outs[k], replace True) out tensor.tensor_to_numpy_float32(out, copy False).flatten() max_idx out.argmax() return self.labels[max_idx], out[max_idx] classifier My_Classifier(\"/root/models/mobilenetv2.mud\") file_path \"/root/cat_224.jpg\" img image.load(file_path, image.Format.FMT_RGB888) label, score classifier.classify(img) print(\"max score:\", label, score) ``` This code: * Loads the model and retrieves `mean` and `scale` parameters from the `mud` file. * Recognizes an image by directly calling `forward_image` for model output. * Applies `softmax` as a postprocessing step and displays the class with the highest probability as an example. More complex models may have elaborate postprocessing, like YOLO, which requires custom CPU processing for certain model parts. ## Adding AI Models and Algorithms for Other Data Types For other data types, like audio or motion sensor data: * Input: Any data, like audio, IMU, or pressure data. * Output: Any data, like classifications, probabilities, or control values. For non image inputs, use `forward` to process raw `float32` data. To prepare data for `forward`, convert it to `tensor.Tensors` from `numpy`: ```python from maix import nn, tensor, time import numpy as np input_tensors tensor.Tensors() for layer in model.inputs_info(): data np.zeros(layer.shape, dtype np.float32) t tensor.tensor_from_numpy_float32(data) input_tensors.add_tensor(layer.name, t, True, True) outputs model.forward(input_tensors, copy_result False, dual_buff_wait True) del input_tensors_li ``` This enables you to send raw data to the model. Alternatively, to reduce memory copy and speed up execution, use: ```python from maix import nn, tensor, time import numpy as np input_tensors tensor.Tensors() input_tensors_li [] for layer in model.inputs_info(): data np.zeros(layer.shape, dtype np.float32) t tensor.tensor_from_numpy_float32(data, copy False) input_tensors.add_tensor(layer.name, t, False, False) input_tensors_li.append(t) outputs model.forward(input_tensors, copy_result False, dual_buff_wait True) del input_tensors_li ``` ## Adding AI Models and Algorithms in C++ Writing Python code allows rapid model validation, but complex preprocessing or postprocessing can slow down performance. In such cases, consider C++ for efficiency. Refer to the [YOLO11 source code](https://github.com/sipeed/MaixCDK/blob/main/components/nn/include/maix_nn_yolo11.hpp) for guidance. Additionally, C++ code can be used in both C++ and MaixPy. By adding comments like `@maixpy maix.nn.YOLO11` to your C++ class, it can be used in MaixPy via `maix.nn.YOLO11`, providing seamless integration."},"/maixpy/doc/en/pro/compile_os.html":{"title":"Compiling a System for MaixCAM MaixPy","content":" title: Compiling a System for MaixCAM MaixPy ## Why Customize the System? Typically, you can download the latest system for MaixCAM directly from [this link](https://github.com/sipeed/MaixPy/releases). However, there are some scenarios where you might need to customize the system: * For example, if you are mass producing 1,000 products and want each to have your own application that automatically starts on boot, without configuring each one individually, you can modify the `builtin_files` and package a system. Once this system is flashed onto the boards, they will all include your custom files, eliminating the need to copy them again after booting. * If the official system does not include the software packages or drivers you need, you can compile your own system and select the packages you want to include. ## Obtaining the Base System The principle is to use a system from [this link](https://github.com/sipeed/LicheeRV Nano Build/releases) as the base (note that this system cannot be directly flashed onto MaixCAM as it may damage the screen), then copy the MaixCAM specific files into the base system and repackage it into a system usable by MaixCAM. If you don't need to customize the base system, you can directly download the latest system image from [here](https://github.com/sipeed/LicheeRV Nano Build/releases). If the base system doesn't meet your requirements, such as needing to add or remove some software packages and drivers, follow the instructions in the [LicheeRV Nano Build repository](https://github.com/sipeed/LicheeRV Nano Build) README to compile the system. It's recommended to use Docker for compilation to avoid environment issues and to use `bash` instead of `zsh`. Remember, the compiled system should not be flashed directly onto MaixCAM, as it might damage the screen. ## Copying Files for MaixCAM Prepare the following: * The base system, which is a `.img` or `.img.xz` file. * Additional files for MaixCAM can be downloaded from the [MaixPy release page](https://github.com/sipeed/MaixPy/releases). Download the latest `builtin_files.tar.xz`. > If you need to add custom files to the system, you can extract the files and add them to the appropriate directory. For example, if you want a `cat.jpg` file to be in the `/root` directory after flashing, simply place `cat.jpg` in the `root` directory. * Download or clone the MaixPy source code locally. * Compile MaixPy to obtain the `.whl` installation package, or you can download the latest installation package from the [MaixPy release page](https://github.com/sipeed/MaixPy/releases). In the `MaixPy/tools/os` directory, run the following command: ```shell ./gen_os.sh <base_os_filepath> <maixpy_whl_filepath> <builtin_files_dir_path> [skip_build_apps] [device_name] ``` Here’s what each parameter means: * **base_os_filepath**: The path to the base system, in `.img` or `.img.xz` format. * **maixpy_whl_filepath**: The MaixPy package, in `.whl` format. * **builtin_files_dir_path**: The custom files for MaixCAM, which can be downloaded from the MaixPy release page. * **skip_build_apps**: Skip compiling built in applications, optional arg. Set to 1 to skip, no this arg it will compile and copy apps from MaixCDK and MaixPy into the system. * **device name**: Can be `maixcam` or `maixcam pro` Example command: ```shell ./gen_os.sh '/home/xxx/.../LicheeRV Nano Build/install/soc_sg2002_licheervnano_sd/images/2024 08 13 14 43 0de38f.img' ../../dist/MaixPy 4.4.21 py3 none any.whl '/home/xxx/.../maixcam_builtin_files' 0 maixcam pro ``` After waiting for the built in apps to compile and copy, you should find a `maixcam pro 2024 08 15 maixpy v4.4.21.img.xz` system image in the `MaixPy/tools/os/tmp` directory."},"/maixpy/doc/en/pro/memory.html":{"title":"MaixPy MaixCAM Memory Usage Guide","content":" title: MaixPy MaixCAM Memory Usage Guide ## Introduction to MaixPy MaixCAM Memory MaixPy is based on the Python language, which runs on a Linux system. The camera, images, models, and applications all require a large amount of memory. Since memory is limited, understanding memory usage and management methods is very important. We can obtain the current memory usage status in many ways, either using built in MaixPy methods or common Linux methods. For example, using Python: ```python from maix import sys print(sys.memory_info()) ``` The output is similar to: ```json {'cma_total': 0, 'cma_used': 0, 'cmm_total': 2147483648, 'cmm_used': 177512448, 'hw_total': 4294967296, 'total': 2060726272, 'used': 339562496} ``` Or ```python import psutil # Get virtual memory info mem psutil.virtual_memory() print(f\"Total memory: {mem.total / (1024 ** 3):.2f} GB\") print(f\"Used memory: {mem.used / (1024 ** 3):.2f} GB\") print(f\"Available memory: {mem.available / (1024 ** 3):.2f} GB\") print(f\"Memory usage: {mem.percent}%\") ``` You can also use command line tools like `cat /proc/meminfo` or the `free` command to see memory info. From the `total` and `used` fields, you can see the total available memory and the memory already used. Note that the memory shown here is the memory available to Linux user space, which is less than the actual physical memory. For example, for a MaixCAM2 device with 4GiB memory, the default shown here is 1GiB. Why this is so will be explained below. ## MaixPy MaixCAM Memory Layout Because we use a Linux system, memory is divided into several regions based on usage: Region Purpose Size Reserved Low level drivers and special purposes; varies by device and vendor Varies by device, generally small Kernel Reserved Memory reserved for the Linux kernel Adjusted based on physical memory and kernel drivers, e.g., about 80MiB on MaixCAM2 User Memory Used by Linux user space programs Adjusted based on physical memory and application needs, e.g., about 1.92GiB on MaixCAM2 CMA Memory Contiguous Memory Allocator, used by Linux GPU/image components Configured according to image related applications CMM Memory Contiguous Memory Management, vendor or user defined contiguous memory region (not a Linux standard). Usually similar in purpose to CMA, but distinguished here as CMM. For example, MaixCAM series do not use standard CMA memory for images but define a custom region for camera, NPU, and other hardware drivers needing frequent large memory access Allocated based on application needs, e.g., MaixCAM2 defaults to 1GiB, MaixCAM defaults to 128MiB The two types of memory to focus on are: * **Linux User Space Memory:** This is the total memory available for our code and applications, such as allocating arrays, creating objects, and loading programs. * **CMM/CMA Memory:** For the MaixCAM series, without a GPU, CMA is usually zero (not used). Chip manufacturers tend to define their own standards. For example, MaixCAM and MaixCAM2 both use a custom memory region for camera, NPU, and other hardware drivers that need frequent large memory access. This improves performance and reduces memory fragmentation. For example: * **Camera frame capture:** The image is first read into this memory region; if the image needs to be viewed in Linux user space, it is mapped or copied into user space. Thus, the higher the camera resolution set by the application, the more memory is required. * **NPU running models:** Models are loaded into this custom memory area, not user space. For instance, if a large LLM model requires 1.5GiB, this memory area must be at least 1.5GiB to load the model successfully. ## Default Memory Sizes for Different Devices Device Hardware Memory Size Linux Kernel + User Space CMM Memory CMA Memory MaixCAM 256MiB 151MiB 105MiB 0MiB MaixCAM2 4GiB 1GiB 3GiB 0MiB MaixCAM2 1GiB 512MiB 512MiB 0MiB ## Adjusting Memory Allocation As mentioned above, default values are generally sufficient for most applications. If you want to adjust sizes, for example to increase CMM memory for loading larger models, you can modify the allocation yourself. Because CMM is typically designed by the CPU manufacturer, the modification methods differ by device: * **MaixCAM:** The CMM memory is actually called ION memory by the vendor. Modifying it is complex and requires recompiling the system. See [GitHub modification reference](https://github.com/sipeed/LicheeRV Nano Build/commit/713161599e1b590249b1cd8a9e7f2a7f68d8d52d). * **MaixCAM2:** The CMM memory is called CMM memory by the vendor. The MaixCAM2 image has been optimized so you only need to modify the `maix_memory_cmm` value in `/boot/configs` to the desired size in MiB. The default value is 1, which means using the default allocation. If you need me to translate the content into a more formal technical document style or adapt it for a specific audience, let me know!"},"/maixpy/doc/en/pro/datasets.html":{"title":"MaixCAM MaixP Where to Find Models and Datasets","content":" title: MaixCAM MaixP Where to Find Models and Datasets ## Where to Find Ready to Use Models for MaixPy MaixCAM Visit the [MaixHub Model Library](https://maixhub.com/model/zoo) and filter by the corresponding hardware platform to find suitable models. ## What Are Datasets Used For? First, check the [MaixHub Model Library](https://maixhub.com/model/zoo) to see if there’s a model you need. If not, you can train your own model. Training a model requires a dataset, which provides the data needed for training. ## Converting Existing Models for Use with MaixCAM MaixPy MaixPy natively supports several model frameworks, such as YOLOv8/YOLO11/Mobilenet, etc. Models trained with these frameworks can be exported to ONNX format and then converted to a format supported by MaixCAM for use with MaixPy MaixCAM. If you don’t want to train your own models, you can find open source pre trained models online. For instance, YOLO11/YOLOv8/YOLOv5 have many shared models. For example, from [this link](https://github.com/Eric Canas/qrdet/releases), you can download YOLOv8 models for QR code detection (`qrdet *.pt`). Simply export the model to ONNX format and convert it to [MaixCAM’s supported format](https://maixhub.com/model/zoo/480). Refer to the [MaixCAM Model Conversion Documentation](../ai_model_converter/maixcam.html) for details on the conversion process. ## Where to Find Datasets? 1. **Method 1: Check Official Documentation for Algorithms** For example, for YOLO11/YOLOv8, the [YOLO Official Documentation Datasets](https://docs.ultralytics.com/datasets/) provides a variety of open source datasets. You can use a single command to train models following the documentation. Afterward, export the model to ONNX format and convert it for MaixCAM. 2. **Method 2: Visit Dataset Websites** Platforms like [Kaggle](https://www.kaggle.com/datasets/riondsilva21/hand keypoint dataset 26k) or [Roboflow](https://universe.roboflow.com/) host numerous datasets for different applications. 3. **Method 3: Use Open Source Datasets and Prepare Training Scripts** Search for open source datasets and format them into scripts compatible with training frameworks like YOLO."},"/maixpy/doc/en/no_translate.html":{"title":"no translation","content":" title: no translation class: md_page <div id \"visit_from\"></div> <div id \"no_translate_hint\">This page not translated yet</div> <div> <span id \"visit_hint\">Please visit</span> <a id \"translate_src\"></a> </div> <div> <script> function getQueryVariable(variable) { var query window.location.search.substring(1); var vars query.split(\"&\"); for (var i 0;i<vars.length;i++) { var pair vars[i].split(\" \"); if(pair[0] variable){return pair[1];} } return(false); } var ref getQueryVariable(\"ref\"); var from getQueryVariable(\"from\"); var link document.getElementById(\"translate_src\"); var fromDis document.getElementById(\"visit_from\"); link.href ref; link.text ref; fromDis.innerHTML from; </script> </div>"}}